{"instruction": "Explain this concept:", "response": "Prompt Engineering\nSlides by Elvis Saravia https://www.promptingguide.ai/ and images\nfrom other sources"}
{"instruction": "Explain this concept:", "response": "Agenda\n\u2022 Introduction to Prompt Engineering\n\u2022 Advanced Techniques for Prompt Engineering\n\u2022 Conclusion & Future Directions"}
{"instruction": "Explain this concept:", "response": "Rise of In-context Learning\nBrown, Tom B. et al. \u201cLanguage Models are Few-Shot Learners.\u201d ArXiv abs/2005.14165 (2020): n. pag."}
{"instruction": "Explain this concept:", "response": "What are prompts?\n\u2022 Prompts involve instructions and context passed to a\nlanguage model to achieve a desired task\n\u2022 Prompt engineering is the practice of developing and\noptimizing prompts to efficiently use language models\n(LMs) for a variety of applications\n\u2022\nPrompt engineering is a useful skill for AI engineers and\nresearchers to improve and efficiently use language models\nWhat is prompt engineering?\nPrompt engineering is a process of creating a set of prompts,\nor questions, that are used to guide the user toward a desired\noutcome. It is an effective tool for designers to create user\nexperiences that are easy to use and intuitive. This method is\noften used in interactive design and software development, as\nit allows users to easily understand how to interact with a\nsystem or product.."}
{"instruction": "Explain this concept:", "response": "Why Prompt Engineering?\n\u2022 Why learn prompt engineering?\n\u2022\nImportant for research, discoveries, and advancement\n\u2022\nHelps to test and evaluate the limitations of LLMs\n\u2022\nEnables all kinds of innovative applications on top of LLMs\nSource: https://jobs.lever.co/Anthropic/e3cde481-d446-460f-b576-93cab67bd1ed"}
{"instruction": "Explain this concept:", "response": "Parameters of Decoding\n\u2022 Greedy/Beam Search generates less surprising/boring responses. Not\ndesirable for open-ended tasks like dialog and story-telling.\n\u2022 Instead, sampling can be used.\n\u2022 Temperature\n- controls sharpness of the next-token distribution\n- value between 0 to 1\n- lower temperature -> sharper distribution -> repetitive generations\n\u2022 Top p\n- value between 0 to 1\n- select smallest set of tokens whose total likelihood exceeds p.\nRedistribute the probabilities\n- smaller p leads to repetitive generations\nSource: https://huggingface.co/blog/how-to-generate"}
{"instruction": "Explain this concept:", "response": "First Basic Prompt\nInstruction\nThe sky is\nblue\nThe sky is a beautiful blue color during the day. The\nblue of the sky is created by the Earth\u2019s atmosphere\ne\ns\nn\nscattering the sun\u2019s light. The blue is a result of the\no\np\ns air molecules in the atmosphere reflecting the shorter\ne\nR\nwavelength of blue light back to our eyes.\nModel: \u201ctext-davinci-003\u201d\ntemperature: 0.7\ntop-p: 1"}
{"instruction": "Explain this concept:", "response": "Elements of a Prompt\n\u2022 A prompt is composed with the following components:\n\u2022\nInstructions\n\u2022\nContext\n\u2022\nInput data\n\u2022\nOutput indicator\nClassify the text into neutral, negative or positive\nText: I think the food was okay.\nSentiment:"}
{"instruction": "Explain this concept:", "response": "Settings to keep in mind\nWhen prompting a language model, you should keep in\nmind a few settings\n\u2022 You can get very different results with prompts when using\ndifferent settings\n\u2022 One important setting is controlling how deterministic the\nmodel is when generating completion for prompts\n\u2022\nTemperature and top_p are two important parameters to keep\nin mind\n\u2022\nGenerally, keep these low if you are looking for exact answers\n\u2022\n\u2026keep them high if you are looking for more diverse responses"}
{"instruction": "Explain this concept:", "response": "Designing Prompts for Different Tasks\n\u2022 In the next few slides, we will cover a few examples of\ncommon tasks using different prompts\n\u2022 Tasks covered:\n\u2022\nText Summarization\n\u2022\nQuestion Answering\n\u2022\nText Classification\n\u2022\nRole Playing\n\u2022\nCode Generation\n\u2022\nReasoning"}
{"instruction": "Explain this concept:", "response": "Text Summarization\nAntibiotics are a type of medication used to treat\nbacterial infections. They work by either killing the\nbacteria or preventing them from reproducing, allowing\nthe body\u2019s immune system to fight off the infection.\nt\nx\ne\nAntibiotics are usually taken orally in the form of\nt\nn\no\npills, capsules, or liquid solutions, or sometimes\nC\nadministered intravenously. They are not effective\nagainst viral infections, and using them inappropriately\ncan lead to antibiotic resistance.\nn\no\ni Explain the above in one sentence:\nt\nc\nu\nr\nt\ns\nn\nI\nAntibiotics are medications used to treat bacterial\ninfections by either killing the bacteria or stopping\nthem from reproducing, but they are not effective against\nviruses and overuse can lead to antibiotic resistance."}
{"instruction": "Explain this concept:", "response": "Question Answering\nAnswer the question based on the context below. Keep the\nanswer short and concise. Respond \"Unsure about answer\"\nif not sure about the answer.\nContext: Teplizumab traces its roots to a New Jersey drug\ncompany called Ortho Pharmaceutical. There, scientists\ngenerated an early version of the antibody, dubbed OKT3.\nOriginally sourced from mice, the molecule was able to\nbind to the surface of T cells and limit their cell-\nkilling potential. In 1986, it was approved to help\nprevent organ rejection after kidney transplants, making\nit the first therapeutic antibody allowed for human use.\nQuestion: What was OKT3 originally sourced from?\nAnswer: Mice.\nContext obtained from: https://www.nature.com/articles/d41586-023-00400-x"}
{"instruction": "Explain this concept:", "response": "Text Classification\nClassify the text into neutral, negative or positive.\nText: I think the food was okay.\nSentiment: Neutral"}
{"instruction": "Explain this concept:", "response": "Role Playing\nThe following is a conversation with an AI research\nassistant. The assistant tone is technical and\nscientific.\nHuman: Hello, who are you?\nAI: Greeting! I am an AI research assistant. How can I\nhelp you today?\nHuman: Can you tell me about the creation of blackholes?\nAI: Sure! Black holes are regions of spacetime where the\ngravitational force is so strong that nothing, not even\nlight, can escape from it. They are created when a very\nmassive star dies and its core collapses in on itself,\nforming a singularity of infinite density. The intense\ngravity of the singularity pulls in all the matter and\nradiation around it, creating the black hole."}
{"instruction": "Explain this concept:", "response": "Code Generation\n\u201c\u201d\u201d\nTable departments, columns = [DepartmentId,\nDepartmentName]\nTable students, columns = [DepartmentId, StudentId,\nStudentName]\nCreate a MySQL query for all students in the Computer\nScience Department\n\u201c\u201d\u201d\nSELECT StudentId, StudentName\nFROM students\nWHERE DepartmentId IN (SELECT DepartmentId FROM\ndepartments WHERE DepartmentName = 'Computer Science');"}
{"instruction": "Explain this concept:", "response": "Reasoning\nThe odd numbers in this group add up to an even number: 15,\n32, 5, 13, 82, 7, 1.\nSolve by breaking the problem into steps. First, identify\nthe odd numbers, add them, and indicate whether the result\nis odd or even.\nOdd numbers: 15, 5, 13, 7, 1\nSum: 41\n41 is an odd number."}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Techniques\n\u2022 Many advanced prompting techniques have been\ndesigned to improve performance on complex tasks\n\u2022\nFew-shot prompts\n\u2022\nChain-of-thought (CoT) prompting\n\u2022\nSelf-Consistency\n\u2022\nKnowledge Generation Prompting\n\u2022\nReAct"}
{"instruction": "Explain this concept:", "response": "Few-shot Prompts\n\u2022 Few-shot prompting allows us to provide exemplars in\nprompts to steer the model towards better performance\nThe odd numbers in this group add up to an even number: 4,\n8, 9, 15, 12, 2, 1.\nA: The answer is False.\nThe odd numbers in this group add up to an even number: 17,\n10, 19, 4, 8, 12, 24.\nA: The answer is True.\nThe odd numbers in this group add up to an even number: 16,\n11, 14, 4, 8, 13, 24.\nA: The answer is True.\nThe odd numbers in this group add up to an even number: 17,\n9, 10, 12, 13, 4, 2.\nA: The answer is False.\nThe odd numbers in this group add up to an even number: 15,\n32, 5, 13, 82, 7, 1.\nA: The answer is True."}
{"instruction": "Explain this concept:", "response": "Chain-of-Thought (CoT) Prompting\n\u2022 Prompting can be further improved by instructing the\nmodel to reason about the task when responding\n\u2022\nThis is very useful for tasks that requiring reasoning\n\u2022\nYou can combine it with few-shot prompting to get better results\n\u2022\nYou can also do zero-shot CoT where exemplars are not\navailable\nThe odd numbers in this group add up to an even number: 4,\n8, 9, 15, 12, 2, 1.\nA: Adding all the odd numbers (9, 15, 1) gives 25. The\nanswer is False.\nThe odd numbers in this group add up to an even number: 15,\n32, 5, 13, 82, 7, 1.\nA: Adding all the odd numbers (15, 5, 13, 7, 1) gives 41.\nThe answer is False.\nSource: Chain-of-Thought Prompting Elicits Reasoning in Large Language Models"}
{"instruction": "Explain this concept:", "response": "Zero-Shot CoT\n\u2022 Involves adding \"Let's think step by step\" to the original\nprompt\nI went to the market and bought 10 apples. I gave 2 apples\nto the neighbor and 2 to the repairman. I then went and\nbought 5 more apples and ate 1. How many apples did I\nremain with?\n11 apples\nI went to the market and bought 10 apples. I gave 2 apples\nto the neighbor and 2 to the repairman. I then went and\nbought 5 more apples and ate 1. How many apples did I\nremain with?\nLet's think step by step.\nFirst, you started with 10 apples.\nYou gave away 2 apples to the neighbor and 2 to the\nrepairman, so you had 6 apples left.\nThen you bought 5 more apples, so now you had 11 apples.\nFinally, you ate 1 apple, so you would remain with 10\napples.\nSource: Large Language Models are Zero-Shot Reasoners"}
{"instruction": "Explain this concept:", "response": "Self-Consistency\n\u2022 Self-Consistency aims to improve on the naive greedy\ndecoding used in chain-of-thought prompting\n\u2022 The idea is to sample multiple, diverse reasoning paths\nthrough few-shot CoT, and use the generations to select\nthe most consistent answer.\n\u2022 This helps to boost the performance of CoT prompting on\ntasks involving arithmetic and commonsense reasoning\nWhen I was 6 my sister was half my age. Now\nI\u2019m 70 how old is my sister?\n35\nMany examples were generated but the\nmodel kept responding 35 as the answer\nSource: Self-Consistency Improves Chain of Thought Reasoning in Language Models"}
{"instruction": "Explain this concept:", "response": "Self-Consistency Example\nQ: There are 15 trees in the grove. Grove workers will plant trees in the\ngrove today. After they are done, there will be 21 trees. How many trees did\nthe grove workers plant today?\nA: We start with 15 trees. Later we have 21 trees. The difference must be the\nnumber of trees they planted. So, they must have planted 21 - 15 = 6 trees.\nThe answer is 6.\nQ: If there are 3 cars in the parking lot and 2 more cars arrive, how many\ncars are in the parking lot?\nA: There are 3 cars in the parking lot already. 2 more arrive. Now there are\n3 + 2 = 5 cars. The answer is 5.\n...\nQ: Olivia has $23. She bought five bagels for $3 each. How much money does\nshe have left?\nA: She bought 5 bagels for $3 each. This means she spent 5\nQ: When I was 6 my sister was half my age. Now I\u2019m 70 how old is my sister?\nA:\n1\nWhen I was 6 my sister was half my age, so she was 3. Now I am 70, so she\nt\nu\np is 70 - 3 = 67. The answer is 67.\nt\nu\nO\n2\nWhen the narrator was 6, his sister was half his age, which is 3. Now\nt\nu that the narrator is 70, his sister would be 70 - 3 = 67 years old. The\np\nt u answer is 67.\nO\n3\nWhen the narrator was 6, his sister was half his age, which is 3. Now\nt\nu that the narrator is 70, his sister would be 70 - 3 = 67 years old. The\np\nt u answer is 67.\nO"}
{"instruction": "Explain this concept:", "response": "Generate Knowledge Prompting\n\u2022 This technique involves using additional knowledge\nprovided as part of the context to improve results on\ncomplex tasks such as commonsense reasoning\n\u2022 The knowledge used in the context is generated by a\nmodel and used in the prompt to make a prediction\n\u2022\nHighest-confidence prediction is used\nSource: Generated Knowledge Prompting for Commonsense Reasoning"}
{"instruction": "Explain this concept:", "response": "Generate Knowledge Prompting Example\n\u2022 The first step is to generate knowledge. Below is an\nexample of how to generate the knowledge samples\nInput: Greece is larger than mexico.\nKnowledge: Greece is approximately 131,957 sq km, while Mexico is\napproximately 1,964,375 sq km, making Mexico 1,389% larger than Greece.\n...\nInput: A rock is the same size as a pebble.\nKnowledge: A pebble is a clast of rock with a particle size of 4 to 64\nmillimetres based on the Udden-Wentworth scale of sedimentology. Pebbles\nare generally considered larger than granules (2 to 4 millimetres\ndiameter) and smaller than cobbles (64 to 256 millimetres diameter).\nInput: Part of golf is trying to get a higher point total than others.\nKnowledge:\n1\nThe objective of golf is to play a set of holes in the least number of strokes. A\ne\ng\nround of golf typically consists of 18 holes. Each hole is played once in the\nd\ne\nround on a standard golf course. Each stroke is counted as one point, and the\nl\nw\no total number of strokes is used to determine the winner of the game.\nn\nK\n2 Golf is a precision club-and-ball sport in which competing players (or golfers)\ne\nuse many types of clubs to hit balls into a series of holes on a course using the\ng\nd\nfewest number of strokes. The goal is to complete the course with the lowest\ne\nl\nw score, which is calculated by adding up the total number of strokes taken on each\no\nn hole. The player with the lowest score wins the game.\nK"}
{"instruction": "Explain this concept:", "response": "Generate Knowledge Prompting Example\n\u2022 The knowledge samples are then used to generate\nknowledge augmented questions to get answer proposals\n\u2022\nThe highest-confidence response is selected as final answer\nQuestion: Part of golf is trying to get a higher point total than others.\nYes or No?\nKnowledge: The objective of golf is...\nExplain and Answer: No, the objective of golf is not to get a higher\npoint total than others. Rather, the objective is to play a set of holes\nin the least number of strokes. The total number of strokes is used to\ndetermine the winner of the game, not the total number of points.\nHigh-confidence prediction\nQuestion: Part of golf is trying to get a higher point total than others.\nYes or No?\nKnowledge: Golf is a precision club-and-ball sport in which competing\nplayers (or golfers) use many types of clubs to hit balls into...\nExplain and Answer: Yes, part of golf is trying to get a higher point\ntotal than others. Each player tries to complete the course with the\nlowest score, which is calculated by adding up the total number of\nstrokes taken on each hole. The player with the lowest score wins the\ngame.\nLow-confidence prediction"}
{"instruction": "Explain this concept:", "response": "Program-aided Language Model (PAL)\n\u2022 Chain-of-thought prompting is a good example of how to\nsteer models to perform better at complex reasoning tasks\n\u2022\nHowever, sometimes CoT is not enough as it depends only on\nthe generated text from the model\n\u2022 Program-aided language models (PAL) uses an LLM to\nread problems and generate programs as the intermediate\nreasoning steps\n\u2022\nIt offloads the solution step to a runtime such as Python\ninterpreter"}
{"instruction": "Explain this concept:", "response": "ReAct\n\u2022 ReAct is a framework where LLMs are used to generate\nboth reasoning traces and task-specific actions in an\ninterleaved manner\n\u2022\nGenerating reasoning traces allow the model to induce, track,\nand update action plans, and even handle exceptions\n\u2022\nThe action step allows to interface with and gather information\nfrom external sources such as knowledge bases or\nenvironments.\n\u2022 ReAct allows LLMs to interact with external tools to\nretrieve additional information that leads to more reliable\nand factual responses"}
{"instruction": "Explain this concept:", "response": "Directional Stimulus Prompting\n\u2022 Prompting technique to better guide the LLM in generating the\ndesired summary.\n\u2022 A tuneable policy LM is trained to generate the hints that guide a\nblack-box frozen LLM."}
{"instruction": "Explain this concept:", "response": "Risks\n\u2022 In this section, we discuss the following:\n\u2022\nPrompt Injection\n\u2022\nPrompt Leaking\n\u2022\nJail Breaking"}
{"instruction": "Explain this concept:", "response": "Prompt Injection\n\u2022 Prompt injection is used to hijack an LM\u2019s output by\ninjecting an untrusted command that overrides instruction\nof a prompt\n\u2022 This could easily happen if you just concatenate your\nprompt with another user generated prompt"}
{"instruction": "Explain this concept:", "response": "Prompt Leaking\n\u2022 Prompt leaking aims to force the model to spit out\ninformation about its own prompt.\n\u2022 This can lead to leaking of either sensitive, private or\ninformation that\u2019s confidential"}
{"instruction": "Explain this concept:", "response": "Jailbreaking\n\u2022 Jailbreaking is another form of prompt injection where the\ngoal is to bypass safety and moderation features\n\u2022 LLMs provided via APIs might be coupled with safety\nfeatures or content moderation which can be bypassed\nwith harmful prompts/attacks\n\u2022 This might sound like a difficult task but it\u2019s not because\nthe model is usually served static and might have these\nvulnerabilities due to many factors such as the data it was\ntrained on, etc."}
{"instruction": "Explain this concept:", "response": "A Systematic Survey of Prompt Engineering in Large Language Models:\nTechniques and Applications\nPranabSahoo1, AyushKumarSingh1, SriparnaSaha1, VinijaJain2,3, Samrat\nMondal1 and AmanChadha2,3\n1DepartmentofComputerScienceAndEngineering,IndianInstituteofTechnologyPatna\n2StanfordUniversity,3AmazonAI\n{pranab_2021cs25, ayush_2211ai27, sriparna, samrat}@iitp.ac.in, hi@vinija.ai,\nhi@aman.ai\nAbstract\nPrompt Engineering\nLLM\nPrompt engineering has emerged as an indispens-\nInstruction\nabletechniqueforextendingthecapabilitiesoflarge\nOutput: Response\nlanguagemodels(LLMs)andvision-languagemod- generated by LLM\nContext\nels(VLMs). Thisapproachleveragestask-specific Pre-trained on\nbillions of\ninstructions,knownasprompts,toenhancemodel\nUser's Input parameters\nefficacywithoutmodifyingthecoremodelparam-\neters. Ratherthanupdatingthemodelparameters,\nFigure 1: Visual breakdown of prompt engineering components:\npromptsallowseamlessintegrationofpre-trained\nLLMstrainedonextensivedata,instructionandcontextaspivotal modelsintodownstreamtasksbyelicitingdesired\nelementsshapingtheprompt,andauserinputinterface.\nmodelbehaviorssolelybasedonthegivenprompt.\nPromptscanbe naturallanguageinstructionsthat\nprovidecontexttoguidethemodelorlearnedvec-\nVLMs. Byofferingamechanismtofine-tunemodeloutputs\ntorrepresentationsthatactivaterelevantknowledge.\nthroughcarefullycraftedinstructions,promptengineeringen-\nThis burgeoning field has enabled success across\nablesthesemodelstoexcelacrossdiversetasksanddomains. various applications, from question-answering to\nThisadaptabilityisdifferentfromtraditionalparadigms,where\ncommonsensereasoning. However,thereremainsa\nmodelretrainingorextensivefine-tuningisoftenrequiredfor\nlackofsystematicorganizationandunderstanding\ntask-specificperformance. Thisisthetransformativepromise\nofthediversepromptengineeringmethodsandtech-\nof prompt engineering, pushing the boundaries of AI and\nniques. Thissurveypaperaddressesthegapbypro-\nopening doors to a future brimming with possibilities. In\nvidingastructuredoverviewofrecentadvancements\nanever-evolvinglandscape,ongoingresearchconsistentlyre-\ninpromptengineering,categorizedbyapplication\nvealsinnovativeapproachesandapplicationswithinprompt\narea. Foreachpromptingapproach,weprovidea\nengineering. Thesignificanceofpromptengineeringisunder-\nsummarydetailingthepromptingmethodology,its\nscoredbyitscapacitytosteermodelresponses,enhancingthe\napplications,themodelsinvolved,andthedatasets\nadaptabilityandapplicabilityofLLMsacrossdiversesectors.\nutilized. Wealsodelveintothestrengthsandlimita-\nThelandscapeofcontemporarypromptengineeringspansa\ntionsofeachapproachandincludeataxonomydia-\nspectrumoftechniques,encompassingfoundationalmethods\ngramandtablesummarizingdatasets,models,and\nlike zero-shot and few-shot prompting to more intricate ap-\ncritical points of each prompting technique. This\nproachessuchas\"chainofcode\"prompting. Thenotionof\nsystematicanalysisenablesabetterunderstanding\npromptengineeringwasinitiallyinvestigatedandpopularized\nof this rapidly developing field and facilitates fu-\nintheLLMs[Liuetal.,2023],[Tonmoyetal.,2024],[Chen\ntureresearchbyilluminatingopenchallengesand\netal.,2023]laterextendedtoVLMs[Wuetal.,2023],[Bahng\nopportunitiesforpromptengineering.\netal.,2022]. Despitetheextensiveliteratureonpromptengi-\nneeringwithinbothLLMsandVLMs,anotablegapremains,\nparticularlyconcerningasystematicoverviewofapplication-\n1 Introduction\ncentric prompt engineering techniques. With recent strides\nPrompt engineering has emerged as a crucial technique for in prompt engineering, there is a pressing need for a com-\nenhancingthecapabilitiesofpre-trainedlargelanguagemod- prehensivesurveythatoffersanuancedunderstandingofap-\nels(LLMs)andvision-languagemodels(VLMs). Itinvolves plicationsandadvancementsincontemporaryresearch. This\nstrategicallydesigningtask-specificinstructions,referredtoas surveydivesdeepintotheever-evolvinglandscapeofprompt\nprompts,toguidemodeloutputwithoutalteringparameters. engineering,analyzingover29distincttechniquescategorized\nThesignificanceofpromptengineeringisespeciallyevident bytheirdiverseapplications. Employingasystematicreview\ninitstransformativeimpactontheadaptabilityofLLMsand approach,wemeticulouslydelveintotheintricaciesofdiverse\n4202\nbeF\n5\n]IA.sc[\n1v72970.2042:viXra"}
{"instruction": "Explain this concept:", "response": "cutting-edge prompting methods. Our examination encom- 2022] introduced Chain-of-Thought (CoT) prompting as a\npassestheirapplications, thelanguagemodelsutilized, and techniquetopromptLLMsinawaythatfacilitatescoherent\nthedatasetssubjectedtoexperimentation,providingadetailed andstep-by-stepreasoningprocesses. Theprimarycontribu-\nandnuancedanalysisoftheevolvinglandscapeofprompten- tionliesintheproposalandexplorationofCoTprompting,\ngineering. Additionally,wediscusstheprosandconsofthese demonstrating its effectiveness in eliciting more structured\ntechniques,offeringinsightsintotheircomparativeefficacy. andthoughtfulresponsesfromLLMscomparedtotraditional\nWeunveilacomprehensivetaxonomydiagramthatshedslight prompts. Throughaseriesofexperiments,theauthorsshow-\non how these techniques navigate the vast terrain of LLM casethedistinctivequalitiesofCoTprompting,emphasizing\ncapabilities. Fromlanguagegenerationandquestionanswer- itsabilitytoguideLLMsthroughalogicalreasoningchain.\ningtocodecreationandreasoningtasks,promptengineering Thisresultsinresponsesthatreflectadeeperunderstanding\nempowerstheLLMsintoperformingfeatsweneverthought ofthegivenprompts. Forexample,thepromptwouldshow\npossible. Bybridgingtheexistinggapintheliterature,this thereasoningprocessandfinalanswerforamulti-stepmath\nsurveyaimstoserveasavaluableresourceforresearchersand wordproblemandmimichowhumansbreakdownproblems\npractitioners, offering insights into the latest developments into logical intermediate steps. The authors achieved state-\nandfacilitatingadeeperunderstandingoftheevolvingland- of-the-artperformanceinmathandcommonsensereasoning\nscape of prompt engineering. The structure of the paper is benchmarksbyutilizingCoTpromptsforPaLM540B,achiev-\norganizedasfollows: Section2presentsthepromptengineer- inganaccuracyof90.2%.\ningtechniquesfrombothbasictoadvancedbycategorizing\nAutomaticChain-of-Thought(Auto-CoT)Prompting\napplication-area and Section 3 provides a conclusion along\nwithconsiderationsforfutureresearchendeavors. Manual creation of high-quality CoT examples is both time-\nconsuming and suboptimal. [Zhang et al., 2022] introduced\n2 PromptEngineering Auto-CoTtoautomaticallyinstructLLMswitha\"Let\u2019sthink\nstep-by-step\"prompttogeneratereasoningchains.Recognizing\nInthissection,wehaveorganizedpromptengineeringtech- thepossibilityoferrorsinindividuallygeneratedchains,Auto-\nniques according to their application areas and provided a CoT enhances robustness through diverse sampling. It sam-\nconciseoverviewoftheevolutionofpromptingtechniques, plesvariousquestionsandgeneratesmultipledistinctreasoning\nspanningfromzero-shotpromptingtothelatestadvancements. chains for each, forming a final set of demonstrations. This\nautomateddiversesamplingminimizeserrorsandenhancesfew-\n2.1 NewTasksWithoutExtensiveTraining\nshotlearning,eliminatingtheneedforlabor-intensivemanual\nZero-ShotPrompting creationofreasoningchains.Auto-CoTdemonstratedenhanced\nZero-shotpromptingoffersaparadigmshiftinleveraginglarge performance, surpassing the CoT paradigm with average ac-\nLLMs.Thistechnique[Radfordetal.,2019]removestheneed curacy improvements of 1.33% and 1.5% on arithmetic and\nforextensivetrainingdata,insteadrelyingoncarefullycrafted symbolicreasoningtasks,respectively,employingGPT-3.\npromptsthatguidethemodeltowardnoveltasks. Specifically,\nthemodelreceivesataskdescriptioninthepromptbutlacksla- Self-Consistency\nbeleddatafortrainingonspecificinput-outputmappings. The [Wang et al., 2022] introduced self-consistency, a decoding\nmodelthenleveragesitspre-existingknowledgetogenerate strategyenhancingreasoningperformancecomparedtogreedy\npredictionsbasedonthegivenpromptforthenewtask. decodinginCoTprompting.Forcomplexreasoningtaskswith\nmultiplevalidpaths,self-consistencygeneratesdiversereason-\nFew-ShotPrompting\ningchainsbysamplingfromthelanguagemodel\u2019sdecoder.It\nFew-shotpromptingprovidesmodelswithafewinput-output\nthenidentifiesthemostconsistentfinalanswerbymarginalizing\nexamplestoinduceanunderstandingofagiventask,unlike\nthesesampledchains. Thisapproachcapitalizesontheobser-\nzero-shotprompting,wherenoexamplesaresupplied[Brown\nvationthatproblemsrequiringthoughtfulanalysisoftenentail\net al., 2020]. Providing even a few high-quality examples\ngreaterreasoningdiversity,leadingtoasolution.Thecombina-\nhasimprovedmodelperformanceoncomplextaskscompared\ntionofself-consistencyandchain-of-thoughtpromptingresults\ntonodemonstration. However,few-shotpromptingrequires\nin significant accuracy improvements across various bench-\nadditionaltokenstoincludetheexamples,whichmaybecome\nmarks,suchas17.9%onGSM8K,11.0%onSVAMP,12.2%\nprohibitive for longer text inputs. Moreover, the selection\nonAQuA,6.4%onStrategyQA,and3.9%onARC-challenge\nandcompositionofpromptexamplescansignificantlyinflu-\ncomparedtothebaselinechain-of-thoughtprompting.\nencemodelbehavior,andbiaseslikefavoringfrequentwords\nmaystillaffectfew-shotresults. Whilefew-shotprompting LogicalChain-of-Thought(LogiCoT)Prompting\nenhances capabilities for complex tasks, especially among TheabilitytoperformlogicalreasoningiscriticalforLLMs\nlargepre-trainedmodelslikeGPT-3,carefulpromptengineer- tosolvecomplex,multi-stepproblemsacrossdiversedomains.\ning is critical to achieve optimal performance and mitigate Existingmethods,likeCoTprompting,encouragestep-by-step\nunintendedmodelbiases. reasoningbutlackeffectiveverificationmechanisms. [Zhao\netal.,2023]proposesaLogicalChain-of-Thought(LogiCoT)\n2.2 ReasoningandLogic\nprompting,aneurosymbolicframeworkthatleveragesprinci-\nChain-of-Thought(CoT)Prompting plesfromsymboliclogictoenhancereasoninginacoherent\nLLMsoftenstumbleinthefaceofcomplexreasoning, lim- andstructuredmanner. Specifically,LogiCoTappliesthecon-\niting their potential. Aiming to bridge this gap, [Wei et al., ceptofreductioadabsurdumtoverifyeachstepofreasoning"}
{"instruction": "Explain this concept:", "response": "Zero-shotPrompting[Radfordetal.,2019]\nNewTasksWithoutExtensive\nTraining\u00a72.1\nFew-shotPrompting[Brownetal.,2020]\nChain-of-Thought(CoT)Prompting[Weietal.,2022]\nAutomaticChain-of-Thought(Auto-CoT)[Zhangetal.,2022]\nSelf-Consistency[Wangetal.,2022]\nLogicalCoT(LogiCoT)Prompting[Zhaoetal.,2023]\nChain-of-Symbol(CoS)Prompting[Huetal.,2023]\nReasoningandLogic\u00a72.2\nTree-of-Thoughts(ToT)Prompting[Yaoetal.,2023a]\nGraph-of-Thought(GoT)Prompting[Yaoetal.,2023b]\nSystem2AttentionPrompting[WestonandSukhbaatar,2023]\nThreadofThought(ThoT)Prompting[Zhouetal.,2023]\nChainofTablePrompting[Wangetal.,2024]\nRetrievalAugmentedGeneration(RAG)[Lewisetal.,2020]\nReActPrompting[Yaoetal.,2022]\nReduceHallucination\u00a72.3 Chain-of-Verification(CoVe)[Dhuliawalaetal.,2023]\nPromptEngineering Chain-of-Note(CoN)Prompting[Yuetal.,2023]\nChain-of-Knowledge(CoK)Prompting[Lietal.,2023d]\nUserInteraction\u00a72.4 Active-Prompt[Diaoetal.,2023]\nFine-TuningandOptimization\u00a72.5 AutomaticPromptEngineer(APE)[Zhouetal.,2022]\nKnowledge-BasedReasoningand AutomaticReasoning\nGeneration\u00a72.6 andTool-use(ART)[Paranjapeetal.,2023]\nImprovingConsistency ContrastiveChain-of-Thought\nandCoherence\u00a72.7 Prompting(CCoT)[Chiaetal.,2023]\nManagingEmotionsandTone\u00a72.8 EmotionPrompting[Lietal.,2023a]\nScratchpadPrompting[Nyeetal.,2021]\nProgramofThoughts(PoT)Prompting[Chenetal.,2022]\nCodeGenerationandExecution\u00a72.9\nStructuredChain-of-Thought\n(SCoT)Prompting [Lietal.,2023c]\nChainofCode(CoC)Prompting[Lietal.,2023b]\nOptimizationandEfficiency\u00a72.10 OptimizationbyPrompting[Yangetal.,2023]\nUnderstandingUserIntent\u00a72.11 RephraseandRespond(RaR)Prompting[Dengetal.,2023]\nMetacognitionandSelf-Reflection\u00a72.12 TakeaStepBackPrompting[Zhengetal.,2023]\nFigure2:TaxonomyofpromptengineeringtechniquesinLLMs,organizedaroundapplicationdomains,providinganuancedframeworkfor\ncustomizingpromptsacrossdiversecontexts.\ngeneratedbythemodelandprovidetargetedfeedbacktorevise Chain-of-Symbol(CoS)Prompting\nincorrectsteps. LogiCoTcanreducelogicalerrorsandhallu-\nLLMsoftenstrugglewithtasksinvolvingcomplexspatialre-\ncinationsthroughathink-verify-reviseloop. Experimenting\nlationshipsduetotheirrelianceonnaturallanguage,whichis\nwith Vicuna-33b and GPT-4, the findings underscore Logi-\nsusceptibletoambiguityandbiases. Toovercomethislimita-\nCoT\u2019snotableenhancementofreasoningabilities,exhibiting tion,[Huetal.,2023]introducedCoS,employingcondensed\nimprovements of 0.16% and 1.42% on the GSM8K dataset\nsymbols instead of natural language. CoS provides distinct\nand3.15%and2.75%ontheAQuAdatasetcomparedtoCoT,\nadvantages:clearandconciseprompts,heightenedspatialrea-\nrespectively.\nsoningforLLMs,andimprovedhumaninterpretability. CoS\nsuffersfromchallengessuchasscalability,generalizability,in-"}
{"instruction": "Explain this concept:", "response": "tegration with other techniques, and interpretability of LLM mathwordproblems.InfactualQA,S2Aattainsanaccuracyof\nreasoningbasedonsymbols. Notably,theimplementationof 80.3%,demonstratingasubstantialenhancementinfactuality.\nCoSsignificantlyelevatesChatGPT\u2019sperformance,boosting Inlong-formgeneration,itimprovesobjectivityandreceivesa\naccuracyfrom31.8%toanimpressive92.6%onBrickWorld scoreof3.82outof5.\ntasks. Moreover, CoS achieves up to a 65.8% reduction in\nThreadofThought(ThoT)Prompting\nprompttokens,streamliningtheprocesswhilemaintaininghigh\naccuracy. [Zhou et al., 2023] presented Thread of Thought (ThoT), a\npromptingtechniquedesignedtoenhancethereasoningabil-\nTree-of-Thoughts(ToT)Prompting\nitiesofLLMswithinchaoticcontexts. ThoT,inspiredbyhu-\n[Yao et al., 2023a] and [Long, 2023] proposed the Tree-of- mancognition,systematicallyexaminesextensivecontextsinto\nThoughts(ToT)frameworktoenhancepromptingcapabilities manageable segments for incremental analysis, employing a\nforcomplextasksrequiringexplorationandlook-aheadreason- two-phaseapproachwheretheLLMfirstsummarizesandex-\ning.ToTextendsCoTpromptingbymanagingatreestructure amineseachsegmentbeforerefiningtheinformationforafinal\nof intermediate reasoning steps, known as \"thoughts\". Each response. ThoT\u2019s flexibility shines as a versatile \"plug-and-\nthought represents a coherent language sequence moving to- play\"module,enhancingreasoningacrossdifferentmodelsand\nwardthefinalsolution.Thisstructureallowslanguagemodels promptingmethods. Evaluationsonquestionansweringand\ntodeliberatelyreasonbyassessingtheprogressgeneratedby conversationdatasetsrevealsubstantialperformanceimprove-\nthoughtsinsolvingtheproblem.ToTintegratesthemodel\u2019sabil- mentsof47.20%and17.8%,respectively,especiallyinchaotic\nitiestoproduceandevaluatethoughtswithsearchalgorithms contexts.\nlike breadth-first or depth-first search. This enables system-\naticexplorationamongreasoningchains,withalook-aheadto Chain-of-TablePrompting\nexpandpromisingdirectionsandtobacktrackwhensolutions ApproacheslikeCoT,PoT,andToTrepresentreasoningsteps\nareincorrect.ToTexcelledintheGameof24tasks,achieving through free-form text or code, which face challenges when\na74%successratecomparedtoCoT\u2019s4%. Additionally, in dealingwithintricatetablescenarios. Thestudyby[Wanget\nword-leveltasks,ToToutperformedCoTwitha60%success al.,2024]introducedapioneeringpromptingtechniquenamed\nrateversus16%. Chain-of-Table.Thismethodusesstep-by-steptabularreason-\ningbydynamicallygeneratingandexecutingcommonSQL/-\nGraph-of-Thoughts(GoT)Prompting\nDataFrame operations on tables. The iterative nature of this\nTheinherentnon-linearnatureofhumanthoughtprocesseschal-\nprocessenhancesintermediateresults,empoweringLLMsto\nlengestheconventionalsequentialapproachofCoTprompt-\nmakepredictionsthroughlogicallyvisualizedreasoningchains.\ning.[Yaoetal., 2023b]introducedthe\"GraphofThoughts\"\nSignificantly,Chain-of-Tableconsistentlyimprovestheperfor-\nprompting,agraph-basedframeworkadvancingtraditionalse-\nmanceoftwobenchmarktabulardatasetsby8.69%onTabFact\nquentialmethodstobetteralignwiththenon-linearcharacter-\nand6.72%onWikiTQ,respectively.\nistics of human thinking. This framework permits dynamic\ninterplay,backtracking,andevaluationofideas,allowingthe 2.3 ReduceHallucination\naggregationandcombinationofthoughtsfromvariousbranches,\nRetrievalAugmentedGeneration(RAG)\ndepartingfromthelinearstructureofthetreeofthoughts.The\nkeycontributionsencompassmodelingthereasoningprocessas LLMshaverevolutionizedtextgeneration,yettheirreliance\nadirectedgraph,offeringamodulararchitecturewithdiverse on limited, static training data hinders accurate responses,\ntransformationoperations.Theframeworkispresentedasaver- especiallyintasksdemandingexternalknowledge.Traditional\nsatileanddynamicapproachtolanguagemodelprompting,cap- promptingfallsshort,requiringexpensiveretraining.Retrieval\nturingtheintricaciesofhumanthoughtprocessesandenhancing AugmentedGeneration(RAG)[Lewisetal.,2020]emerges\nmodel capabilities. The GoT reasoning model demonstrates asanovelsolution,seamlesslyweavinginformationretrieval\nsubstantialgainsovertheCoTbaseline, improvingaccuracy intothepromptingprocess. RAGanalyzesuserinput,crafts\nby3.41%withT5-baseand5.08%withT5-largeonGSM8K. atargetedquery, andscoursapre-builtknowledgebasefor\nIt also boosts accuracy over the state-of-the-art Multimodal- relevantresources.Retrievedsnippetsareincorporatedintothe\nCoT by 6.63% using T5-base and 1.09% with T5-large on originalprompt,enrichingitwithcontextualbackground. The\nScienceQA. augmentedpromptempowerstheLLMtogeneratecreative,\nfactuallyaccurateresponses. RAG\u2019sagilityovercomesstatic\nSystem2Attention(S2A)Prompting limitations,makingitagame-changerfortasksrequiringup-\nThesoftattentionmechanisminTransformer-basedLLMsis to-dateknowledge. RAGoutperformedseq2seqmodelsand\npronetoincorporatingirrelevantcontextinformation,impact- task-specificarchitecturesonODQAbenchmarks,achieving\ningtokengenerationadversely. Toaddressthis,[Westonand exactmatchscores,reachingupto56.8%onTriviaQAand\nSukhbaatar,2023]proposedSystem2Attention(S2A),utilizing 44.5%onNaturalQuestions.\nthereasoningabilitiesofLLMstoselectivelyattendtorelevant\nportions by regenerating the input context. S2A employs a ReActPrompting\ntwo-stepprocesstoenhanceattentionandresponsequalityby Unlikepreviousstudiesthattreatedreasoningandactionsep-\nemployingcontextregenerationandresponsegenerationwith arately, ReAct [Yao et al., 2022] enables LLMs to generate\nrefinedcontext. TheeffectivenessofS2Aisevaluatedacross reasoningtracesandtask-specificactionsconcurrently. This\nvarioustasks,includingfactualQA,long-formgeneration,and interleavedprocessenhancessynergybetweenreasoningand"}
{"instruction": "Explain this concept:", "response": "action,facilitatingthemodelininducing,tracking,andupdat- comprehensivereasoningpreparationstage,wherethecontext\ningactionplanswhilehandlingexceptions. ReActisapplied isestablished,andtheproblemisframed.Subsequently,iten-\nto diverse language and decision-making tasks, showcasing gagesinadynamicknowledgeadaptationphase,meticulously\nits effectiveness over state-of-the-art baselines. Notably, in gatheringevidencefromvarioussources,suchasitsinternal\nquestionanswering(HotpotQA)andfactverification(Fever), knowledgebase,externaldatabases,andthegivenprompt.\nReActaddresseshallucinationanderrorpropagationissuesby\n2.4 UserInterface\ninteractingwithasimpleWikipediaAPI,producingmoreinter-\npretabletask-solvingtrajectories. Additionally,ininteractive ActivePrompting\ndecision-making benchmarks like ALFWorld and WebShop, [Diaoetal.,2023]introducedActive-Promptingasasolution\nReActsurpassesbothimitationandreinforcementlearningap- tothechallengeofadaptingLLMstodiversereasoningtasks.\nproaches, achieving notable success rates of 34% and 10%, TheyaddresstheissuebyproposingActive-Prompttoenhance\nrespectively,withminimalin-contextexamples. LLMs\u2019 performance on complex question-and-answer tasks\nthroughtask-specificexamplepromptswithchain-of-thought\nChain-of-Verification(CoVe)Prompting\n(CoT) reasoning. Unlike existing CoT methods that rely on\nToaddresshallucinationsinLLMs,[Dhuliawalaetal.,2023]\nfixedsetsofhuman-annotatedexemplars,Active-Promptintro-\nproposedChain-of-Verification(CoVe),whichinvolvesasys-\nducesamechanismfordeterminingthemostimpactfulques-\ntematicfour-stepprocessincludingthemodelgeneratebaseline\ntions for annotation. Drawing inspiration from uncertainty-\nresponses,planverificationquestionstocheckitswork,answer\nbased active learning, the method utilizes various metrics to\nthequestionsindependently, andproducearevisedresponse\ncharacterizeuncertaintyandselectsthemostuncertainques-\nincorporatingtheverification. Byverifyingitsworkthrough\ntionsforannotation. Active-Promptingexhibitssuperiorper-\nthis deliberate multi-step approach, the LLM enhances logi-\nformance, outperforming self-consistency by an average of\ncalreasoningabilitiesandreduceserrorsevenwithcontradic-\n7.0%and1.8%acrosseightcomplexreasoningtasksintext-\ntoryinformation.CoVeemulateshumanverificationtobolster\ndavinci-002 and code-davinci-002, respectively, showcasing\nthecoherenceandprecisionofLLMoutput. Experimentson\nstate-of-the-artresults.\nlistquestions,QA,andlong-formgenerationdemonstratethat\nCoVe decreases hallucinations while maintaining facts. Fo- 2.5 Fine-TuningandOptimization\ncusedverificationquestionshelpmodelsidentifyandcorrect\nAutomaticPromptEngineer(APE)\ntheirinaccuracies.\nWhilecraftingeffectivepromptsforLLMshastraditionally\nChain-of-Note(CoN)Prompting been a laborious task for expert annotators, [Zhou et al.,\n2022] introduced Automatic Prompt Engineer (APE) as an\nRetrieval-augmented language models (RALMs) enhance\ninnovativeapproachtoautomaticinstructiongenerationand\nlargelanguagemodelsbyincorporatingexternalknowledge\nselectionforLLMs. APEshedsthelimitationsofstatic,hand-\nto reduce factual hallucination. However, the reliability of\ndesigned prompts by dynamically generating and selecting\nretrievedinformationisnotguaranteed,leadingtopotentially\nthemostimpactfulpromptsforspecifictasks. Thisingenious\nmisguidedresponses.StandardRALMsstruggletoassesstheir\nmethodanalyzesuserinput,craftscandidateinstructions,and\nknowledgeadequacyandoftenfailtorespondwith\"unknown\"\nwhenlackinginformation. Toaddressthesechallenges,[Yu thenleveragesreinforcementlearningtochoosetheoptimal\netal.,2023]introducedanovelapproachtoimproveRALMs prompt,adaptingitontheflytodifferentcontexts. Extensive\ntests on the diverse BIG-Bench suite and the CoT reason-\nrobustness by handling noisy, irrelevant documents and ac-\ningtaskrevealedAPE\u2019sprowess,exceedinghuman-authored\ncuratelyaddressingunknownscenarios. CoNsystematically\npromptsinmostcases(19outof24tasks)andsignificantly\nevaluates document relevance, emphasizing critical and re-\nboostingLLMsreasoningabilities. Thisbreakthroughinauto-\nliable information to filter out irrelevant content, resulting\nmaticpromptengineeringpavesthewayforLLMstotacklea\nin more precise and contextually relevant responses. Test-\nwiderrangeoftaskswithgreaterefficiencyandadaptability,\ningacrossdiverseopen-domainquestion-answeringdatasets\nunlockingtheirfullpotentialacrossdiverseapplications.\ndemonstratednotableimprovements,includinga+7.9average\nboostinexactmatchscoresfornoisyretrieveddocumentsand\n2.6 Knowledge-BasedReasoningandGeneration\na+10.5enhancementinrejectionratesforquestionsbeyond\nAutomaticReasoningandTool-use(ART)\npre-trainingknowledge.\nThelimitedreasoningabilitiesandlackofexternaltoolutiliza-\nChain-of-Knowledge(CoK)Prompting tionhinderthepotentialofLLMsincomplextasks.[Paranjape\nTraditionalpromptingtechniquesforLLMshaveprovenpower- etal.,2023]introducedAutomaticReasoningandTool-use\nfulintacklingbasictasks.However,theirefficacydiminishes (ART)totacklethiscriticalbarrierthatempowersLLMsto\nduetocomplexreasoningchallenges,oftenresultinginunre- reasonthroughmulti-stepprocessesandseamlesslyintegrate\nliable outputs plagued by factual hallucinations and opaque externalexpertise. ARTbridgesthereasoninggap,enabling\nthought processes. This limitation arises from their reliance LLMstotacklecomplexproblemsandexpandbeyondsimple\nonfixedknowledgesources,ineffectivestructuredquerygen- textgeneration. Byintegratingexternaltoolsforspecialized\neration,andlackofprogressivecorrectionthatfailstoguide knowledge and computations, ART unlocks unprecedented\nthe LLMadequately. Motivated by human problem-solving, versatilityandinformsLLMoutputswithreal-worldrelevance.\nCoK [Li et al., 2023d] systematically breaks down intricate ThisallowsLLMstocontributetodiversefieldslikescientific\ntasksintowell-coordinatedsteps.Theprocessinitiateswitha research, data analysis, and even decision-making support."}
{"instruction": "Explain this concept:", "response": "Moving beyond traditional prompting techniques, ART au- Promptingtechniqueoutperforms(MostlyBasicPythonPro-\ntomatesreasoningstepsthroughstructuredprograms,elimi- gramming)MBPP-augwitha46.8%successrate.Combining\nnatingtheneedforlaborioushand-crafting. Itsdynamictool CodeNetandsingle-linedatasetsyieldsthehighestperformance,\nintegrationensuressmoothcollaboration,pausinggeneration achieving26.6%correctfinaloutputsand24.6%perfecttraces.\ntoincorporateexternaltooloutputsandseamlesslyresuming Scratchpadpromptingtechniquefaceslimitations,includinga\ntheflow. Empiricalevidenceonchallengingbenchmarks(Big- fixedcontextwindowsizeof512tokensandadependencyon\nBench and MMLU) demonstrates ART\u2019s effectiveness, sur- supervisedlearningforscratchpadutilization.\npassingtraditionalpromptingandevenmatchinghand-crafted\nProgramofThoughts(PoT)Prompting\ndemonstrationsinsomecases.\nLanguagemodelsaresuboptimalforsolvingmathematicalex-\n2.7 ImprovingConsistencyandCoherence pressions due to their proneness to arithmetic errors, incapa-\nbilityinhandlingcomplexequations, andinefficiencyinex-\nContrastiveChain-of-Thought(CCoT)Prompting\npressingextensiveiterations.Toenhancenumericalreasoning\nTraditionalCoTpromptingforLLMsoftenmissesacrucial\nin language models, [Chen et al., 2022] presents Program-\nelement: learningfrommistakes. ThatiswhereContrastive\nof-Thoughts(PoT)prompting,advocatingtheuseofexternal\nChain-of-ThoughtPrompting(CCoT)[Chiaetal.,2023]dives\nlanguageinterpretersforcomputationsteps.PoTenablesmod-\nin,providingbothvalidandinvalidreasoningdemonstrations\nelslikeCodextoexpressreasoningthroughexecutablePython\nalongsideoriginalprompts. Imagineexploringamapwiththe\nprograms,resultinginanaverageperformanceimprovementof\nrightpathandthewrongturnstoavoid\u2013thatistheadvantage\napproximately12%comparedtoCoTpromptingondatasets\nofcontrastiveCoT!Thisdual-perspectiveapproach,testedon\ninvolvingmathematicalwordproblemsandfinancialquestions.\nreasoningbenchmarkslikeSQuADandCOPA,pushesLLMs\ntostep-by-stepreasoning,leadingto4-16%improvementsin StructuredChain-of-Thought(SCoT)Prompting\nstrategicandmathematicalreasoningevaluationscompared LLMshaveexhibitedimpressiveproficiencyincodegener-\nto traditional CoT, further improved by approximately 5% ation. The widely used CoT prompting involves producing\nwhenintegratedwithself-consistencytechniques. However, intermediatenaturallanguagereasoningstepsbeforegenerat-\nquestionsremainaboutthistechnique,suchastheautomated ingcode. Despiteitsefficacyinnaturallanguagegeneration,\ngenerationofcontrastingdemonstrationsfordiverseproblems CoT prompting demonstrates lower accuracy when applied\nanditsapplicabilitytootherNLPtasksbeyondreasoning. to code generation tasks. [Li et al., 2023c] introduce Struc-\nturedChain-of-Thought(SCoTs)asaninnovativeprompting\n2.8 ManagingEmotionsandTone technique tailored specifically for code generation. By in-\nEmotionPrompting corporatingprogramstructures(sequence,branch,andloop\nWhileLLMsdemonstrateimpressivecapabilitiesonvarious structures) into reasoning steps, SCoT prompting enhances\ntasks,theirabilitytocomprehendpsychologicalandemotional LLMs\u2019performanceingeneratingstructuredsourcecode.This\ncues remains uncertain. The study by [Li et al., 2023a] ad- approach explicitly guides LLMs to consider requirements\ndressedtheuncertaintysurroundingLLMs\u2019abilitytocompre- fromthesourcecodeperspective,improvingtheiroverallef-\nhendemotionalcuesbyintroducingEmotionPrompt.Drawing fectivenessincodegenerationcomparedtoCoTprompting.\ninspirationfrompsychologicalresearchonlanguage\u2019simpact TheauthorsvalidatedtheeffectivenessofSCoTonChatGPT\nonhumanperformance,theyappend11emotionalstimulus andCodexacrossthreebenchmarks(HumanEval,MBPP,and\nsentencestopromptstoenhanceLLMemotionalintelligence. MBCPP)anddemonstratedasuperiorperformanceoverthe\nExperimentalresultsdemonstrateseamlessintegrationofthese CoTpromptingbyupto13.79%.\nstimuli,significantlyimprovingLLMperformanceacrossvar- Chain-of-Code(CoC)Prompting\nioustasks. EmotionPromptdemonstratesan8.00%relative WhileCoTpromptinghasprovenveryeffectiveforenhancing\nimprovementininstructioninductionandanimpressive115% Languagemodels(LMs)semanticreasoningskills,itstrug-\nboost in BIG-Bench tasks, underscoring its efficacy in aug- glestohandlequestionsrequiringnumericorsymbolicreason-\nmentingLLMcapabilitiesinprocessingaffectivesignals. An ing.[Lietal.,2023b]introduceChain-of-Code(CoC)asan\nevaluationinvolving106participantsrevealsanaverageim- extensiontoimproveLMreasoningbyleveragingcodewriting\nprovementof10.9%inperformance,truthfulness,andrespon- forbothlogicandsemantictasks. CoCencouragesLMsto\nsibilitymetricsforgenerativetaskswhenemployingEmotion- formatsemanticsub-tasksasflexiblepseudocode,allowing\nPromptcomparedtostandardprompts. aninterpretertocatchundefinedbehaviorsandsimulatethem\nwithan\"LMulator.\"ExperimentsdemonstrateCoC\u2019ssuperior-\n2.9 CodeGenerationandExecution\nityoverChainofThoughtandotherbaselines,achievingan\nScratchpadPrompting 84%accuracyonBIG-BenchHard,a12%gain. CoCproves\nDespitetheprowessofTransformer-basedlanguagemodelsin effectivewithbothlargeandsmallmodels,expandingLMs\u2019\ngeneratingcodeforbasicprogrammingtasks,theyencounter abilitytocorrectlyanswerreasoningquestionsbyincorporat-\nchallengesincomplex,multi-stepalgorithmiccalculationsre- inga\"thinkincode\"approach.\nquiringprecisereasoning. Addressingthis,[Nyeetal.,2021]\nintroduceanovelapproach,centeredontaskdesignratherthan 2.10 OptimizationandEfficiency\nmodelmodification,introducea\u2018scratchpad\u2019concept.Thepro- OptimizationbyPrompting(OPRO)\nposalenablesthemodeltogenerateanarbitrarysequenceofin- In various domains, optimization is a fundamental process\ntermediatetokensbeforeprovidingthefinalanswer.Scratchpad ofteninvolvingiterativetechniques. [Yangetal.,2023]intro-"}
{"instruction": "Explain this concept:", "response": "Table1:SummaryofprevalentpromptingtechniquesofLLMsbasedonthefollowingfactors:application,promptacquisition,promptturn,\nlanguagemodel,dataset,andmetrics.\nPrompting ComparisonScope\nApplication\nTechnique PromptAcquisition PromptTurn LanguageModel(s) Dataset Metric(s)\nNewTasksWithout\nTrainingData Zero-shot Manual Single GPT-2 Arithmetic,Symbolic Accuracy,ROUGEScore\nFew-shot Manual Single GPT-3 NaturalQS,WebQS,TriviaQA Accuracy\nCoT Manual Multi PaLM540B GSM8K Accuracy\nLogiCoT Manual Multi Vicuna-33b,GPT-4 GSM8K,AQuA,SocialQA Accuracy\nCoS Manual Multi gpt-3.5-turbo,GPT-4 SPARTUN Accuracy,Precision,Recall\nAuto-CoT LMGenerated Multi GPT-3 Arithmetic,Symbolic Accuracy\nSelf-Consistency Manual Single PaLM540B Arithmetic,Commonsense Accuracy\nReasoningandLogic ToT RetrievalBased Multi GPT-4 Gameof24,CreativeWriting SuccessRate\nGoT RetrievalBased Multi T5-large GSM8K,ScienceQA ROUGEScore\nS2A Manual Single Llama2-70B QA,GSM8K Accuracy\nThoT Hybrid Multi gpt-3.5-turbo,Llama2-70b-chat PopQA,EntityQ,MTCR ExactMatch(EM)Score\nChainofTable Manual Multi GPT3.5,LLaMA2 TabFact,WikiTQ BLEU,ROUGEScore\nCoVe RetrievalBased Multi Llama65B Wikidata,QUEST,MultiSpanQA Precision,F1\nReAct RetrievalBased Multi PaLM-540B,GPT-3 HotpotQA,FEVER ExactMatch(EM),Accuracy\nReduceHallucination RAG RetrievalBased Single RAG-Token,RAG-Seq. MSMARCO,SearchQA ROUGE,BLEUscore\nCoN LMGenerated Multi Llama2,DPR NQ,TriviaQA,WebQ ExactMatch(EM),F1Score\nCoK LMGenerated Multi gpt-3.5-turbo-0613 Ho Mtp Mot LQ UA, PF hE ysV icE sR a, nM de Bd iM oloC gQ yA, ExactMatch(EM),Accuracy\nUserInteraction Active-Prompt Manual Single code-davinci-002,text-davinci-003 Arithmetic,Commonsense,Symbolic VariaD ni cs ea ,g Sre ee lfm -ce on nt, fiE dn entr co ep Sy core\nFi One p- tT imun izin atg ioa nnd APE LMGenerated Single text-curie-001,text-davanci-002 BBII,TruthfulQA Execu Eti fo fin cia ec nc tu sr ca oc ry e,L eso tg imp ar to iob nability,\nKnowledge-Based\nReasoningandGeneration ART Hybrid Multi GPT-3(175B) BigBench,MMLU Accuracy\nImpr ao nv din Cg oC heo rn es ni cst eency CCoT LMGenerated Multi gpt-3.5-turbo-0301 Arithmetic,FactualQA Accuracy\nManagingEmotions\nandTone EmotionPrompting Manual Single GPT-4 BIG-Bench,InstructionInduction Accuracy\nSCoT Hybrid Multi ChatGPT,Codex HumanEval,MBPP,MBCPP pass@k\nC ao nd de EG xe en ce ur ta it oi non PoT Manual Single gpt-3.5-turbo GSM8K,SVAMP,FinQA ExactMatch(EM)Score\nCoC Manual Single text-davinci-003,gpt-3.5-turbo BIG-BenchHard Accuracy\nScratchpadPrompting Manual Single GPT-3 MBPP,MBPP-aug Accuracy\nOptimizationand\nEfficiency OPRO Manual Single PaLM2-L-IT,text-bison GSM8K,BIG-BenchHard Accuracy\nU Und se er rs Ita nn ted nin tg RaR Manual Single GPT-4-0613 Knowledge,Symbolic LanA gc uc au gr eay M,F oa di er liS nc go Sre c, ore\nMetacognition MMLU-Physics,MMLU-Chemistry\nandSelf-Reflection TakeaStepBack Manual Single PaLM2-L,GPT-4 TimeQA,SituatedQA,StrategyQA Accuracy\nduceOptimizationbyPROmpting(OPRO),anovelapproach ous tasks. The study highlights that in contrast to casually\nthatleveragesLLMsasoptimizers. Unliketraditionalmeth- posedhumanqueries,therephrasedquestionscontributetoen-\nods, OPRO utilizes natural language prompts to iteratively hancedsemanticclarityandtheresolutionofinherentambiguity.\ngeneratesolutionsbasedontheproblemdescription,enabling Thesefindingsoffervaluableinsightsforunderstandingand\nquickadaptationtodifferenttasksandcustomizationofthe enhancingtheefficacyofLLMsacrossvariousapplications.\noptimizationprocess. ThepotentialofLLMsforoptimization\nisdemonstratedthroughcasestudiesonclassicproblemslike 2.12 MetacognitionandSelf-Reflection\nlinearregressionandthetravelingsalesmanproblem. Addi- TakeaStepBackPrompting\ntionally,itexplorestheoptimizationofpromptstomaximize\nAddressingthepersistentchallengeofcomplexmulti-steprea-\naccuracy in natural language processing tasks, highlighting\nsoning,[Zhengetal.,2023]introducedtheStep-Backprompt-\nthesensitivityofLLMs. Theexperimentsshowthatoptimiz-\ningtechnique,tailoredexplicitlyforadvancedlanguagemodels\ningpromptsforaccuracyonasmalltrainingseteffectively\nlikePaLM-2L.Thisinnovativeapproachempowersmodelsto\ntranslates to high performance on the test set. OPRO leads\nengageinabstraction,extractinghigh-levelconceptsandfun-\nto a significant performance boost, with the most effective\ndamental principles from specific instances. The Step-Back\npromptsoptimizedbyOPROoutperforminghuman-designed\npromptingmethodinvolvesatwo-stepprocedure,integrating\npromptsbyupto8%ontheGSM8Kdatasetandupto50%\nAbstractionandReasoning.Throughextensiveexperiments,ap-\nonchallengingtasksinBig-Bench.\nplyingStep-BackPromptingtoPaLM-2Lindiversereasoning-\nintensivetaskssuchasSTEM,KnowledgeQA,andMulti-Hop\n2.11 UnderstandingUserIntent\nReasoning,theresultsdemonstrateasubstantialenhancement\nRephraseandRespond(RaR)Prompting\ninreasoningcapabilities.Noteworthyperformanceboostsare\nThestudyby[Dengetal.,2023]bringsattentiontoanoften-\nobserved,withimprovementsintaskslikeMMLUPhysicsand\nneglecteddimensioninexploringLLMs:thedisparitybetween Chemistryby7%,TimeQAby27%,andMuSiQueby7%.\nhuman thought frames and those of LLMs and introduces\nRephraseandRespond(RaR).RaRallowsLLMstorephrase\n3 Conclusion\nand expand questions in a single prompt, demonstrating im-\nprovedcomprehensionandresponseaccuracy. Thetwo-step Inthedomainofartificialintelligence,promptengineeringhas\nRaR variant, incorporating rephrasing and response LLMs, become a transformative force, unlocking the vast potential\nachieves substantial performance enhancements across vari- ofLLMs. Thissurveypaperaimstoserveasafoundational"}
{"instruction": "Explain this concept:", "response": "resourcethatsystematicallycategorizes29distinctpromptengi- [Huetal.,2023] HanxuHu,HongyuanLu,HuajianZhang,\nneeringtechniquesbasedontheirtargetedfunctionalities,inspir- Yun-ZeSong,WaiLam,andYueZhang. Chain-of-symbol\ningfurtherresearchandempoweringinnovatorsintheevolving promptingelicitsplanninginlargelangaugemodels,2023.\nlandscapeofpromptengineering.Theanalysisspansapplica-\n[Lewisetal.,2020] Patrick Lewis, Ethan Perez, Aleksan-\ntions,models,anddatasets,sheddinglightonthestrengthsand\ndra Piktus, Fabio Petroni, Vladimir Karpukhin, Naman\nlimitationsofeachapproach. Furthermore,wehaveaddeda\nGoyal, HeinrichK\u00fcttler, MikeLewis, Wen-tauYih, Tim\ndiagramandatabletohighlighttheimportantpoints.Despite\nRockt\u00e4schel, et al. Retrieval-augmented generation for\ntheremarkablesuccesses,challengespersist,includingbiases,\nknowledge-intensivenlptasks. AdvancesinNeuralInfor-\nfactual inaccuracies, and interpretability gaps, necessitating\nmationProcessingSystems,33:9459\u20139474,2020.\nfurtherinvestigationandmitigationstrategies. Thefutureof\npromptengineeringholdsimmensepotential,withemerging [Lietal.,2023a] Cheng Li, Jindong Wang, Yixuan Zhang,\ntrendslikemeta-learningandhybridpromptingarchitectures KaijieZhu,WenxinHou,JianxunLian,FangLuo,Qiang\npromisingamplifiedcapabilities. However,ethicalconsidera- Yang,andXingXie. Largelanguagemodelsunderstand\ntionsareparamount,emphasizingresponsibledevelopmentand andcanbeenhancedbyemotionalstimuli. arXivpreprint\ndeploymenttoensurepositiveintegrationintoourlives. arXiv:2307.11760,2023.\n[Lietal.,2023b] Chengshu Li, Jacky Liang, Andy Zeng,\nReferences\nXinyun Chen, Karol Hausman, Dorsa Sadigh, Sergey\n[Bahngetal.,2022] Hyojin Bahng, Ali Jahanian, Swami Levine, Li Fei-Fei, Fei Xia, and Brian Ichter. Chain of\nSankaranarayanan, and Phillip Isola. Exploring visual code: Reasoningwithalanguagemodel-augmentedcode\npromptsforadaptinglarge-scalemodels. arXivpreprint emulator. arXivpreprintarXiv:2312.04474,2023.\narXiv:2203.17274,2022. [Lietal.,2023c] Jia Li, Ge Li, Yongmin Li, and Zhi Jin.\n[Brownetal.,2020] TomB.Brown,BenjaminMann,Nick Structuredchain-of-thoughtpromptingforcodegeneration.\nRyder,MelanieSubbiah,JaredKaplan,PrafullaDhariwal, arXivpreprintarXiv:2305.06599,2023.\nArvindNeelakantan,PranavShyam,GirishSastry,Amanda\n[Lietal.,2023d] Xingxuan Li, Ruochen Zhao, Yew Ken\nAskell, SandhiniAgarwal, ArielHerbert-Voss, Gretchen\nChia,BoshengDing,ShafiqJoty,SoujanyaPoria,andLi-\nKrueger, Tom Henighan, Rewon Child, Aditya Ramesh,\ndong Bing. Chain-of-knowledge: Grounding large lan-\nDanielM.Ziegler,JeffreyWu,ClemensWinter,Christo-\nguagemodelsviadynamicknowledgeadaptingoverhet-\npherHesse,MarkChen,EricSigler,MateuszLitwin,Scott\nerogeneoussources,2023.\nGray, Benjamin Chess, Jack Clark, Christopher Berner,\nSamMcCandlish,AlecRadford,IlyaSutskever,andDario [Liuetal.,2023] Pengfei Liu, Weizhe Yuan, Jinlan Fu,\nAmodei. Languagemodelsarefew-shotlearners,2020. Zhengbao Jiang, Hiroaki Hayashi, and Graham Neubig.\nPre-train, prompt, and predict: A systematic survey of\n[Chenetal.,2022] WenhuChen,XueguangMa,XinyiWang,\npromptingmethodsinnaturallanguageprocessing. ACM\nandWilliamWCohen. Programofthoughtsprompting:\nComputingSurveys,55(9):1\u201335,2023.\nDisentanglingcomputationfromreasoningfornumerical\nreasoningtasks. arXivpreprintarXiv:2211.12588,2022. [Long,2023] JieyiLong. Largelanguagemodelguidedtree-\nof-thought. arXivpreprintarXiv:2305.08291,2023.\n[Chenetal.,2023] BanghaoChen,ZhaofengZhang,Nicolas\nLangren\u00e9,andShengxinZhu. Unleashingthepotentialof [Nyeetal.,2021] MaxwellNye,AndersJohanAndreassen,\npromptengineeringinlargelanguagemodels: acompre- GuyGur-Ari,HenrykMichalewski,JacobAustin,David\nhensivereview. arXivpreprintarXiv:2310.14735,2023. Bieber,DavidDohan,AitorLewkowycz,MaartenBosma,\n[Chiaetal.,2023] YewKenChia,GuizhenChen,LuuAnh DavidLuan,etal. Showyourwork: Scratchpadsforinter-\nmediatecomputationwithlanguagemodels. arXivpreprint\nTuan,SoujanyaPoria,andLidongBing. Contrastivechain-\narXiv:2112.00114,2021.\nof-thoughtprompting. arXivpreprintarXiv:2311.09277,\n2023. [Paranjapeetal.,2023] BhargaviParanjape,ScottLundberg,\n[Dengetal.,2023] Yihe Deng, Weitong Zhang, Zixiang SameerSingh,HannanehHajishirzi,LukeZettlemoyer,and\nChen,andQuanquanGu. Rephraseandrespond: Letlarge MarcoTulioRibeiro. Art: Automaticmulti-stepreasoning\nlanguagemodelsaskbetterquestionsforthemselves. arXiv and tool-use for large language models. arXiv preprint\npreprintarXiv:2311.04205,2023. arXiv:2303.09014,2023.\n[Dhuliawalaetal.,2023] Shehzaad Dhuliawala, Mojtaba [Radfordetal.,2019] Alec Radford, Jeffrey Wu, Rewon\nKomeili, Jing Xu, Roberta Raileanu, Xian Li, Asli Ce- Child, David Luan, Dario Amodei, Ilya Sutskever, et al.\nlikyilmaz,andJasonWeston. Chain-of-verificationreduces Languagemodelsareunsupervisedmultitasklearners.Ope-\nhallucination in large language models. arXiv preprint nAIblog,1(8):9,2019.\narXiv:2309.11495,2023.\n[Tonmoyetal.,2024] SMTonmoy,SMZaman,VinijaJain,\n[Diaoetal.,2023] Shizhe Diao, Pengcheng Wang, Yong Anku Rani, Vipula Rawte, Aman Chadha, and Amitava\nLin, and Tong Zhang. Active prompting with chain- Das. A comprehensive survey of hallucination mitiga-\nof-thought for large language models. arXiv preprint tiontechniquesinlargelanguagemodels. arXivpreprint\narXiv:2302.12246,2023. arXiv:2401.01313,2024."}
{"instruction": "Explain this concept:", "response": "[Wangetal.,2022] XuezhiWang, JasonWei, DaleSchuur- viaabstractioninlargelanguagemodels. arXivpreprint\nmans,QuocLe,EdChi,SharanNarang,AakankshaChowd- arXiv:2310.06117,2023.\nhery,andDennyZhou. Self-consistencyimproveschain\n[Zhouetal.,2022] YongchaoZhou,AndreiIoanMuresanu,\nofthoughtreasoninginlanguagemodels. arXivpreprint\nZiwenHan,KeiranPaster,SilviuPitis,HarrisChan,and\narXiv:2203.11171,2022.\nJimmyBa. Largelanguagemodelsarehuman-levelprompt\n[Wangetal.,2024] Zilong Wang, Hao Zhang, Chun-Liang engineers. arXivpreprintarXiv:2211.01910,2022.\nLi,JulianMartinEisenschlos,VincentPerot,ZifengWang, [Zhouetal.,2023] Yucheng Zhou, Xiubo Geng, Tao Shen,\nLeslyMiculicich,YasuhisaFujii,JingboShang,Chen-Yu\nChongyangTao,GuodongLong,Jian-GuangLou,andJian-\nLee,andTomasPfister. Chain-of-table: Evolvingtablesin\nbingShen. Threadofthoughtunravelingchaoticcontexts.\nthereasoningchainfortableunderstanding,2024. arXivpreprintarXiv:2311.08734,2023.\n[Weietal.,2022] Jason Wei, Xuezhi Wang, Dale Schuur-\nmans,MaartenBosma,FeiXia,EdChi,QuocVLe,Denny\nZhou,etal. Chain-of-thoughtpromptingelicitsreasoning\ninlargelanguagemodels. AdvancesinNeuralInformation\nProcessingSystems,35:24824\u201324837,2022.\n[WestonandSukhbaatar,2023] JasonWestonandSainbayar\nSukhbaatar. System2attention(issomethingyoumight\nneedtoo). arXivpreprintarXiv:2311.11829,2023.\n[Wuetal.,2023] ChenfeiWu,ShengmingYin,WeizhenQi,\nXiaodong Wang, Zecheng Tang, and Nan Duan. Visual\nchatgpt: Talking,drawingandeditingwithvisualfounda-\ntionmodels,2023.\n[Yangetal.,2023] Chengrun Yang, Xuezhi Wang, Yifeng\nLu, Hanxiao Liu, Quoc V Le, Denny Zhou, and Xinyun\nChen. Largelanguagemodelsasoptimizers. arXivpreprint\narXiv:2309.03409,2023.\n[Yaoetal.,2022] ShunyuYao,JeffreyZhao,DianYu,Nan\nDu,IzhakShafran,KarthikNarasimhan,andYuanCao.Re-\nact: Synergizingreasoningandactinginlanguagemodels.\narXivpreprintarXiv:2210.03629,2022.\n[Yaoetal.,2023a] ShunyuYao,DianYu,JeffreyZhao,Izhak\nShafran, Thomas L Griffiths, Yuan Cao, and Karthik\nNarasimhan. Tree of thoughts: Deliberate problem\nsolving with large language models. arXiv preprint\narXiv:2305.10601,2023.\n[Yaoetal.,2023b] YaoYao,ZuchaoLi,andHaiZhao. Be-\nyond chain-of-thought, effective graph-of-thought rea-\nsoning in large language models. arXiv preprint\narXiv:2305.16582,2023.\n[Yuetal.,2023] Wenhao Yu, Hongming Zhang, Xiaoman\nPan, Kaixin Ma, Hongwei Wang, and Dong Yu. Chain-\nof-note: Enhancingrobustnessinretrieval-augmentedlan-\nguagemodels,2023.\n[Zhangetal.,2022] ZhuoshengZhang,AstonZhang,MuLi,\nandAlexSmola. Automaticchainofthoughtpromptingin\nlargelanguagemodels. arXivpreprintarXiv:2210.03493,\n2022.\n[Zhaoetal.,2023] Xufeng Zhao, Mengdi Li, Wenhao Lu,\nCornelius Weber, Jae Hee Lee, Kun Chu, and Stefan\nWermter. Enhancing zero-shot chain-of-thought reason-\ninginlargelanguagemodelsthroughlogic,2023.\n[Zhengetal.,2023] HuaixiuStevenZheng,SwaroopMishra,\nXinyun Chen, Heng-Tze Cheng, Ed H Chi, Quoc V Le,\nand Denny Zhou. Take a step back: evoking reasoning"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering or Fine-Tuning:\nAn Empirical Assessment of LLMs for Code\nJiho Shin\u2217, Clark Tang, Tahmineh Mohati\u2020, Maleknaz Nayebi\u2217, Song Wang\u2217, Hadi Hemmati\u2217\u2020\n\u2217 Lassonde School of Engineering, York University, Toronto, Canada\n\u2020 Schulich School of Engineering, University of Calgary, Calgary, Canada\n{jihoshin, mnayebi, wangsong, hemmati}@yorku.ca, clark.f.tang@gmail.com, tahmineh.mohati1@ucalgary.ca\nAbstract\u2014The rapid advancements in large language models still in its early stages and lacks systematic studies on its\n(LLMs) have greatly expanded the potential for automated performance compared to fine-tuned models in code tasks.\ncode-related tasks. Two primary methodologies are used in this This paper presents a quantitative and qualitative investi-\ndomain:promptengineeringandfine-tuning.Promptengineering\ngation of OpenAI\u2019s ChatGPT, specifically the latest version\ninvolves applying different strategies to query LLMs, like Chat-\nGPT, while fine-tuning further adapts pre-trained models, such (GPT-4),whichhasshownnotableimprovementsoveritspre-\nas CodeBERT, by training them on task-specific data. Despite decessor, GPT-3.5. We focus on three automated code tasks:\nthe growth in the area, there remains a lack of comprehensive codesummarization(SC-to-NL),codegeneration(NL-to-SC),\ncomparative analysis between the approaches for code models.\nand code translation (SC-to-SC). These tasks are chosen for\nInthispaper,weevaluateGPT-4usingthreepromptengineer-\ntheir commonality among developers and prevalence in the\ning strategies\u2014basic prompting, in-context learning, and task-\nliterature. We employ three automated prompting techniques\nspecificprompting\u2014andcompareitagainst17fine-tunedmodels\nacrossthreecode-relatedtasks:codesummarization,generation, (basic, in-context learning, and task-specific prompts) and\nand translation. Our results indicate that GPT-4 with prompt compare them with 17 fine-tuned language models. Addition-\nengineering does not consistently outperform fine-tuned models. ally, we surveyed 27 graduate students and 10 industry prac-\nForinstance,incodegeneration,GPT-4isoutperformedbyfine-\ntitioners to gather qualitative insights through conversational\ntuned models by 28.3% points on the MBPP dataset. It also\nprompts. We address the following research questions:\nshows mixed results for code translation tasks.\nAdditionally,auserstudywasconductedinvolving27graduate RQ1:HowdoesGPT-4withautomatedpromptingcompare\nstudents and 10 industry practitioners. The study revealed to fine-tuned models in performance? We quantitatively\nthat GPT-4 with conversational prompts, incorporating human assess GPT-4 using three prompting strategies: (a) basic\nfeedbackduringinteraction,significantlyimprovedperformance\nprompt, (b) in-context learning, and (c) task-specific prompt,\ncompared to automated prompting. Participants often provided\ncomparing results with fine-tuned language models.\nexplicit instructions or added context during these interactions.\nThese findings suggest that GPT-4 with conversational prompt- RQ2:HowdoparticipantsperceivetheusefulnessofGPT-4\ning holds significant promise for automated code-related tasks, usingabasicprompt?Weaskparticipantstoassessthebasic\nwhereas fully automated prompt engineering without human prompting strategy qualitatively to understand its usefulness.\ninvolvement still requires further investigation.\nRQ3: How do participants refine their prompts when\nIndex Terms\u2014Prompt engineering, Fine-tuning, LLM4SE,\ninteracting with GPT-4? We investigate how participants\nEmpirical study, Survey.\nrefine prompts to achieve better results by analyzing their\ninteraction logs and summarizing prompt evolution patterns.\nI. INTRODUCTION\nRQ4: What is the impact of different prompt evolution\nLanguage models have gained significant interest, leading patterns? We investigate the impact of prompt evolution\nto numerous studies adapting their use for automated code- patterns derived from participant interactions.\nrelated tasks. Although originally developed for natural lan- Our quantitative analysis suggests that prompt-engineered\nguage,LargeLanguageModels(LLMs)demonstratepotential GPT-4 does not consistently outperform fine-tuned LLMs in\nin automated code-related tasks. Previous literature mainly all tasks. For code summarization, GPT-4 with task-specific\nfocuses on pre-training on large corpora of Source Code (SC) prompting outperforms the top fine-tuned model by 8.33%\nand Natural Language (NL) and fine-tuning with task-specific points in the BLEU score. In code generation, GPT-4 outper-\ndatasetsforvarioustasks.State-of-the-artLLMs,however,are forms fine-tuned models by 8.59% points on the HumanEval\ntrained using massive unsupervised corpora and use prompts dataset. However, it is outperformed by fine-tuned models\nfor task querying. The large training corpus and numerous by 28.3% points on the MBPP dataset. In code translation,\nparameters enable LLMs to perform well on automated code- GPT-4 and fine-tuned models also show mixed results. Anal-\nrelated tasks with simple prompts. ysis of participant interactions reveals significant performance\nPrompting offers advantages over fine-tuning: it does not improvements when using conversational prompts compared\nrequire labelled datasets, which are costly to acquire. Further- to automated strategies, with improvements of 15.8% points,\nmore, running a prompt is less resource-intensive than fine- 18.3% points, and 16.1% points for code summarization, gen-\ntuning an LLM. Despite its potential, prompt engineering is eration, and translation, respectively. Participants frequently\n5202\nbeF\n91\n]ES.sc[\n2v80501.0132:viXra"}
{"instruction": "Explain this concept:", "response": "requested improvements, added context, or provided specific [39] studied ChatGPT\u2019s ability to find failure-inducing test\ninstructions to enhance GPT-4\u2019s output. Our results indicate cases, showing that performance improved drastically with\nthat GPT-4 with conversational prompting holds potential for correct guidance. Feng et al. [30] introduced AdbGPT, an\nautomated code-related tasks, but fully automated prompt LLM-based approach for reproducing bugs using few-shot\nengineering requires further development. learning and chain-of-thought reasoning. Kabir et al. [40]\nThis paper contributes the following: analyzed ChatGPT\u2019s responses to Stack Overflow questions,\n\u2022 Thefirstempiricalstudycomparingautomatedprompting finding over half of the answers incorrect and 77% verbose.\nstrategies on GPT-4 with fine-tuned LLMs for three Geng et al. [31] adopted in-context learning for code sum-\nautomated code-related tasks. marization, using customized strategies like selection and re-\n\u2022 A user study with 27 graduate students and ten industry ranking to enhance performance.\npractitioners exploring the evolution of conversational Wang et al. [41] compared prompt-tuning and fine-tuning\nprompts in automated code-related tasks. on three code tasks (defect prediction, code summarization,\n\u2022 Identification of gaps between conversational and auto- andcodetranslation)usingtwopre-trainedmodels,CodeBERT\nmated prompts, and suggestions for leveraging LLMs in andCodeT5.Incontrast,weexploredtheeffectivenessofGPT-\nautomated code-related tasks. 4usingthreepromptingtechniques(basic,in-contextlearning,\n\u2022 Release of all study artifacts to facilitate replication and andtask-specificprompting)against16fine-tunedLLMs.The\nextension by other researchers1. main difference between the studies lies in model size: they\nassessed models with 220M and 125M parameters, whereas\nII. BACKGROUNDANDRELATEDWORK we evaluated GPT-4 with 1.76T parameters. We also included\nA. Fine-Tuning Language Models for Code a qualitative analysis surveying academia and industry partic-\nipants to explore the impact of different prompting strategies.\nThe field of automating code-related tasks has increasingly\nDespite growing interest in prompt engineering, compar-\nadopted language models (LMs) [1]\u2013[5]. These methods offer\nisons between fine-tuned and prompt-engineered LLMs in\nsignificant advantages over traditional approaches such as\nASE tasks remain limited. Fine-tuning modifies pre-trained\ndomain-specific models, probabilistic grammars, and simple\nLM parameters to optimize a task, while prompt engineering\nneuralLMs.Inrecentyears,LMshavebeenappliedtovarious\ncrafts natural language queries to obtain the desired output\nASE applications, including code completion [6], code search\nwithout parameter changes. Fine-tuning can achieve better\n[7], code generation [8], [9], test case generation [10], [11],\nperformance but requires more data and resources [41], [42].\ntest oracle generation [12], code summarization [13], code\nPrompt engineering leverages LLMs\u2019 versatility but may face\ntranslation [14], and automated program repair [15].\nissues like inconsistency and insufficient domain knowledge\nPre-trained code models learn general-purpose code repre-\noptimization[42]\u2013[44].Toaddressthisgap,weconductedthe\nsentationscapturinglexical,syntactic,semantic,andstructural\nfirstquantitativeandqualitativecomparisonoffine-tuningand\ninformation. Fine-tuning adapts these models to specific tasks\nprompt engineering methods.\nusing task-specific data, allowing them to outperform existing\nbaselines. Numerous studies have leveraged pre-training on III. EMPIRICALSTUDYSETUP\nsource code and natural language corpora, followed by fine-\nA. Downstream Tasks on Code Automation\ntuning for downstream tasks, e.g., code search, code summa-\nrization, code generation, etc. [16]\u2013[20]. We select three typical automated code-related software\nengineering tasks for the experiments, i.e., Code summariza-\nB. Prompt Engineering in Software Engineering tion: [45]\u2013[47]The model generates a short natural language\nPrompt engineering is an alternative to fine-tuning that summaryfromasourcecodesnippet(SC-to-NL),Codegener-\nadaptspre-trainedLMswithoutrequiringasuperviseddataset. ation: [48]\u2013[50]The model generates the corresponding code\nInstead, it uses prompts to treat different tasks as generation snippet from a natural language description (NL-to-SC), and\nproblems. These models, termed Large Language Models Code translation: [51]\u2013[53] The model translates a source\n(LLMs),havelargercorporaandmoreparameters.Theadvent code snippet into another programming language (SC-to-SC).\nof LLMs and prompt engineering has significantly improved These tasks assess GPT-4\u2019s ability to generate various\ntask performance [21]\u2013[27]. Studies have explored LLMs modalities (SC and NL). The programming languages differ\nand prompt engineering to tackle code tasks with various per task: code summarization involves Ruby, JavaScript,\nprompting strategies, such as basic prompting, in-context Go, Python, Java, and PHP; code generation targets\nlearning,task-specificprompting,chain-of-thoughtprompting, Python; and code translation involves Java and C#, cov-\nauto-prompting, and soft prompting [6], [28]\u2013[38]. ering both translation directions. The languages are chosen\nGao et al. [29] investigated three key factors in in-context based on the benchmarks assessed, which are discussed in the\nlearningforcodetasks:selection,order,andnumberofexam- following section.\nples. They found that both similarity and diversity in example\nB. Dataset and Data Process\nselection were crucial for performance and stability. Li et al.\nWe use well-known benchmarks for each examined task.\n1https://github.com/shinjh0849/gpt4 ase tasks Therationaleforusingthesebenchmarksistheircommonality,"}
{"instruction": "Explain this concept:", "response": "TABLE I: Stats of dataset used in quantitative analysis TABLE II: Baseline Fine-tuned models in three code tasks.\nTasks Dataset Language # of test instance Tasks Baseline Pre-trainedModel Param Fine-Tuned\nPolyglotCodeBERT[20] CodeBERT[16] 125M\nCode CodeSearchNet\nPython 14,918 CoTexT[58] T5-base[59] 220M\nSummarization (CSN)\nProphetNet-X[60] ProphetNet-Code[60] 300M\nPHP 14,014\nPanGu-Coder2[61] StarCoder[62] 15B\nHumanEval\nGo 8,122 WizardCoder[63] StarCoder[62] 15B\nCodSum CSN [54] XCoder[64] LLaMA3-8B-Base[65] 8B (HE)\nJava 10,955 Code\nCODE-T-ITER[66] cushman-001 12B HE&MBPP\ngeneration\nJavaScript 3,291 CODE-T[66] davinci-001&002[56] 175B\nStarCoder[62] StarCoderBase[62] 15.5B MBPP\nRuby 1,261\nStarCoder2[67] StarCoder2-15B[67] 15B\nC# 1,000 Code StructCoder[68] CodeT5based[69] 224M CodeTrans\nCodTran CT [55] translation PLBART[19] PLBART[19] 140M (CT)\nJava 1,000\nHumanEval [56] Python 164\nCodGen\nMBPP [57] Python 500 Quantitative analysis\nenabling comparison with other studies without retraining BC eo nl cle hc mtio an rk wG /oe n pe rr oa mtio pn ts P Ee vrf ao lr um aa tion nce C wo /m bp aa sr lii ns eo sn\nmodels, and allowing assessment of generalization across\nQualitative analysis\nmultiple languages. For code summarization and translation,\nweuseCodeXGLUE(CSNandCT)[55].Forcodegeneration,\nNew data Manual task Perform Categorize &\nweuseHumanEval[56]andMBPP[57].Datasetstatisticsare from GitHub solving survey analyze\nin Table I. Since we do not train a new model, we show only\nthe number of instances in the test sets used for evaluation. Fig. 1: Overall workflow of our study.\nSince GPT-4\u2019s training data is not public, it is uncertain\nif these benchmarks are included in its training. To reduce\nTheleaderboardcanbefoundfromeachdataset:CodeXGLUE\npotential information leakage, we collected sample test sets\n(CSN & CT)3, HumanEval4, and MBPP5.\nfrom GitHub repositories created after Sept. 2021 (GPT-\n4\u2019s end of training) for qualitative analysis. The criteria for D. Evaluation Metrics\nselectingrepositoriesincluded:createdafterOct.2021,written\nWe use the same metrics as the benchmarks to evaluate\nin Java, actively maintained, not a fork, and with English GPT-4. For code generation, we use pass@k [56], defined as\ncomments. We selected three top-rated projects in different\nthe probability that one of the top k-generated samples passes\ndomains(library,algorithm,web)andpickedtwoexamplesper\nthe unit tests (with k set to 1). For code summarization, we\ntask, totalling 18 sets of problems. The average token length useBLEU [70],whichassessessimilaritytothegroundtruth\nis14forNLdescriptionsand38forSCsnippets.Theselected using n-gram precision. For code translation, we use BLEU,\nexamples cover different domains, include multi-modalities ACC [71], and CodeBLEU [72], which combines n-gram\nand vary in difficulty.\nprecision, keyword matching, AST matching, and dataflow\nWe followed common pre- and post-processing for each matching.\ntask. For code summarization, we removed special characters\n(except commas and periods) and tokenized them. For code\nIV. METHODOLOGYANDPROTOCOLS\ngeneration, we formatted generated Python code to match In this section, we discuss the details of the methodology\nthe datasets. For code translation, we tokenized the generated and the protocol designs for our quantitative and qualitative\ncodeusingctokinPython,asevaluationmetricsaresensitive study. Figure 1 shows the overall workflow of this study.\nto tokenization. GPT-4 sometimes generated non-code text,\nA. Quantitative Study of GPT-4 with Baselines (RQ1)\nrequiringpost-processingtokeeponlycode.ToautomateGPT-\n4prompting,weusedtheOpenAI API2,specifyingtheGPT-4 To quantitatively assess the performance of GPT-4 for each\nASEtask,weusewidelyknownandusedbenchmarkdatasets:\nmodel. We set the temperature parameter to zero for better\nCodeXGLUE, HumanEval, and MBPP (details in Section\ndeterminism [43]. Metrics were calculated using scripts from\nIII-B). For the baseline on fine-tuned models, we report the\nthe benchmark dataset\u2019s repository.\nscores from the leaderboard of each benchmark and compare\nthem with the evaluation metrics calculated from the results\nC. Baselines\nof GPT-4. To generate the results from GPT-4, we use three\nTable II lists the fine-tuned baseline models compared with different kinds of prompting strategies:\nGPT-4. These are the best-performing fine-tuned models re- 1) Basic prompting [21], [32]: we directly query GPT-4\nportedineachbenchmark\u2019sleaderboard.Notethatsomebase- with the input (code or description) and ask to generate\nlines reported on the leaderboard were not publicly available, solutions in the form of the desired output.\nso we only included publicly available models in the table.\n3https://microsoft.github.io/CodeXGLUE/\n4https://paperswithcode.com/sota/code-generation-on-humaneval\n2https://platform.openai.com/ 5https://paperswithcode.com/sota/code-generation-on-mbpp"}
{"instruction": "Explain this concept:", "response": "2) In-contextprompting[31],[33]:togetherwiththebasic the existing studies [29], [74] that used BM25 similarity-\nprompt, we give a set of input/output examples to GPT- based selection [75]. We calculate the similarity of each test\n4. This idea is very similar to few-shot learning in the instance\u2019s input to each training instance\u2019s input and select\ncontext of fine-tuned language models. the top three similar instances for the context examples. The\n3) Task specific engineered prompting [34], [35], [73]: training set here refers to the benchmark datasets, i.e., the\ntogether with the basic prompt, we design additional dataset in Table. I. Although we do not use them for training,\nprompts to guide GPT-4 in generating better results for we use them to select the context examples. It was shown\neach task. to outperform the random selection or the fixed selection for\nin-context learning [29], [74].\nWe have selected three prompting strategies covering a range\n3) Task-Specific Engineered Prompts: We created task-\nof prompts from simple to more advanced. We use a small\nspecific prompts using simple heuristics to address the chal-\nsubset(between10and50,dependingonthetask)toevaluate\nlenges we faced with basic and in-context prompting strate-\nhow a crafted prompt results from the actual data (using the\ngies. We manually analyzed a subset of outputs to identify\nBLEU metric). This phase is done to avoid obvious mistakes\nthe root causes of issues. We discuss the challenges and how\nand not to optimize the prompt for each task. Note that\nweaddressedthemwithtask-specificpromptsinthefollowing\nfinding the optimal prompting strategy is not the goal of\nparagraphs.\nthe paper. Although the selections of prompts do not cover\nForcodesummarization,wetestedGPT-4\u2019sinitialresponses\nall possibilities, our goal is to provide an initial study on\nusing ten samples and found that the BLEU score decreased\nautomated prompting techniques in three different levels of\ndue to verbose explanations. To improve the score, we in-\ncomplexity. We select basic prompting as a baseline or the\nstructed GPT-4 to limit the output to 15 tokens, the average\nleast complex, in-context learning as one of the best and most\ntokenlengthofthetrainingset\u2019soutputs.Wealsofoundcases\ngeneric prompting approaches (at the time of conducting this\nwhere GPT-4 tries to summarize code for each line of code at\nstudy) and task-specific prompting as a specialized ad-hoc\nthesyntaxlevel.Tomitigatethis,weadded\u201cgenerateoneline\nprompt that potentially outperforms in-context learning. We\nof semantic focused and abstract summary of the code\u201d to the\nmake the basic prompt as simple and generic as possible\nprompt. Also, due to the creativity and immense dictionary of\nto compare how adding different strategies shows/improves\nGPT-4, it generates synonyms that are not used in the input.\ndifferentresults.Similarly,forin-contextlearning,wewantto\nTo mitigate this problem, we added instructions to guide the\nkeep the prompt as close to the basic prompt as possible and\nmodel in using naturalized identifiers to form the summary.\nonly add different contexts so we can compare how giving\nFor code generation, we found that GPT-4 had a hard time\nexamples improves the basic prompts.\ninferring the steps that were missing in the input (descrip-\n1) Basic Prompting: The basic prompts used for each task tion of code). To mitigate this, we tried to guide GPT-4 to\nare shown in Listing 1. They are the simplest prompts that consider the steps required to solve the described problem\nconsist of the input code/description and the description of carefully. First, we selected five potential prompts. Two of\nthe expected output. them used in-context prompting, one used chain-of-thought\n[36],andthefinaltwopromptscombinedin-contextandchain-\nof-thought. For the prompts with in-context prompting, we\n### code summarization task\nf\"Given a {lang} code snippet surrounded in ???, randomly selected three samples from the sanitized train set\ngenerate one line of semantic focused and\nfor the MBPP dataset and the test set for the HumanEval\nabstract summarization ???{code}???\"\ndataset. For the samples, we selected 50 examples out of\n### code generation task\n120 from the sanitized train set for the MBPP dataset and 50\nf\"Generate Python source code for a function, given\na natural language prompt (surrounded by ???) randomlyselectedexamplesoutof164fromthetestsetforthe\ndescribing the function\u2019s purpose. Output only the\nHumanEval dataset. Based on the Pass@1 scores, we chose\nPython source code on exactly one line. ???{nl}???\"\nthe best-performing prompt. From the preliminary evaluation,\n### code translation task\nwe found that using only the chain-of-thought resulted in the\nf\"Given that the code surrounded by ??? is written\nin C#, output only the corresponding Java code best performance.\ncondensed on one line ???{code}???\"\nFor code translation, we selected ten samples to check\nListing 1: Basic prompt used to query. the initial results. We observed that GPT-4 fails to generate\nsyntactic keywords that were prevalent in the ground truth,\n2) In-Context Prompting: For in-context prompting, we e.g., virtual and override. However, missing syntactic\nadd input/output examples on the basic prompt as additional keywords were easily mitigated by giving more context to\ncontexts. In-context prompting is one of the advanced types thedifferentlanguages,i.e.,input-outputexamples.Therefore,\nof automated prompts that help LLM learn the additional we decided to modify the in-context prompt to overcome its\ncontext.Weselectthreeinput-outputexamplesforthecontext. weaknesses. Five code pair examples were chosen as opposed\nA very high number of examples will consume more tokens tothreeforthein-contextprompt,withtherationalebeingthat\nthat might not fit into the allowed context length and also be a wider variety of lengths and keywords would provide better\nmore expensive. To select the context examples, we follow exposure to the \u201cquirks\u201d of the dataset that the first in-context"}
{"instruction": "Explain this concept:", "response": "promptmissed(e.g.,C#codereferringtoJavalibrariessuch\nAdvanced Intermediate Beginner\nas java.nio.ByteBuffer). Listing 2 shows the resulting\n100%\ntask-specific prompt.\n### code summarization task 75%\nf\"Generate one line of semantic focused and abstract\nsummary of the code surrounded by ???. Compose\nthe summarization by naturalizing the identifier 50%\nof variables and function names in the code as\nkeywords. The summarization should be very\nconcise, with an approximate limitation of\naround 15 tokens in length.\" 25%\n### code generation task\nf\"Generate Python source code for a function, given\na natural language prompt (surrounded by ???) 0% Student Java Industry Java Student C# Industry C#\ndescribing the function\u2019s purpose. Carefully\nconsider the steps required to fulfill the\nfunction\u2019s purpose stated in the natural (a) Participant programming language proficiency.\nlanguage prompt. Output only the Python source\ncode on exactly one line. ???{nl}???\" 100% Native\nExcellent\n### code translation task Very Good\nf\"Given the five examples of translating Java code\nGood\nto C# code (both surrounded by ???), translate 75%\nthe 5 Java code samples (surrounded by ???) to Fair\nC# code and output each C# code on exactly one 6<\nline. Do not include any extra text such as \u2019C# 4-6\ncode sample\u2019, and do not include surrounding ??? 50% 1-3\neither. ???{5_i_o_examples}???\" <1\nListing 2: Task-specific prompt used to query.\n25%\nB. Qualitative Analysis of Participants\u2019 Perception and Inter-\naction with GPT-4 (RQ2 and RQ3) 0% Student experienceIndustry experience Student writing Industry writing\nTo answer RQ2 and RQ3, we performed a user study by\n(b) Programming experience and writing proficiency.\nsurveying participants from both academia and industry. We\nFig. 2: Demographics from the survey.\nfirst discuss the protocols for sampling and conducting this\nempirical evaluation.\nSurvey Participants: We conducted a user study with 27\nUser Observation Study: We assign two out of three tasks\ngraduate students (recruited via convenience sampling [76]\nto each participant, considering their familiarity with pro-\nfrom an EECS mailing list; 31 respondents, 27 selected for\ngramming languages. Since participants should have some\navailability) and 10 industry professionals (recruited from 26\nknowledge about C# for the code translation task, we first\ninvited across Canadian, U.S., and Asian tech companies; 12\naskediftheyknewC#andonlygavethemthecodetranslation\nrespondents, 10 selected).\ntask. The rest of the tasks were assigned randomly. For each\nParticipants\u2019 demographics are shown in Figure 2. From\ntask, we gave one sample from each of 3 projects. The partic-\nAcademic participants, the distributions are 77.8% M.Sc.,\nipants worked on the task in a conversation with ChatGPT-4.\n18.5%Ph.D.,1Postdoc.FromIndustryparticipants,thedistri-\nChatGPT-4isachat-interfacedframeworkthatrunsonGPT-4\nbutions are 4 senior software engineers, 3 software engineers,\n6. We keep the settings of ChatGPT-4 to default. We ensured\n1associateMLdeveloper,1leadengineer,and1MLOpslead.\nabalancedallocationoftasksamplestoparticipantsrandomly\nFor LLM familiarity, 5.4% never used, 37.8% application-\n(with an equal number of participants per project and task).\nlevel users, 29.7% users with blog/research exposure, 8.1%\nWeaskparticipantstostartwiththeirbasicpromptandthento\nimplemented models, 16.2% active researchers, 2.7% no prior\nproceedwithconversationstoimprovetheresultbasedontheir\nexposure.\nfirst evaluation until they consider the solution satisfactory\nStudy Design: The survey consists of three primary phases,\nor have spent 5 minutes per task sample. In this setup, we\ni.e.,thebackgroundanddemographicsurvey,userobservation,\nhavenotprovidedanyexamplepromptstoavoidanybiasand\nand post-experimentation survey.\nto get participants\u2019 fresh perspectives. Overall, we evaluated\nBackground and Demographic: We ask for participants\u2019\nthree tasks over six samples per task. Each sample has been\ndemographic information, i.e., years of experience in pro-\nassigned and solved with an average of 12 times by different\ngrammingoutsideofschool,degreeoffamiliaritywithLLMs,\nparticipants [77]. The survey was designed to take no more\ncurrentposition,degreeoffamiliaritywiththetargetlanguage\nthan 60 minutes for each participant.\n(Java and C#), and the level of technical writing skills in\nEnglish. We asked six close-ended questions overall. 6https://chat.openai.com/"}
{"instruction": "Explain this concept:", "response": "TABLE III: Result of code summarization. Bold denotes the\nPost-Experimentation Survey: After the observation study,\nhighestscore.Theasterisk(*)denotesthatthesourcewasnot\nwe posed four closed-ended questions to the participants. We\npublic. GPT-4 + B/IC/TS denotes basic, in-context, and task-\ninquire whether they received an incorrect response from the\nspecific prompts, respectively.\nmodel, the degree to which they found GPT-4 useful for the\nassigned task, their perception of response quality on the first BLEU\nand last attempt, and their overall assessment. Additionally, Model Ruby JS Go Py Java PHP Avg\nStarCoder-LoRA* 17.21 18.15 21.61 23.13 22.61 28.76 21.91\nwe included four open-ended questions to assess the utility of DistillCodeT5* 15.75 16.42 20.21 20.59 20.51 26.58 20.01\nthe GPT-4 model. Strengths and weaknesses of the first and PolyglotCodeBERT[20] 14.75 15.80 18.77 18.71 20.11 26.23 19.06\nCoTexT[58] 14.02 14.96 18.86 19.73 19.06 24.68 18.55\nlast response for both tasks. ProphetNet-X[60] 14.37 16.60 18.43 17.87 19.39 24.57 18.54\nForRQ2,wereporttheresponsesfromthefourclose-ended GPT-4+B 19.93 19.96 29.70 27.19 24.11 18.63 23.25\nGPT-4+IC 19.60 20.08 28.98 19.90 24.45 19.24 22.04\nquestions explained in the post-experimentation survey. We GPT-4+TS 25.48 27.23 35.34 29.61 33.08 30.67 30.24\nshow the percentage of the evaluation responses from all the\nTABLE IV: Result of code generation task.\nparticipants. The evaluation of each survey question can be\nanswered to one out of the five following selections: 1) poor, HumanEval MBPP\n2) fair, 3) good, 4) very good, and 5) excellent. We consider Model Pass@1 Model Pass@1\nCODE-T(davinci-002)[66] 65.80 CODE-T(davinci-002)[66] 67.70\nthe evaluation to be satisfactory if they answer either good, CODE-T-Iter(davinci-002)[66] 65.20 StarCoder2[67] 66.20\nPanGu-Coder2[61] 61.64 CODE-T(davinci-001)[66] 61.90\nvery good, or excellent.\nWizardCoder[63] 57.30 CODE-T(cushman-001)[66] 55.40\nFor RQ3, we investigate the chat logs of the participants XCoder[64] 57.30 StarCoder[62] 52.70\nand manually categorize the conversational prompts. We first GPT-4+B 69.51 GPT-4+B 39.60\nGPT-4+IC 64.63 GPT-4+IC 40.60\nextracted the conversational prompts as a list of abstract GPT-4+TS 74.39 GPT-4+TS 39.40\ninstructions for the qualitative analysis. We then used an\nOpen Coding Approach, where two authors of this paper,\nperformanceofconversationalprompts.Wedidnotincludethe\nwho were not involved in the extraction process, performed\nin-contextpromptasacomparisonbaselineforthisexperiment\nsessions of card sorting for our qualitative data analysis [78].\nas we do not have a separate training set to select similar\nThis analysis design was chosen to make the categorization\ncontext examples (as mentioned in Section IV-A2).\nand extraction as independent from each other as possible.\nAllthesurveyquestions,surveyresponses,therawchatlogs\nAfter that, a third author acted as a moderator to resolve\nfrom the participants, and analyzed prompt categories by the\nthe disagreements. Finally, all other authors are involved in\nauthors are available in our replication package.\nmanually reviewing the forms completed by the three authors\nand resolving any disagreements that occurred during the\nV. EXPERIMENTALRESULTS\nprocess to ensure consensus among reviewers. Note that we\nA. RQ1: Automated Prompts vs. Fine-Tuned Models\nalso provide the category of conversational prompt and the\nraw chat logs for each participant in our replication package Table III shows the results of fine-tuned baselines and\nto provide transparent results of our work. After categorizing GPT-4 on the code summarization task. The basic prompt-\nthe types of conversational prompts, we count and rank the ing improved the 1st ranked baseline by 1.34% points on\noccurrences for each task to show the overall trends among average. However, this pattern wasn\u2019t consistent across all\nthe participants. programming languages where for PHP, we see a 10.13%\npoints decrease. We observe the biggest improvement from\nC. Analysis of Users\u2019 Conversational Prompts (RQ4)\nGowith8.09%pointsimprovement.Thein-contextprompting\nFor RQ4, we investigate each prompt created by the par- improvedthe1strankedbaselineby0.13%pointsonaverage.\nticipants and the response by GPT-4. Since GPT-4 generates However, the inconsistent pattern was similar to the basic\nverboseexplanationsforitsresponse,wemanuallyextractthe prompt where a decrease was shown in PHP with a 9.52%\ncore part of the answer for each task. For code generation points and the highest improvement from Go with a 7.37%\nand translation tasks, we excerpt the code snippets from the points improvement. When comparing in-context prompting\ngeneratedresponse.Forcodesummarizationtasks,weexcerpt to basic prompting, it didn\u2019t improve the basic prompting,\nthetextsthatsummarizethegivencodesnippet.Theresponses showing a 1.21% points decrease on average. The lowest\nare written in a specific code editor format, allowing us to decrease was found in Python with 7.29% points. The\nextract them from the logs manually. After we extract the highest improvement was found in PHP with 0.61% points.\nanswers, we rank them based on the BLEU score. For each The task-specific prompt had the most noticeable improve-\ncategory of conversational prompt, we calculate the mean re- ment compared to the 1st ranked baseline with 8.33% points\nciprocal rank (MRR) to find which type of prompt constitutes improvementonaverage.Whencomparingtask-specifictothe\nthemosttoahigherrank.Lastly,wereporttheaverageBLEU basic prompt, it improved by 6.99% points overall.\nscore of the conversational prompts, the highest BLEU score Table IV shows the results for the code generation tasks.\nachieved by a participant, and the generation from the two The basic prompting was able to improve the 1st ranked\nautomatedpromptstrategiesweusedinourquantitativestudy, baseline with an increase of 3.71% points for HumanEval.\ni.e., basic prompts and tasks-specific prompts, to compare the However, for MBPP, it decreased by 28.10% points. The"}
{"instruction": "Explain this concept:", "response": "TABLEV:Resultofcodetranslation.Bolddenotesthehighest was an improvement in C#-Java. When comparing in-\nscore. The asterisk (*) denotes that the source was not public.\ncontext prompting and basic prompting, it showed a drastic\nGPT-4 + B/IC/TS denotes basic, in-context, and task-specific improvement in both Java-C# and C#-Java. We observed\nprompts, respectively.\nthat task-specific prompting outperforms basic prompting.\nJavatoC# C#toJava However, it could not outperform in-context learning, as we\nModel BLEU ACC CB BLEU ACC CB found in our initial subset when designing the prompt.\nStructCoder[68] 85.02 66.60 88.42 80.66 67.70 86.03\nOverall, GPT-4 with different automated prompting strate-\nPLNMT-sys0* 83.37 64.60 87.38 80.91 66.80 85.87\nPLBART[19] 83.02 64.60 87.92 78.35 65.00 85.27 gies could not outperform the fine-tuned models significantly.\nCodePALM* 83.26 65.50 86.37 78.94 65.20 83.74 For code summarization, a reason for this can be due to the\nContraBERT G* 80.78 59.90 84.98 76.24 60.50 81.69\nverboseandcreativegenerationofGPT-4.Sincetheevaluation\nGPT-4+B 55.33 5.50 68.34 63.08 20.20 73.57\nmetrics heavily rely on textual similarity, diverse and verbose\nGPT-4+IC 80.72 34.10 86.69 84.12 50.20 87.44\nGPT-4+TS 80.55 34.30 86.87 81.29 49.20 85.33 generation will significantly impact the metric scores. When\nwe tried to mitigate this with our task-specific prompt, the\nresultimprovedbyabigmargin,outperformingthefine-tuned\nin-context prompting also couldn\u2019t outperform the baselines baselines. For code generation, the code generated by the\n(1.17% points decrease for HumanEval and 27.64% points GPT-4 seems plausible and executable. However, they fail to\ndecrease for MBPP). When comparing in-context prompting pass the tests, e.g., off-by-one error, using imprecise logical\nto basic prompting, there is a decrease of 4.88% points in operators(choosingbetweenandandor operator),ormissing\nHumanEval and a small increase of 1.00% points in MBPP. to generate intermediate steps that are not mentioned in the\nThe task-specific prompting improved from the baseline, with NL description. A possible reason is that since it is not fine-\nan8.59%pointsincreaseinHumanEval.However,forMBPP, tuned, it is harder to generate code that requires project-/data-\nit decreased by 28.30% points. When comparing task-specific specificknowledge,whichcouldbeleveragedtogeneratecode\nto basic prompting, it didn\u2019t show a significant improvement, with correct functionality. For code translation, the model\nshowing a 4.88% points improvement in HumanEval and a failed to generate the same code (low ACC) for both target\ndecrease of 0.20% points in MBPP. languages. The possible reasons can be: 1) a much larger\nTable V shows the results for code translation. The basic and diverse corpus is used for training, resulting in a more\nprompting couldn\u2019t outperform the 1st ranked baseline with a creative generation, and 2) generating something irrelevant to\ndecrease of 29.69% points in BLEU, 61.10% points in ACC, the input, i.e., LLM hallucination. Failing to generate similar\nand 20.08% points in CodeBLEU for translating Java-C#. (low BLEU) C# code compared to Java code can be due\nSimilar patterns showed when translating C#-Java with a to the difference in data size used for training GPT-4, since\ndecrease of 17.83% points in BLEU, 47.50% points in ACC, Java is more commonly used than C# in GitHub7.\nand 12.46% points in CodeBLEU. The low ACC scores when\ntranslating Java-C# are mainly caused by the failure to Answer to RQ1: Automated prompting with GPT-4 did\npredictthefrequentlyusedsyntactickeywords(e.g.,virtual not significantly outperform fine-tuned baselines, except\nand override). For example, the keyword override ap- for code summarization, indicating the need for more so-\npearedin216,andvirtualappearedin431outofthe1,000 phisticated prompting strategies to fully leverage GPT-4\u2019s\ntest samples (totaling 647, as no sample used both keywords). capabilities.\nConversely,GPT-4generatedthekeywordoverrideinonly\n48 results and never generated virtual. We also observed\nthat it failed to infer code elements that are missing from B. RQ2: Participants\u2019 Perception of GPT-4\nthe input, e.g., parameters in API calls that are not in the\nFigure 3 shows how the participants qualitatively assessed\nsource language but are in the target language. Since ACC\nthe responses of GPT-4. The first bar shows participants\u2019\nrequires exact matching, a single token could prevent an\nprior perception of GPT-4\u2019s general usefulness. More than\nexact match, severely impacting the overall ACC score. We\n83.8%oftheparticipantsweresatisfied(>=Good)withGPT-\nobservedanothertypeoffailurewhichwascausedbytheLLM\n4. Some of the participants (5.4%) have never used GPT-4\nhallucination[79]\u2013[82]wherethemodelgeneratescontentthat\nbefore.Thesecondbarshowsiftheyencounteredanyincorrect\nis irrelevant, made-up, or inconsistent from the input data.\nresponses on any steps during the study. We could see that\nFor example, the model would generate APIs that are not\nalmost half (48.6%) of the participants did not encounter any\nused in the source language or even APIs that don\u2019t exist.\nincorrectresponsesduringthestudy.Thethirdbarshowstheir\nFor C#-Java, we did not find any noticeable frequently\nevaluation of the responses from the first attempt of prompts,\nomitted syntactic keyword, which could explain the relatively\nand the last bar shows their evaluation at the end of the\nhigherACCscore.Adrasticimprovementwasachievedbyin-\nexperiment. The evaluations for initial responses were already\ncontext and task-specific prompting, especially for generating\nconsiderably good with more than 83.8% satisfactory score\nthe missing syntactic keywords. Overall, GPT-4 with in-\ncontextpromptingshowedmixedperformance,i.e.,itcouldn\u2019t\noutperform the baselines in translating Java-C# while there 7https://gitnux.org/github-languages-statistics/"}
{"instruction": "Explain this concept:", "response": "functionality, we observe that most of the participants request\n100% No\nimprovements in abstracting and focusing on the semantics.\nYes\nN/A Previous literature also points to the verbosity of LLMs\u2019\nExcellent explanations of code [40], [87]. In addition, some participants\n75%\nVery Good requestedmoredescriptionsforcorelogic,wheresyntax-level\nGood\nexplanations are lacking, and added specific instructions to\nFair\n50% move the line-level explanations to the top of the method to\nresemble technical documentation, e.g., JavaDoc. Since the\noutput of the code summarization task is a natural language\n25% description of code, participants also request the model to\ngenerate descriptions with a certain style or format.\nAdding specific instructions was found to be the most\n0%\nUsefulness Incorrect First attempt End of experiment common for code generation tasks. The instructions have a\nvariety of ranges, e.g., \u201cremove the main method\u201d, \u201cremove\nFig. 3: RQ2: Qualitative results.\nthe import statements\u201d, \u201cuse a certain type for the variables/-\nparameters\u201d, \u201cremove external packages\u201d, etc. Close runner-\nupswere\u201crequestingimprovements\u201dand\u201caskingquestionsto\n(>= Good). After applying the conversational prompt, the re-\nguidethemodel\u201d.Thekeywordsusedforrequestingimprove-\nsponses received higher evaluations with a 94.6% satisfactory\nment in code generation are e.g., \u201cgood format\u201d, \u201creadable\u201d,\nscore (>= Good), an increase of 10.8% points.\n\u201cbetter quality\u201d, \u201cmost succinct working\u201d, \u201cclear\u201d, etc. An\nWe can see that their evaluations change positively from\nexample for asking questions to the model would be, e.g.,\nthe initial responses to the end of the experiment (excellent\n\u201cIs the following code snippet syntactically correct?\u201d, \u201cIs\n+24.3%), indicating that their conversational prompt helped\nthe generated code executable?\u201d, \u201cIs this the most efficient\nGPT-4 in generating better responses. This result is also\nversion of code?\u201d, etc. These questions aren\u2019t the type to\nconsistent with previous studies that multiple conversational\nask to perform a specific instruction but to question the\nprompting improves the results of LLMs [83]\u2013[85].\nreasoning path of the model to re-think their decision and\nAnswer to RQ2: Our qualitative analysis shows that par- then make changes if they were incorrect. We suspect these\nticipants were generally satisfied with GPT-4\u2019s responses. conversational prompts are made as GPT-4 fails to infer some\nSatisfaction increased from 83.8% to 94.6% after using missingspecificationsthataren\u2019texplainedintheabstracttext\nconversational prompts, a 10.8% points improvement, in- description of code. To fill these gaps, participants would\ndicating the benefit of conversational prompts. 1) add specific instructions, 2) ask for improvement of code\nquality, or 3) ask questions for verification or to guide the\nmodel into a different reasoning path. Generally, our findings\nC. RQ3: Trend in Forming Conversational Prompts\narealignedwithpreviousliterature[88]inthatdeveloperstend\nFrom analyzing the chat logs, we identified nine types to add specific instructions for refactoring (i.e., add/remove\nof conversational prompts: (1) Request improvements with certain code elements) or request improvement code quality\ncertain keywords, (2) Provide more context, (3) Add specific from LLMs. \u201cAsking questions for verification to guide the\ninstructions, (4) Point mistakes then request fixes, (5) Ask model to correct reasoning paths\u201d is a new pattern we found\nquestions to guide the correct way, (6) Request verification, in this study. We also observed that participants requested to\n(7) Request more examples, (8) Request more detailed de- removeirrelevantgenerations(e.g.,importantstatements,main\nscription, and (9) Request another or a different version of method, external packages) due to LLM hallucination.\ngeneration. For the code translation task, the most common conver-\nTable VI shows the categorized conversational prompts sational prompt was to give certain instructions to perform.\nand their numbers used for each task. We can see that the These instructions include, e.g., \u201cusing a certain type\u201d, \u201cusing\ntrends tend to differ per task. For code summarization, we a certain logic\u201d, \u201cmodifying the code to support certain\ncan see that requesting certain improvements with different data types\u201d, \u201cremoving import statements\u201d, \u201chandling certain\nkeywordswasthemostcommon.Thekeywordsforrequesting parameters\u201d, etc. The runner-up asked certain questions to\nimprovement are e.g., \u201cmore natural\u201d, \u201cabstract\u201d, \u201csemantic- verify or double-check if the generated code met the specific\nfocused\u201d, \u201chuman-readable\u201d, etc. We suspect they requested syntax of the target language. Generally, the results were\nthese improvements due to GPT-4\u2019s verbose generations, similar to code generation as their output is both source code.\nwhich generate line-by-line explanations of the code\u2019s syntax. Othercommonlyusedconversationalpromptsinclude\u201cadding\nCodesummarizationfocusesonautomatingthetechnicaldoc- more context information\u201d, \u201crequesting more details or de-\numentationprocessasitisanontrivialtask[86].Explaininga scriptions\u201d, \u201cpointing out mistakes then requesting fixes\u201d, etc.\nmethod\u2019s syntax line-by-line would defeat the whole purpose Apreviousstudyfoundthatthemostprevalenttranslationbug\nof the task as developers would not read all the syntax wasdata-related,i.e.,datatypes,parsinginputdata,andoutput\nexplanations. To get the implicit summary of the method\u2019s formattingissues[52].Wecanobservethattheparticipantstry"}
{"instruction": "Explain this concept:", "response": "TABLE VI: Trend: the number of different prompt categories\nFrom the observation, we can state that the conversational\nparticipants used for each task.\npromptthatrequestedcertainimprovementswasmosteffective\nPrompts ComGen Codgen CodTrans in mitigating verbose and syntax-focused code summariza-\nRequestimprovements 12 7 5 tion. For example, \u201cplease make summaries more natural\nRequestmoredescription 7 - - and abstract\u201d, \u201cimprove this by generating semantic-focused\nAddspecificinstructions 7 8 9\nand abstract summaries\u201d, etc. The prompt resulted in GPT-4\nAskquestionstofindcorrectway 6 7 7\nAddmorecontext 5 6 3 generate that are closer to the ground truth, which are short\nRequestexamples 3 1 1 phrases that summarize the functionality of the method. For\nRequestverification 2 1 1\ncode generation, adding more context was most effective to\nPointmistakethenrequestfix 2 5 4\nRequestanothergeneration 1 - 2 mitigate the limitation of GPT-4 in inferring the missing steps\nthat the abstract text description encapsulates. For example,\nthey would add the class information, e.g., class signature,\nto address these problems by adding specific instructions like description of what the class does, and other public method\n\u201cuse a certain type\u201d, \u201cmodify to support certain data type\u201d, signatures within the class. The model was able to infer\nor \u201chandle certain parameters\u201d. We also observe that similar the missing steps by the project- or class-specific knowledge\nto the code generation task, they tried to remove irrelevant providedbytheadditionalcontext.Forcodetranslation,asking\ngenerations due to LLM hallucination. Previous studies have questions to find the correct way was the most effective in\nalso reported that more than one-third of translation bugs are generating the correct syntax of different languages and data\ndue to syntactic and semantic differences between languages. types.Forexample,\u201cwhataboutthe@Parameterannotation?\u201d,\nThis finding is also in line with our previous findings in \u201cThe JSON.parseArray function needs to know the generic\nSection IV-A3 that they fail to generate certain keywords type T to know the format of the return value, how should\nspecific to the target language. We observe that participants I modify the code?\u201d, etc. As the examples show, they would\ntried to mitigate this by adding specific instructions or asking ask questions to guide the model in resolving the data issue.\nquestions for target language-specific syntax.\nThesimilarityofthetworankscanbeinterpretedaspartici-\npantshavingagoodknowledgeofhowtoguidethemodelsfor\nAnswer to RQ3: Different trends were observed in conver-\nbetter results, even though the ground truth wasn\u2019t provided.\nsational prompts per task. \u201cRequesting improvements with\nItisinterestingtofindthat\u201crequestingimprovement\u201dactually\ngeneral keywords\u201d was most common for code summariza-\nhelps generate better responses even though these prompts do\ntion, while \u201cadding specific instructions\u201d was most frequent\nnotnecessarilyprovidespecificknowledgeonhowtoimprove.\nfor code generation and translation. Participants used con-\nversationalpromptstoaddressknownLLMlimitations,such We also report the average BLEU score from the responses\nas verbosity, missing information, and hallucination. of conversational prompts in Table VIII. We can see that\nthe average BLEU score of all conversational prompts is\ncomparable to the two automated prompts, i.e., basic and\nD. RQ4: Efficacy of Conversational Prompts task-specific prompts. However, only the code translation task\nRQ4 aims to investigate which category of conversation outperforms the better-automated prompt, i.e., task-specific\nprompts contributes to better results. Table VII shows the prompt, by a mere 0.27% points. The reason is due to the\ntop five ranks of conversational prompts that contribute the large variance caused by the different types of conversational\nmost to better results. \u201cRequesting improvement\u201d was shown prompts. The conversational prompt that produces the highest\nto be the most effective conversational prompt category for BLEU score outperforms the automated prompts by a big\ncode summarization. \u201cAdding more context information\u201d was margin, 15.8% points for code summarization, 18.3% points\nshown to improve code generation the most. For code transla- for code generation, and 16.1% points for code translation\ntion,\u201caskingquestionstofindthecorrectreasoningpath\u201dwas tasks.\nshown to have the best performance. We can observe that the\nrankingoftrendsandtheranksthatbestimprovetheresponses\nAnswer to RQ4: Overall, conversational prompts can im-\nare very similar but not the same. For example, the first rank\nprove code tasks. \u201cRequesting improvement\u201d worked best\nin trend and improvement for code summarization were both\nfor code summarization, \u201cadding context\u201d for code gen-\nrequesting certain improvements. For code generation, adding\neration, and \u201casking questions\u201d for code translation. The\nspecific instruction was first ranked on the trend ranking but\nrank of trends and improvements showed similar patterns,\nappeared second in the improvement ranking. Adding more\nindicating participants effectively guided the models. The\ncontextwasfoundtobethefirstforimprovementbutfoundto\nbest conversational prompts significantly outperformed au-\nbe the third most common in the trends. For code translation,\ntomatedprompts,improvingby15.8%points,18.3%points,\nasking questions to find the correct path was the first rank in\nand 16.1% points for code summarization, generation, and\nimprovement but appears on the second in the trend ranking.\ntranslation, respectively.\nAdding specific instructions was the most common prompt\nbut was found to be the third most effective in improvement."}
{"instruction": "Explain this concept:", "response": "TABLE VII: Improvement: Top-5 prompt categories that ef-\n\u2022 EaseofUse:Promptengineeringhasasignificantbenefit\nfectively improve the basic prompts.\nfromthisaspect,especiallywhenusingnaturallanguage-\nRank Codesummarization Codegeneration Codetranslation based interfaces. It becomes more significant as we see\n1 Requestimprovements. Addmorecontext. Askquestions. participantswithlittletonobackgroundinLLMscanper-\n2 Addmorecontext. Addinstructions. Pointmistakethenfix.\n3 Askquestions. Requestimprovements. Addinstructions. formASEtaskswithChatGPT-4.Incontrast,theywould\n4 Addinstructions. Askquestions. Requestimprovements. not be able to perform with fine-tuned models without\n5 Requestverification. Pointmistakethenfix. Addmorecontext.\na certain knowledge of deployment, configuration, and\ndeveloping fine-tuned models.\nTABLE VIII: Result of conversational prompts (conv.) vs\nautomated prompts. Bold indicates the highest score. \u2022 Control: Finally, another aspect of the trade-off is the\nlevelofcontrolontheinternalconfigurationsandoutputs.\nBLEU For example, with GPT-4, although you can set the tem-\nTask Averageconv. Highestconv. Basic Task-specific\nperature to zero, you still get non-deterministic answers,\nCodSum 36.22 55.75 35.76 39.95\nCodGen 53.15 69.16 57.33 50.85 whichmakestheverificationofthemodelsmoredifficult\nCodTran 67.43 83.26 52.10 67.16\nand thus less interesting to some users.\nVI. THREATSTOVALIDITY\nE. Discussion\nInternal Validity: A key threat is whether the designed\nprompts truly reflect the intended strategy or if better ap-\nIn this section, we discuss the main findings of this study\nproachescouldexist.Forexample,poorresultsofGPT-4with\nin terms of prompt engineering and summarize them as\nin-context learning might be due to confounding factors, such\nrecommendationsforfuturework.Then,wediscussthetrade-\nas suboptimal sample choices or vocabulary. To address this,\noffs between prompt engineering and fine-tuning.\nwereportedtheexactpromptsusedandconductedaqualitative\nAs the quantitative results indicate, even though GPT-4 is a\nsurveywhereparticipantscreatedtheirownprompts,providing\nlarger model than its baselines, it does not always yield better\na broader view of prompt engineering effectiveness. Another\nresults. Even typical prompt engineering strategies, such as\npotentialthreattovalidityarisesfromnotexplicitlyexamining\nin-contextlearning,didnotshowsignificantimprovement.On\nbiases such as task order and participant learning effects,\ntheotherhand,thequalitativeuserstudyrevealedthatthekey\nthough we mitigated order-related risks by randomizing task\npotentialofGPT-4isonconversation-basedprompting,which\nsequences across participants. The implications of participant\nincludes human feedback in the loop.\nsegregation, while a compelling avenue for future analysis,\nThere are two takeaway messages from our results: (a) To\nwere beyond the scope of this work due to brevity and will\nleverage LLMs to their best, a sequence of back-and-forth\nbe explored in future research.\nquerieswiththemodelmaybeneeded.Iterativeprocessessuch\nConstruct Validity: We used common metrics, such as\nas search-based optimization or reinforcement learning-based\nBLEU scores, as in the original benchmarks. However, these\nagents might be useful to \u201cengineer\u201d prompts for ASE tasks\nmetrics may not fully capture what they aim to measure.\nwith full automation. (b) Human feedback still plays a crucial\nIn the qualitative study, participants\u2019 evaluations of GPT-\nrole in optimizing the prompts, as shown by conversational\n4\u2019s responses might be subjective. To mitigate this, we also\nprompting.Thus,toremovehumansfromtheloopcompletely,\nperformedamanualanalysisbythreeco-authors.Investigating\nfuturestudiesareneededtoanalyzedeveloper-writtenprompts\nbetterhuman-alignedmetricsremainsanopenareainthefield.\nmore carefully and extract patterns that can be implemented\nConclusion Validity: The non-deterministic behavior of\nas rules, fitness functions, rewards, and policies.\nLLMs [43] means results may not be reproducible. We set\nTo summarize our findings, we conclude that, like most\nthe temperature parameter to zero to increase determinism\nengineeringproblems,thechoicebetweenpromptengineering\nbut acknowledge this does not eliminate the issue entirely.\nand fine-tuning LLMs is a trade-off. The following are some\nWe provide generated responses from the quantitative study\nof the main factors that play a role in the trade-off:\nand raw chat logs from the survey to address this. A further\n\u2022 Performance: The most explicit aspect is the perfor- limitationliesintheabsenceofstatisticaltestsforquantitative\nmance of the models. From the results, neither approach comparisons, as fine-tuned model baseline distributions were\nalways dominates the other. However, conversational unavailable (benchmark scores were sourced directly from\nprompt engineering has shown to have great potential. leaderboards), and the qualitative analysis\u2019s limited sample\n\u2022 Cost: Although fine-tuning is usually considered more size (6 samples per task) precluded meaningful statistical\nexpensive than prompt engineering, depending on the testing. While these constraints temper the robustness of our\nsubscription models of the commercial LLMs, inference conclusions,weintendtoaddresstheminfutureworkthrough\nfees can quickly become a huge burden. For example, expanded datasets and formal statistical evaluations.\nrunning GPT-4 for this study costs around 1,500 USD. External Validity: We used recent state-of-the-art fine-\nTherefore, developers who have a large number of in- tuned models for each task, but new baselines could alter our\nstances for inference should consider using a cheaper observations. Although we used widely accepted benchmark\nmodel or even fine-tuning a smaller in-house model. datasets,theydonotrepresentalldomains.Forthesurvey,we"}
{"instruction": "Explain this concept:", "response": "used 18 samples to limit participant time, acknowledging that [12] J. Shin, H. Hemmati, M. Wei, and S. Wang, \u201cAssessing evaluation\nthislimitsgeneralizability.Wechosea1-hoursurveyduration metricsforneuraltestoraclegeneration,\u201dIEEETransactionsonSoftware\nEngineering,2024.\ntorecruitmoreparticipants,balancingpracticalconstraintslike\n[13] X.Hu,G.Li,X.Xia,D.Lo,andZ.Jin,\u201cDeepcodecommentgenera-\nparticipant availability and budget. To mitigate this threat, we tion,\u201dinProceedingsofthe26thconferenceonprogramcomprehension,\nconsidered diverse domains and difficulty levels in the exam- 2018,pp.200\u2013210.\n[14] W.Sun,C.Fang,Y.Chen,G.Tao,T.Han,andQ.Zhang,\u201cCodesearch\nples. We also included three different project types (library,\nbased on context-aware code translation,\u201d in Proceedings of the 44th\nalgorithm, and web) to cover different domains. While in- InternationalConferenceonSoftwareEngineering,2022,pp.388\u2013400.\ncontext learning was one of the best automated prompting [15] Y.Li,S.Wang,andT.N.Nguyen,\u201cDear:Anoveldeeplearning-based\nstrategiesatthestartofourstudy,newermethodslikeRetrieval approach for automated program repair,\u201d in Proceedings of the 44th\nInternationalConferenceonSoftwareEngineering,2022,pp.511\u2013523.\nAugmented Generation (RAG) [89] and Self-Reflection [90]\n[16] Z.Feng,D.Guo,D.Tang,N.Duan,X.Feng,M.Gong,L.Shou,B.Qin,\ncould affect our findings, warranting further study. T.Liu,D.Jiangetal.,\u201cCodebert:Apre-trainedmodelforprogramming\nandnaturallanguages,\u201dinFindingsoftheAssociationforComputational\nVII. CONCLUSION Linguistics:EMNLP2020,2020,pp.1536\u20131547.\n[17] A. Kanade, P. Maniatis, G. Balakrishnan, and K. Shi, \u201cLearning and\nIn this paper, we evaluated GPT-4 using basic, in-context\nevaluating contextual embedding of source code,\u201d in International\nlearning, and task-specific prompts and compared them with conferenceonmachinelearning. PMLR,2020,pp.5110\u20135121.\nfine-tuned language models. We also conducted a user study [18] D. Guo, S. Lu, N. Duan, Y. Wang, M. Zhou, and J. Yin, \u201cUnixcoder:\nUnified cross-modal pre-training for code representation,\u201d in Proceed-\nwith 27 academic and 10 industry participants to assess the\ningsof the60thAnnual MeetingoftheAssociation forComputational\nquality of GPT-4\u2019s responses and examine how participants Linguistics(Volume1:LongPapers),2022,pp.7212\u20137225.\ndesignedconversationalprompts.Ourquantitativeresultsshow [19] W. Ahmad, S. Chakraborty, B. Ray, and K.-W. Chang, \u201cUnified pre-\ntraining for program understanding and generation,\u201d in Proceedings of\nthat GPT-4 does not significantly outperform the baselines.\nthe2021ConferenceoftheNorthAmericanChapteroftheAssociation\nQualitative results indicate that participants often request im- for Computational Linguistics: Human Language Technologies, 2021,\nprovements, add context, or give specific instructions, which pp.2655\u20132668.\nhelpsGPT-4generatebetterresponses.Ourstudysuggeststhat [20] T.AhmedandP.Devanbu,\u201cMultilingualtrainingforsoftwareengineer-\ning,\u201d in Proceedings of the 44th International Conference on Software\nGPT-4 has great potential for code tasks but requires careful Engineering,2022,pp.1443\u20131455.\nhuman verification and interpretation. [21] T.B.Brown,B.Mann,N.Ryder,M.Subbiah,J.Kaplan,P.Dhariwal,\nA. Neelakantan, P. Shyam, G. Sastry, A. Askell, S. Agarwal,\nREFERENCES A.Herbert-Voss,G.Krueger,T.Henighan,R.Child,A.Ramesh,D.M.\nZiegler, J. Wu, C. Winter, C. Hesse, M. Chen, E. Sigler, M. Litwin,\n[1] S.Wang,T.Liu,andL.Tan,\u201cAutomaticallylearningsemanticfeatures S. Gray, B. Chess, J. Clark, C. Berner, S. McCandlish, A. Radford,\nfordefectprediction,\u201dinProceedingsofthe38thInternationalConfer- I.Sutskever,andD.Amodei,\u201cLanguagemodelsarefew-shotlearners,\u201d\nenceonSoftwareEngineering,2016,pp.297\u2013308. 2020.[Online].Available:https://arxiv.org/abs/2005.14165\n[2] M.White,M.Tufano,C.Vendome,andD.Poshyvanyk,\u201cDeeplearning [22] L.Ouyang,J.Wu,X.Jiang,D.Almeida,C.L.Wainwright,P.Mishkin,\ncode fragments for code clone detection,\u201d in Proceedings of the 31st C. Zhang, S. Agarwal, K. Slama, A. Ray, J. Schulman, J. Hilton,\nIEEE/ACMinternationalconferenceonautomatedsoftwareengineering, F. Kelton, L. Miller, M. Simens, A. Askell, P. Welinder, P. Christiano,\n2016,pp.87\u201398. J.Leike,andR.Lowe,\u201cTraininglanguagemodelstofollowinstructions\n[3] P.YinandG.Neubig,\u201cTranx:Atransition-basedneuralabstractsyntax withhumanfeedback,\u201d2022.\nparser for semantic parsing and code generation,\u201d in Proceedings of\n[23] OpenAI,\u201cGpt-4technicalreport,\u201d2023.\ntheConferenceonEmpiricalMethodsinNaturalLanguageProcessing\n[24] H. Touvron, T. Lavril, G. Izacard, X. Martinet, M.-A. Lachaux,\n(DemoTrack),2018.\nT.Lacroix,B.Rozie`re,N.Goyal,E.Hambro,F.Azhar,A.Rodriguez,\n[4] H.Liu,J.Jin,Z.Xu,Y.Zou,Y.Bu,andL.Zhang,\u201cDeeplearningbased\nA. Joulin, E. Grave, and G. Lample, \u201cLlama: Open and efficient\ncode smell detection,\u201d IEEE transactions on Software Engineering,\nfoundationlanguagemodels,\u201d2023.\nvol.47,no.9,pp.1811\u20131837,2019.\n[25] H. T. et al., \u201cLlama 2: Open foundation and fine-tuned chat models,\u201d\n[5] M. Allamanis, M. Brockschmidt, and M. Khademi, \u201cLearning to rep-\n2023.\nresentprogramswithgraphs,\u201dinInternationalConferenceonLearning\n[26] A.C.etal.,\u201cPalm:Scalinglanguagemodelingwithpathways,\u201d2022.\nRepresentations,2018.\n[27] R.A.etal.,\u201cPalm2technicalreport,\u201d2023.\n[6] Y. Wei, C. S. Xia, and L. Zhang, \u201cCopiloting the copilots: Fusing\nlargelanguagemodelswithcompletionenginesforautomatedprogram [28] J. Y. Khan and G. Uddin, \u201cAutomatic code documentation generation\nrepair,\u201d in Proceedings of the 31st ACM Joint European Software using gpt-3,\u201d in Proceedings of the 37th IEEE/ACM International\nEngineeringConferenceandSymposiumontheFoundationsofSoftware ConferenceonAutomatedSoftwareEngineering,2022,pp.1\u20136.\nEngineering,2023. [29] S. Gao, X.-C. Wen, C. Gao, W. Wang, and M. R. Lyu, \u201cConstruct-\n[7] X. Gu, H. Zhang, and S. Kim, \u201cDeep code search,\u201d in Proceedings of ing effective in-context demonstration for code intelligence tasks: An\nthe 40th International Conference on Software Engineering, 2018, pp. empirical study,\u201d in Proceedings of the 38th IEEE/ACM International\n933\u2013944. ConferenceonAutomatedSoftwareEngineering,2023.\n[8] J.ShinandJ.Nam,\u201cAsurveyofautomaticcodegenerationfromnatural [30] S.FengandC.Chen,\u201cPromptingisallyourneed:Automatedandroid\nlanguage,\u201d Journal of Information Processing Systems, vol. 17, no. 3, bug replay with large language models,\u201d in 2024 IEEE/ACM 46th\npp.537\u2013555,2021. InternationalConferenceonSoftwareEngineering(ICSE),2023.\n[9] J.Shin,M.Wei,J.Wang,L.Shi,andS.Wang,\u201cThegood,thebad,and [31] M. Geng, S. Wang, D. Dong, H. Wang, G. Li, Z. Jin, X. Mao, and\nthemissing:Neuralcodegenerationformachinelearningtasks,\u201dACM X.Liao,\u201cLargelanguagemodelsarefew-shotsummarizers:Multi-intent\nTransactionsonSoftwareEngineeringandMethodology,vol.33,no.2, comment generation via in-context learning,\u201d in 2024 IEEE/ACM 46th\npp.1\u201324,2023. InternationalConferenceonSoftwareEngineering(ICSE),2024.\n[10] J.Shin,S.Hashtroudi,H.Hemmati,andS.Wang,\u201cDomainadaptation [32] P. Liu, W. Yuan, J. Fu, Z. Jiang, H. Hayashi, and G. Neubig, \u201cPre-\nfor code model-based unit test case generation,\u201d in Proceedings of the train, prompt, and predict: A systematic survey of prompting methods\n33rdACMSIGSOFTInternationalSymposiumonSoftwareTestingand innaturallanguageprocessing,\u201dACMComputingSurveys,vol.55,no.9,\nAnalysis,2024,pp.1211\u20131222. pp.1\u201335,2023.\n[11] J.Shin,R.Aleithan,H.Hemmati,andS.Wang,\u201cRetrieval-augmented [33] J. Liu, D. Shen, Y. Zhang, B. Dolan, L. Carin, and W. Chen, \u201cWhat\ntest generation: How far are we?\u201d arXiv preprint arXiv:2409.12682, makes good in-context examples for gpt-3?\u201d in Deep Learning Inside\n2024. Out:3rdWorkshoponKnowledgeExtractionandIntegrationforDeep"}
{"instruction": "Explain this concept:", "response": "LearningArchitectures,DeeLIO2022. AssociationforComputational translation:Astudyofbugsintroducedbylargelanguagemodelswhile\nLinguistics(ACL),2022,pp.100\u2013114. translatingcode,\u201din2024IEEE/ACM46thInternationalConferenceon\n[34] Y. He, S. Zheng, Y. Tay, J. Gupta, Y. Du, V. Aribandi, Z. Zhao, SoftwareEngineering(ICSE),2024.\nY. Li, Z. Chen, D. Metzler et al., \u201cHyperprompt: Prompt-based task- [53] Z. Yang, F. Liu, Z. Yu, J. W. Keung, J. Li, S. Liu, Y. Hong, X. Ma,\nconditioningoftransformers,\u201dinInternationalConferenceonMachine Z.Jin,andG.Li,\u201cExploringandunleashingthepoweroflargelanguage\nLearning. PMLR,2022,pp.8678\u20138690. models in automated code translation,\u201d Proceedings of the ACM on\n[35] S. Carta, A. Giuliani, L. Piano, A. S. Podda, L. Pompianu, and SoftwareEngineering,vol.1,no.FSE,pp.1585\u20131608,2024.\nS. G. Tiddia, \u201cIterative zero-shot llm prompting for knowledge graph [54] H. Husain, H.-H. Wu, T. Gazit, M. Allamanis, and M. Brockschmidt,\nconstruction,\u201d2023. \u201cCodesearchnetchallenge:Evaluatingthestateofsemanticcodesearch,\u201d\n[36] J.Wei,X.Wang,D.Schuurmans,M.Bosma,F.Xia,E.Chi,Q.V.Le, 2019.\nD. Zhou et al., \u201cChain-of-thought prompting elicits reasoning in large [55] S. Lu, D. Guo, S. Ren, J. Huang, A. Svyatkovskiy, A. Blanco,\nlanguagemodels,\u201dAdvancesinNeuralInformationProcessingSystems, C. Clement, D. Drain, D. Jiang, D. Tang, G. Li, L. Zhou, L. Shou,\nvol.35,pp.24824\u201324837,2022. L.Zhou,M.Tufano,M.Gong,M.Zhou,N.Duan,N.Sundaresan,S.K.\n[37] T.Shin,Y.Razeghi,R.L.LoganIV,E.Wallace,andS.Singh,\u201cAuto- Deng, S. Fu, and S. Liu, \u201cCodexglue: A machine learning benchmark\nprompt:Elicitingknowledgefromlanguagemodelswithautomatically datasetforcodeunderstandingandgeneration,\u201d2021.\ngeneratedprompts,\u201dinProceedingsofthe2020ConferenceonEmpirical [56] C.etal.,\u201cEvaluatinglargelanguagemodelstrainedoncode,\u201d2021.\nMethods in Natural Language Processing (EMNLP), 2020, pp. 4222\u2013 [57] J. Austin, A. Odena, M. Nye, M. Bosma, H. Michalewski, D. Dohan,\n4235. E. Jiang, C. Cai, M. Terry, Q. Le, and C. Sutton, \u201cProgram synthesis\n[38] K. Hambardzumyan, H. Khachatrian, and J. May, \u201cWarp: Word-level withlargelanguagemodels,\u201d2021.\nadversarialreprogramming,\u201dinProceedingsofthe59thAnnualMeeting [58] L. Phan, H. Tran, D. Le, H. Nguyen, J. Annibal, A. Peltekian, and\noftheAssociationforComputationalLinguisticsandthe11thInterna- Y. Ye, \u201cCotext: Multi-task learning with code-text transformer,\u201d 2021.\ntional Joint Conference on Natural Language Processing (Volume 1: [Online].Available:http://dx.doi.org/10.18653/v1/2021.nlp4prog-1.5\nLongPapers),2021,pp.4921\u20134933. [59] C. Raffel, N. Shazeer, A. Roberts, K. Lee, S. Narang, M. Matena,\n[39] T.-O. Li, W. Zong, Y. Wang, H. Tian, Y. Wang, S.-C. Cheung, and Y.Zhou,W.Li,andP.J.Liu,\u201cExploringthelimitsoftransferlearning\nJ. Kramer, \u201cNuances are the key: Unlocking chatgpt to find failure- with a unified text-to-text transformer,\u201d Journal of machine learning\ninducing tests with differential prompting,\u201d in Proceedings of the 38th research,vol.21,no.140,pp.1\u201367,2020.\nIEEE/ACMInternationalConferenceonAutomatedSoftwareEngineer- [60] W. Qi, Y. Gong, Y. Yan, C. Xu, B. Yao, B. Zhou, B. Cheng,\ning,2023. D. Jiang, J. Chen, R. Zhang, H. Li, and N. Duan, \u201cProphetNet-X:\n[40] S. Kabir, D. N. Udo-Imeh, B. Kou, and T. Zhang, \u201cIs stack overflow Large-scale pre-training models for English, Chinese, multi-lingual,\nobsolete?anempiricalstudyofthecharacteristicsofchatgptanswersto dialog, and code generation,\u201d in Proceedings of the 59th Annual\nstackoverflowquestions,\u201d2023. Meeting of the Association for Computational Linguistics and the\n[41] C. Wang, Y. Yang, C. Gao, Y. Peng, H. Zhang, and M. R. Lyu, \u201cNo 11th International Joint Conference on Natural Language Processing:\nmorefine-tuning?anexperimentalevaluationofprompttuningincode System Demonstrations, Online, Aug. 2021, pp. 232\u2013239. [Online].\nintelligence,\u201dinProceedingsofthe30thACMJointEuropeanSoftware Available:https://aclanthology.org/2021.acl-demo.28\nEngineeringConferenceandSymposiumontheFoundationsofSoftware [61] B.Shen,J.Zhang,T.Chen,D.Zan,B.Geng,A.Fu,M.Zeng,A.Yu,\nEngineering,2022,pp.382\u2013394. J.Ji,J.Zhaoetal.,\u201cPangu-coder2:Boostinglargelanguagemodelsfor\n[42] F.TradandA.Chehab,\u201cPromptengineeringorfine-tuning?acasestudy codewithrankingfeedback,\u201darXivpreprintarXiv:2307.14936,2023.\non phishing detection with large language models,\u201d Machine Learning [62] R. Li, Y. Zi, N. Muennighoff, D. Kocetkov, C. Mou, M. Marone,\nandKnowledgeExtraction,vol.6,no.1,pp.367\u2013384,2024. C. Akiki, L. Jia, J. Chim, Q. Liu et al., \u201cStarcoder: may the source\n[43] S. Ouyang, J. M. Zhang, M. Harman, and M. Wang, \u201cLlm is like a bewithyou!\u201dTransactionsonMachineLearningResearch.\nboxofchocolates:thenon-determinismofchatgptincodegeneration,\u201d [63] Z.Luo,C.Xu,P.Zhao,Q.Sun,X.Geng,W.Hu,C.Tao,J.Ma,Q.Lin,\n2023. and D. Jiang, \u201cWizardcoder: Empowering code large language models\n[44] H.Li,Y.Hao,Y.Zhai,andZ.Qian,\u201cAssistingstaticanalysiswithlarge withevol-instruct,\u201dinTheTwelfthInternationalConferenceonLearning\nlanguage models: A chatgpt experiment,\u201d in Proceedings of the 31st Representations.\nACMJointEuropeanSoftwareEngineeringConferenceandSymposium [64] Y. Wang, K. He, D. Fu, Z. Gongque, H. Xu, Y. Chen, Z. Wang,\nontheFoundationsofSoftwareEngineering,2023,pp.2107\u20132111. Y. Fu, G. Dong, M. Diao et al., \u201cHow do your code llms perform?\n[45] T.AhmedandP.Devanbu,\u201cFew-shottrainingllmsforproject-specific empowering code instruction tuning with high-quality data,\u201d arXiv\ncode-summarization,\u201d in Proceedings of the 37th IEEE/ACM Interna- preprintarXiv:2409.03810,2024.\ntionalConferenceonAutomatedSoftwareEngineering,2022,pp.1\u20135. [65] A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman,\n[46] T. Ahmed, K. S. Pai, P. Devanbu, and E. Barr, \u201cAutomatic semantic A. Mathur, A. Schelten, A. Yang, A. Fan et al., \u201cThe llama 3 herd of\naugmentation of language model prompts (for code summarization),\u201d models,\u201darXivpreprintarXiv:2407.21783,2024.\nin Proceedings of the IEEE/ACM 46th International Conference on [66] B.Chen,F.Zhang,A.Nguyen,D.Zan,Z.Lin,J.-G.Lou,andW.Chen,\nSoftwareEngineering,2024,pp.1\u201313. \u201cCodet:Codegenerationwithgeneratedtests,\u201d2022.\n[47] E. Shi, Y. Wang, L. Du, J. Chen, S. Han, H. Zhang, D. Zhang, and [67] A. Lozhkov, R. Li, L. B. Allal, F. Cassano, J. Lamy-Poirier, N. Tazi,\nH.Sun,\u201cOntheevaluationofneuralcodesummarization,\u201dinProceed- A. Tang, D. Pykhtar, J. Liu, Y. Wei et al., \u201cStarcoder 2 and the stack\ningsofthe44thinternationalconferenceonsoftwareengineering,2022, v2:Thenextgeneration,\u201darXivpreprintarXiv:2402.19173,2024.\npp.1597\u20131608. [68] S. Tipirneni, M. Zhu, and C. K. Reddy, \u201cStructcoder: Structure-aware\n[48] F.Mu,L.Shi,S.Wang,Z.Yu,B.Zhang,C.Wang,S.Liu,andQ.Wang, transformerforcodegeneration,\u201d2023.\n\u201cClarifygpt: A framework for enhancing llm-based code generation [69] Y. Wang, W. Wang, S. Joty, and S. C. Hoi, \u201cCodet5: Identifier-aware\nvia requirements clarification,\u201d Proceedings of the ACM on Software unified pre-trained encoder-decoder models for code understanding\nEngineering,vol.1,no.FSE,pp.2332\u20132354,2024. and generation,\u201d in Proceedings of the 2021 Conference on Empirical\n[49] Y. Li, D. Choi, J. Chung, N. Kushman, J. Schrittwieser, R. Leblond, MethodsinNaturalLanguageProcessing,2021,pp.8696\u20138708.\nT. Eccles, J. Keeling, F. Gimeno, A. Dal Lago et al., \u201cCompetition- [70] K. Papineni, S. Roukos, T. Ward, and W.-J. Zhu, \u201cBleu: a method for\nlevelcodegenerationwithalphacode,\u201dScience,vol.378,no.6624,pp. automaticevaluationofmachinetranslation,\u201dinProceedingsofthe40th\n1092\u20131097,2022. annualmeetingoftheAssociationforComputationalLinguistics,2002,\n[50] Y.Dong,X.Jiang,Z.Jin,andG.Li,\u201cSelf-collaborationcodegeneration pp.311\u2013318.\nviachatgpt,\u201dACMTransactionsonSoftwareEngineeringandMethod- [71] S.V.Stehman,\u201cSelectingandinterpretingmeasuresofthematicclassi-\nology,vol.33,no.7,pp.1\u201338,2024. fication accuracy,\u201d Remote sensing of Environment, vol. 62, no. 1, pp.\n[51] Y.Luo,R.Yu,F.Zhang,L.Liang,andY.Xiong,\u201cBridginggapsinllm 77\u201389,1997.\ncode translation: Reducing errors with call graphs and bridged debug- [72] S. Ren, D. Guo, S. Lu, L. Zhou, S. Liu, D. Tang, N. Sundaresan,\ngers,\u201d in Proceedings of the 39th IEEE/ACM International Conference M. Zhou, A. Blanco, and S. Ma, \u201cCodebleu: a method for automatic\nonAutomatedSoftwareEngineering,2024,pp.2448\u20132449. evaluationofcodesynthesis,\u201dCoRR,vol.abs/2009.10297,2020.\n[52] R. Pan, A. R. Ibrahimzada, R. Krishna, D. Sankar, L. P. Wassi, [73] C. Liu, X. Bao, H. Zhang, N. Zhang, H. Hu, X. Zhang, and M. Yan,\nM.Merler,B.Sobolev,R.Pavuluri,S.Sinha,andR.Jabbarvand,\u201cLostin \u201cImprovingchatgptpromptforcodegeneration,\u201d2023."}
{"instruction": "Explain this concept:", "response": "[74] Z. Yuan, J. Liu, Q. Zi, M. Liu, X. Peng, and Y. Lou, \u201cEvaluating Linguistics,vol.11,pp.1500\u20131517,2023.\ninstruction-tuned large language models on code comprehension and [83] T. Wu, E. Jiang, A. Donsbach, J. Gray, A. Molina, M. Terry, and\ngeneration,\u201d2023. C. J. Cai, \u201cPromptchainer: Chaining large language model prompts\n[75] S. Robertson, H. Zaragoza et al., \u201cThe probabilistic relevance frame- through visual programming,\u201d in CHI Conference on Human Factors\nwork: Bm25 and beyond,\u201d Foundations and Trends\u00ae in Information inComputingSystemsExtendedAbstracts,2022,pp.1\u201310.\nRetrieval,vol.3,no.4,pp.333\u2013389,2009. [84] T.Wu,M.Terry,andC.J.Cai,\u201cAichains:Transparentandcontrollable\n[76] B.KitchenhamandS.L.Pfleeger,\u201cPrinciplesofsurveyresearch:part5: human-ai interaction by chaining large language model prompts,\u201d in\npopulationsandsamples,\u201dACMSIGSOFTSoftwareEngineeringNotes, Proceedingsofthe2022CHIconferenceonhumanfactorsincomputing\nvol.27,no.5,pp.17\u201320,2002. systems,2022,pp.1\u201322.\n[77] G.Guest,A.Bunce,andL.Johnson,\u201cHowmanyinterviewsareenough? [85] D. Trautmann, \u201cLarge language model prompt chaining for long legal\nan experiment with data saturation and variability,\u201d Field methods, document classification,\u201d SwissText\u201923: The 8th edition of the Swiss\nvol.18,no.1,pp.59\u201382,2006. TextAnalyticsConference\u2013GenerativeAI&LLM,June12\u201314,2023,\n[78] M. B. Miles and A. M. Huberman, Qualitative data analysis: An Neucha\u02c6tel,Switzerland,2023.\nexpandedsourcebook. sage,1994. [86] X.Hu,G.Li,X.Xia,D.Lo,andZ.Jin,\u201cDeepcodecommentgeneration\n[79] N. Alshahwan, J. Chheda, A. Finegenova, B. Gokkaya, M. Harman, with hybrid lexical and syntactical information,\u201d Empirical Software\nI. Harper, A. Marginean, S. Sengupta, and E. Wang, \u201cAutomated unit Engineering,vol.25,pp.2179\u20132217,2020.\ntestimprovementusinglargelanguagemodelsatmeta,\u201d2024. [87] S.MacNeil,A.Tran,A.Hellas,J.Kim,S.Sarsa,P.Denny,S.Bernstein,\n[80] J.Yang,H.Jin,R.Tang,X.Han,Q.Feng,H.Jiang,S.Zhong,B.Yin, andJ.Leinonen,\u201cExperiencesfromusingcodeexplanationsgenerated\nand X. Hu, \u201cHarnessing the power of llms in practice: A survey on by large language models in a web software development e-book,\u201d\nchatgptandbeyond,\u201dACMTransactionsonKnowledgeDiscoveryfrom in Proceedings of the 54th ACM Technical Symposium on Computer\nData,2023. ScienceEducationV.1,2023,pp.931\u2013937.\n[81] Y. Chen, Q. Fu, Y. Yuan, Z. Wen, G. Fan, D. Liu, D. Zhang, Z. Li, [88] E.A.AlOmar,A.Venkatakrishnan,M.W.Mkaouer,C.D.Newman,and\nand Y. Xiao, \u201cHallucination detection: Robustly discerning reliable A.Ouni,\u201cHowtorefactorthiscode?anexploratorystudyondeveloper-\nanswers in large language models,\u201d in Proceedings of the 32nd ACM chatgptrefactoringconversations,\u201d2024.\nInternationalConferenceonInformationandKnowledgeManagement, [89] J. Chen, H. Lin, X. Han, and L. Sun, \u201cBenchmarking large language\n2023,pp.245\u2013255. modelsinretrieval-augmentedgeneration,\u201d2024.\n[82] N. M. Guerreiro, D. M. Alves, J. Waldendorf, B. Haddow, A. Birch, [90] N. Shinn, F. Cassano, A. Gopinath, K. Narasimhan, and S. Yao, \u201cRe-\nP. Colombo, and A. F. Martins, \u201cHallucinations in large multilingual flexion:Languageagentswithverbalreinforcementlearning,\u201dAdvances\ntranslationmodels,\u201dTransactionsoftheAssociationforComputational inNeuralInformationProcessingSystems,vol.36,2024."}
{"instruction": "Explain this concept:", "response": "1\nA Systematic Survey of Prompt Engineering on\nVision-Language Foundation Models\nJindong Gu, Zhen Han, Shuo Chen, Ahmad Beirami, Bailan He, Gengyuan Zhang, Ruotong Liao,\nYao Qin, Volker Tresp, Philip Torr\nAbstract\u2014Promptengineeringisatechniquethatinvolvesaugmentingalargepre-trainedmodelwithtask-specifichints,knownas\nprompts,toadaptthemodeltonewtasks.Promptscanbecreatedmanuallyasnaturallanguageinstructionsorgenerated\nautomaticallyaseithernaturallanguageinstructionsorvectorrepresentations.Promptengineeringenablestheabilitytoperform\npredictionsbasedsolelyonpromptswithoutupdatingmodelparameters,andtheeasierapplicationoflargepre-trainedmodelsin\nreal-worldtasks.Inpastyears,Promptengineeringhasbeenwell-studiedinnaturallanguageprocessing.Recently,ithasalsobeen\nintensivelystudiedinvision-languagemodeling.However,thereiscurrentlyalackofasystematicoverviewofpromptengineeringon\npre-trainedvision-languagemodels.Thispaperaimstoprovideacomprehensivesurveyofcutting-edgeresearchinprompt\nengineeringonthreetypesofvision-languagemodels:multimodal-to-textgenerationmodels(e.g.,Flamingo),image-textmatching\nmodels(e.g.,CLIP),andtext-to-imagegenerationmodels(e.g.,StableDiffusion).Foreachtypeofmodel,abriefmodelsummary,\npromptingmethods,prompting-basedapplications,andthecorrespondingresponsibilityandintegrityissuesaresummarizedand\ndiscussed.Furthermore,thecommonalitiesanddifferencesbetweenpromptingonvision-languagemodels,languagemodels,and\nvisionmodelsarealsodiscussed.Thechallenges,futuredirections,andresearchopportunitiesaresummarizedtofosterfuture\nresearchonthistopic.\nIndexTerms\u2014PromptEngineering,VisionLanguageModel,Multi-modalModel,NaturalLanguageProcessing,ComputerVision.\n\u2726\n1 INTRODUCTION\nP ROMPT engineering is an approach to adapting a large pre- task. Recently, prompting a pre-trained large model to adapt it\ntrained model, also known as a foundation model, to new for specific tasks has become a new trend. The key idea of\ntasks by augmenting the model input with task-specific hints. prompt engineering is to provide hints along with input to guide\nSpecifically, the model\u2019s input is augmented by an additional a pre-trained model for solving a new task using its existing\npart, called prompt, which could be manually created natural knowledge. If the hints are human-interpretable natural language\nlanguage instructions [4], automated generated natural language (hard prompts), the related studies have been referred to as In-\ninstructions[5],orautomatedgeneratedvectorrepresentations[6]. Context Learning [7], which enable the model to learn from task\nThe natural language instructions have been also referred to as instructions, demonstrations with a few examples, or supporting\ndiscretepromptsorhardprompts,whilethevectorrepresentations information in the context. Also, the hints could be continuous\narecalledcontinuouspromptsorsoftprompts. vector representations (soft prompts). The related work has been\nPromptengineeringhasindeedco-appearedandgainedpromi- referred to as Prompt-Tuning [6], which optimizes prompts di-\nnencewiththeemergenceoflargepre-trainedmodelsandtogether rectlyintheembeddingspaceofthemodel.\nledtoaparadigmshiftinmachinelearning(ML).Thetraditional Compared to the traditional paradigm, prompt engineering\nparadigm requires labeling a considerable amount of data and has multiple advantages. Firstly, it requires a few labeled data\nthen training a task-specific ML model from scratch or fine- to adapt a pre-trained model to new tasks, which greatly reduces\ntuningapre-trainedlargemodel.Themodel\u2019sperformanceheavily theeffortofhumansupervisionandcomputationresourceforfine-\nrelies on the quality and amount of labeled data, which can be tuning.Secondly,promptengineeringenablesapre-trainedmodel\nresource-intensive to acquire. Besides, the traditional paradigm to perform predictions on new tasks solely based on the prompt\nrequirestuningthemodel\u2019sparameterstosomeextent,i.e.,entire withoutupdatinganyofthemodel\u2019sparameters,allowingserving\nparameters in the case of training an ML model from scratch or a large scale of downstream tasks using the same model. This\nfullyfine-tuningapre-trainedmodelandpartialparametersinthe makesitpossibletoapplylarge-scalepre-trainedmodelsforreal-\ncaseofparameter-efficientfinetuning.Thislimitstheextensibility worldapplications.\nof an ML model and requires a specific model copy for each Prompt engineering has been first studied and popularized in\nnatural language processing (NLP) [8, 9], and then gained great\nattentionincomputervision[10,11],aswellasinvision-language\n\u2022 JindongGuandPhilipTorrarewithUniversityofOxford.\n\u2022 ZhenHan,ShuoChen,BailanHearewithLudwigMaximilianUniversity modeling [1, 12]. While there is an abundance of literature on\nofMunich. prompt engineering in the NLP domain, there is currently no\n\u2022 AhmadBeiramiiswithGoogleResearch. systematic overview available to provide insight into the current\n\u2022 GengyuanZhang,RuotongLiao,andVolkerTresparewithLudwigMaxi-\nstateofpromptengineeringonpre-trainedvision-languagemodels\nmilianUniversityofMunichandMunichCenterforMachineLearning.\n\u2022 Yao Qin is with Google Research and University of California, Santa (VLMs),whichpresenttheirownuniquechallenges.\nBarbara. In this paper, we aim to bridge this gap by providing a\n\u2022 CorrespondingE-mailfromJindongGu:{jindong.gu}@outlook.com.\ncomprehensive survey of cutting-edge research in prompt engi-\n3202\nluJ\n42\n]VC.sc[\n1v08921.7032:viXra"}
{"instruction": "Explain this concept:", "response": "2\n(a)Multimodal-to-TextGeneration (b)Image-TextMatching (c)Text-to-ImageGeneration\nFig.1:Vision-LanguageFoundationModels.Thecutting-edgeresearchinpromptengineeringonVision-LanguageFoundationModels\nis systematically summarized. Three main types of vision-language models are focused in this work, namely, multimodal-to-text\ngeneration models (e.g., Flamingo [1]) in subfigure a, image-text matching models (e.g., CLIP [2]) in subfigure b, and text-to-image\ngenerationmodels(e.g.,StableDiffusion[3])inthesubfigurec.Moredetailsofeachtypeareintroducedinthelatersections.\nneeringofpre-trainedVLMs.Specifically,weclassifyprompting 2.1 Terminology\nmethods into two main categories based on the readability of the\nThis is a list of terms along with their descriptions. Note that\ntemplates,i.e.,hardpromptandsoftprompt.hardpromptscanbe\ninsteadofformallydefiningthefollowingconcepts,weprovidea\nfurtherdividedintofoursubcategories,namelytaskinstruction,in-\ngeneraldescriptionforreaders.\ncontext learning, retrieval-based prompting, and chain-of-thought\nprompting.Softprompts,ontheotherhand,arecontinuousvectors \u2022 Prompt:Additionalinformationorhintsprovidedtoamodel\ntoguideitsbehaviororhelpitperformaspecifictask;\nthatcanbefine-tunedusinggradient-basedmethods.Notethatthis\nsurveyprimarilyfocusesonpromptingmethodsthatmaintainthe \u2022 PromptingMethod:Anapproachusedtoincorporateprompts\nmodel\u2019sarchitecture,andthus,themethodssuchasP-tuning[13] into the input to guide model behavior or enhance model\nand LoRa [14] that introduce additional modules into the model, performance;\narenottheprimaryscopeofthissurvey.\n\u2022 Multimodal-to-Text Generation: Generating textual descrip-\nWe investigate the prompt engineering on three types of VL\ntionsornarrativesfrommultimodalinputdata,e.g.,acombi-\nmodels,whicharemuiltimodal-to-textgenerationmodels,image-\nnationofvisionandlanguagedata;\ntext-matching models, and text-to-image generation models. A\ncleardefinitionofeachmodeltypeisprovidedinSec.2.1.More- \u2022 Image-TextMatching:Establishingasemanticrelationshipor\nover,wecategorizeexistingprompt-engineeringapproachesfrom alignmentbetweenimagesandtextualdescriptions;\nan encoder-decoder perspective as shown in Fig. 1, i.e., encode- \u2022 Text-to-Image Generation: Generating visual images from\nside prompting or decode-side prompting, where the prompts are textualdescriptions.\naddedtotheencoderanddecoder,respectively.\n\u2022 In-context Learning: A prompting method by providing\nThe rest of this paper is organized as follows. In Sec. 2, we\nmodels with instructions or demonstrations within relevant\nsummarize and define the taxonomy and notations that we use\ncontexts to solve new tasks without requiring additional\nacross this survey. Sec. 3, 4, and 5 present the current progress\ntraining.\nof prompt engineering on multimodal-to-text generation models,\nimage-text-matching models, and text-to-image generation mod- \u2022 Chain-of-thought: A prompting method that enhances rea-\nels, where each section first presents the preliminaries of the soning skills by instructing a model to generate a sequence\ncorresponding models followed by a detailed discussion of the of intermediary actions that guide towards solving a multi-\nprompting methods, then investigates the applications and the stepproblemandreachingtheultimatesolution.\nresponsibleAIconsiderationsofsuchpromptingmethods.Sec.6\nprovides a comparison between prompting unimodal models and\n2.2 Notations\nVLMs,andwemakeanin-depthdiscussionabouttheiranalogies\nanddifferences.Finally,inSec.7,wehighlightthechallengesand Thesearethemathematicalnotationsthatarefollowedthroughout\npotentialresearchdirections. the paper (Tab. 1). All the formulations of this work will stick to\nIn order to facilitate the literature search, we also build and thesenotationsunlessotherwisespecified.\nreleaseaprojectpage 1 wherethepapersrelevanttoourtopicare\norganizedandlisted.\n3 PROMPTING MODEL IN MULTIMODAL-TO-TEXT\nGENERATION\n2 TAXONOMY 3.1 PreliminariesofMultimodal-to-TextGeneration\nInthissection,termsandnotationsrelatedtoPromptingEngineer- Large language models (LLMs) have demonstrated impressive\ningonVLMsusedthroughoutthepaperareintroduced. capabilities in the field of NLP, prompting researchers to explore\nways of integrating visual modality into these models\u2019 training\nframework. This integration aims to enhance their linguistic\n1.https://github.com/JindongGu/Awesome-Prompting-on-Vision-\nLanguage-Model/ prowessandexpandtheirapplicabilitytomultimodaltasks."}
{"instruction": "Explain this concept:", "response": "3\nTABLE 1: The used mathematical notations are listed. They are </s> to mark the beginning and end of a sequence. In another\nfollowedthroughoutthepaper. approach, [1] employs <BOS> to represent the \u201cbeginning of\nsequence\u201d and <EOC> to denote \u201cend of chunk\u201d. These special\nx Acleaninputimage\ntokens serve to differentiate and identify the boundaries between\nt Asentencepairedwithanimage differentmodalities,allowingthemodeltoeffectivelyprocessand\ny Aground-truthclasslabelofanimage leveragemultimodalinformation.\nVisual Feature. To obtain a consistent representation of in-\n\u03c7 Inputdistribution\nput as a sequence of embeddings for both modalities, the im-\nf(\u00b7) Avision-languagemodel\nage x is transformed into a sequence of embedding vectors:\nfv(\u00b7) Avisualencoder x =< v 1,v 2,...,v\nM\n>.Accuratelyrepresentingtheinformation\nfe(\u00b7) Atextualencoder conveyed by images is crucial for downstream tasks but can be\n{vi}M\ni=1\nvisualtokens challenging. CNN structures have been commonly used in prior\nresearch for extracting image features. For instance, models like\n{ci}M\ni=1\ntextualtokens\nViLBERT [25] and VL-T5 [26] employ faster R-CNN [27] to\n{zi}M i=1 visualprompttokens detect object regions in images and encode them as a sequence\n{ti}M\ni=1\ntextualprompttokens of Region-Of-Interest (ROI) features. However, this approach\nmay overlook important regions in an image. To address this\nHl lthAlayerofthetargetnetwork\nlimitation, approaches like OFA [28] and Flamingo [1] utilize\nL Labelwordtoken\nResNettoencodeinformationfromtheentireimage,considering\nHi thekthactivationinlthlayerofthetargetmodel a broader context. Additionally, leveraging the powerful feature\nk\nzi Modeloutputlogits extractioncapabilitiesofthetransformerarchitecture,modelssuch\nas SimVLM [29], PaLI [30], MAGMA [31], and BLIP2 [32]\nadopt the Vision Transformer (ViT) [33] architecture for image\nrepresentation. This allows them to effectively capture visual\nTo maintain consistency with the training methodologies\ninformationandincorporateitintothemultimodalframework.\nemployed by LLMs, generation-based vision-language models\nFusion Module. The fusion module plays a crucial role in\n(VLMs) typically comprise three essential components: text fea-\nintegrating text and image embeddings to create a joint repre-\nture,visualfeature,andfusionmodule.Thesecomponentssyner-\nsentation.Awell-designedfusionmodulecancaptureinteractions\ngistically collaborate, enabling the models to effectively leverage\nand relationships between modalities, prevent information loss,\ntextualandvisualinformationtogeneratecoherentandcontextu-\navoid semantic mismatch, mitigate biases, and enables compre-\nallyrelevantoutputs.\nhensive understanding. For example, in Visual Question Answer-\nIncorporating the visual modality into LLMs has opened up\ning(VQA),thefusionmoduleenablesthemodeltoleverageboth\nexcitingopportunitiesforvariousapplications,suchasvisualcom-\ntextualandvisualinformationtounderstandthequestionandthe\nmonsense reasoning [15], visual question answering [1, 16, 17,\ncorrespondingimage,leadingtoaccurateanswers.Toimprovethe\n15], multimodal dialogue systems [18, 1, 12], etc. By combining\nability of answer generation, prompts can be manually designed\ntextualandvisualcues,VLMshavethepotentialtoprovideamore\nfor different tasks and included as part of the input to the fusion\ncomprehensive understanding of multimodal data and produce\nmodule. These prompts serve as additional information or cues\noutputsthatalignwithhuman-likereasoningandperception[19].\nthat guide the model\u2019s understanding of the question and the\nFurthermore, the fusion of text and visual features within VLMs\nimage. As for generation-based VLMs, there are two main types\nplays a crucial role in seamlessly integrating information from\nof fusion module approaches based on the integration of visual\nbothmodalities.Thisfusionprocessenablesthemodeltocapture\nand textual modalities: encoder-decoder as a multi-modal fusion\ninterdependencies and interactions between textual and visual\nmoduleanddecoder-onlyasamulti-modalfusionmodule.\nelements, resulting in more accurate and contextually grounded\nIn the encoder-decoder as a multi-modal fusion module ap-\ngenerations[19].\nproach, models like VL-T5 [26], SimVLM [29], OFA [28], and\nText Feature. Early studies on VLMs commonly em- PaLI [30] focus on creating a joint representation that combines\nployed the preprocessing technique introduced by BERT [20]. both modalities at an early stage. The overall formulation can be\nThe raw text undergoes tokenization and is concatenated representedas:\nwith special tokens, [CLS] and [SEP], represented as\n<[CLS],c ,...,c ,[SEP]>, where token c is associated\n1 m i y =G(E(x )) (1)\ninput\nwith a word embedding. However, with the progression of lan-\nguage model research, more advanced models have emerged, wherethex representsthegiveninputandydenotesthecor-\ninput\nshowcasing emergent abilities such as in-context learning [21] responding ground-truth, respectively. The fusion encoder func-\nandchain-of-thoughtreasoning[4].Buildingupontheseadvance- tionE integratesthevisualandtextualinformationtocreateajoint\nments, the latest generation of VLMs has embraced powerful representation that captures their interactions and dependencies.\nlanguage models like T5 [22] and GPTs [23], which further This fused representation is then fed into the generating module\nenhancestheirlinguisticcapabilities. G, which performs further processing and generates the desired\nTo accommodate different modalities in the input, recent outputsforthedownstreamtasks.\nworks have introduced new special tokens. For example, [24] Inthedecoder-onlyasamulti-modalfusionmoduleapproach,\nincorporate an additional image classification token [CLS_I], modelslikeFrozen[17],Flamingo[1],andMAGMA[31]directly\nwhile [15] use <image> and </image> to indicate the begin- combinesthevisualandtextualinformationinthedecodingstage,\nning and end of the encoded image embedding and <s> and withoutexplicitlycreatingajointrepresentationatanearlierstage."}
{"instruction": "Explain this concept:", "response": "4\nThis approach allows the model to effectively incorporate both thespecificapproachandtheavailablepromptpoolorknowledge\nmodalitiesduringthegenerationprocessandproducecontextually base. This method allows the model to benefit from existing\nrelevantoutputs.Theformulationcanberepresentedas: information and improve its performance by leveraging relevant\npromptsorcontextduringthegenerationprocess[38,39,40].\ny =G(x ) (2) Chain-of-Thought Prompting. Chain-of-Thought Prompting [4,\ninput\n9, 5] is a method where the model is prompted with a series of\nA special case is PICa [16], which represents images as\ninstructionsorquestionsthatprogressivelybuilduponeachother.\ntextualdescriptionsandutilizesGPT-3asthefusionmodule.This\nEachpromptinthechainaddscontextornarrowsdownthefocus,\napproachtreatsimagesastextandleveragesapre-trainedlanguage\nenabling the model to generate more coherent and contextually\nmodellikeGPT-3togenerateoutputsbasedonthepuretextinput.\nappropriate responses. This method helps the model maintain\nIn addition, BLIP-2 [32] examines the fusion of two distinct\na logical \u201cchain\u201d throughout the conversation. The formulation\nmodules for integration: the decoder-based OPT [34] and the\nfor the chain-of-thought prompting method does not involve a\nencoder-decoder based FlanT5 [35]. The study further offers an\nspecific equation but rather the iterative process of applying\nanalysis of the respective strengths and benefits offered by these\nprompts [4]. At each step l in the chain, the model\u2019s response\nfusionmodules.\nfrom the previous prompt is used as input for the next prompt.\nThis can be represented as Tl+1 = Tl(x,t). Here T represents\n3.2 Multimodal-TextPromptingMethods the prompt function that takes the image x and text t inputs and\nFig.2illustratestheclassificationofpromptingmethods.Prompt- generates a response. The output of the l-th prompt, denoted as\ning methods fall into two categories: hard prompts, which are Tl(x,t), serves as the input for the (l+1)-th prompt Tl+1. By\nlabor-intensive, manually crafted text prompts with discrete to- progressively building upon the previous prompts, the iterative\nkens, and soft prompts, which are optimizable, learnable tensors natureofthechain-of-thoughtpromptingmethodhelpsthemodel\nconcatenated with input embeddings, but lack human readability maintain coherence and generate responses that align with the\nduetotheirnon-alignmentwithrealwordembeddings. evolvingcontextoftheconversation[4].\n3.2.1 Hardprompt 3.2.2 Softprompt\nHardpromptsinvolvemanuallycrafted,interpretabletexttokens, Unlike hard prompts, soft prompts are characterized as contin-\ne.g., adding \u201cA photo of \u201d before the input for captioning tasks. uous vectors that can be fine-tuned using gradient-based meth-\nHardpromptscanbefurtherdividedintofoursubcategories:task ods[6,41].Forexample,thisprocessmightinvolveconcatenating\ninstruction, in-context learning, retrieval-based prompting, and a learnable vector with the input embeddings and subsequently\nchain-of-thought prompting. It is important to note that retrieval- optimizing these to align with a particular dataset. Soft prompts\nbased prompting is often used to select samples for in-context can be classified according to whether new tokens are internally\nlearning. incorporated within the model\u2019s architecture or simply attached\nTask Instruction Prompting. This method involves the use of to the input. This distinction generally relates to two specific\ncarefully designed prompts that provide explicit task-related in- strategies: prompt tuning and prefix token tuning. However, this\nstructionstoguidethemodel\u2019sbehavior[36,37].Theformulation surveyfocusesexclusivelyonpromptmethodsthatdonotinvolve\nfor this method can be represented as x input = H(x,t). Here, H modifyingtheunderlyingmodelitself,andthustechniqueslikeP-\nservesasthetaskinstructionfunction,takingtheimagexandtext tuning[13]andLoRa[14],whichalterthefundamentalstructure\ntasinputsandproducingthemodifiedinputrepresentationx input. ofthemodel,arenotwithintheprimaryscopeofthisstudy.\nIn-context Learning. In-context Learning [7, 23] is a method PromptTuning.Prompttuning[6]createscontinuousvectorrep-\nwhere the model is exposed to a sequence of related examples resentationsasinputhints.Duringthetrainingprocess,themodel\norprompts,enablingittolearnandgeneralizefromtheprovided learns to refine the prompts, aiming to improve its performance\ncontext.Thein-contextlearningmethodcanberepresentedusing on specific tasks. This method enables the model to dynamically\ntheequationx input =H(C,x,t).Here,Hdenotesthetaskinstruc- generateeffectivepromptsbasedonitsunderstandingofthetask.\ntionfunctionwhichintegratesthegivencontextC withtheimage Theobjectiveofprompttuning,withthepromptingparameterx ,\np\nx and text t inputs. The resulting modified input representation canbedemonstratedasfollows:\nx captures the model\u2019s understanding of the context and is\ninput\nusedtogeneratecoherentandcontextuallyrelevantresponses.By argminL(F(y i,x p)|y <i,x input) (3)\nexposingthemodeltoasequenceofrelatedexamplesorprompts,\nxp\nthe in-context learning method promotes improved performance whereF(y ,x )representsthemodel\u2019soutputgiventheprompt-\ni p\ninunderstandingandgeneratingresponses[4]. ing parameter x . Here y denotes the previously generated\np <i\nRetrieval-based Prompting. Retrieval-based Prompting [16, 38, outputs, and x refers to the modified input based on the\ninput\n39, 40] is a method that involves selecting prompts or context prompt. The objective of prompt tuning is to minimize the loss\nusing retrieval techniques. In this approach, the model retrieves L between the model\u2019s output and the desired output, given the\nrelevant prompts or context from a prompt pool or external previously generated outputs and the modified input. By contin-\nknowledge base to guide its generation or decision-making pro- uously refining the prompts through prompt tuning, the model\ncess. The retrieval-based prompting method can be denoted by adaptsitsbehaviorandimprovesitsperformanceonspecifictasks.\nthe formulation: C = R(x,t). In this equation, R signifies the Thedynamicgenerationofeffectivepromptsbasedonthemodel\u2019s\nretrievalmethodthatgarnerspertinentpromptsorcontextbasedon understanding enhances its capability to generate accurate and\ntheimagexandtexttinputs.TheretrievedcontextC isthenused contextuallyrelevantresponses.\nto guide the model\u2019s generation or decision-making process. It is Prefix Token Tuning. Similar to prompt tuning, prefix token\nworth noting that the retrieval method R can vary depending on tuning [42] involves adding task-specific vectors to the input."}
{"instruction": "Explain this concept:", "response": "5\nFig.2:Promptingmethodsinmultimodal-to-textgeneration.Promptingmethodscanbedividedintotwomaincategoriesbasedonthe\nreadability of the templates: hard prompt and soft prompt. Hard prompt encompasses four subcategories: task instruction, in-context\nlearning,retrieval-basedprompting,andchain-of-thoughtprompting.Softpromptsareclassifiedintotwostrategies:prompttuningand\nprefixtokentuning,basedonwhethertheyinternallyaddnewtokenstothemodel\u2019sarchitectureorsimplyappendthemtotheinput.\nHowever,thisstudyprimarilyconcentratesonpromptmethodsthatavoidalteringthebasemodel,excludingtechniqueslikeP-tuning\nandLoRathatmodifythemodel\u2019scorestructure.\nHowever,inthiscase,thesevectorsareinsertedinallmodellayers on the specified visual region. OFA [28] prompts the proposed\nand can be trained and updated independently while keeping the grounded question answering task using the template \u201cQ: what\nrestofthepre-trainedmodel\u2019sparametersfrozen. color is the car in the region? region: <x ,y ,x ,y > A:\u201d,\n1 1 2 2\nIt\u2019s worth noting that these prompting methods are not mutu- providing instructions for the model to answer the question by\nallyexclusive.Theycanbecombinedandusedtogethertoachieve referring to the specified visual region. Prompt tuning on OFA is\ndesiredresultsinvarioussettingsandtasks.Thechoiceofprompt- explored by [43], who introduce tunable prompt embeddings at\ning method depends on the specific task, dataset availability, and eachlayer.Experimentalresultsdemonstratethatthislightweight\nthe desired level of control and customization required for the prompt-tuning approach is not only efficient but also resilient\nmodel\u2019sbehavior. againstadversarialattacks.\nPrompting Models with Decoder-based Fusion Module. An-\nother line of research focuses on utilizing the decoder as a\n3.3 AdvancesinPromptingTechniquesforVLM\nfusionmoduleinVLMs.Frozen[44]andBLIP-2[45]exemplify\nThis section will overview the use of prompting techniques in models that employ image conditional prefix tuning. Frozen [17]\nvarious VLMs to boost performance. For a clear and structured introduces the concept of preserving the language capabilities\npresentation, models will be divided into two main types based of a LLM while incorporating visual information as a prefix. It\non their fusion modules: 1) models utilizing an encoder-decoder achievesthisbyfreezingthemodelandtrainingaseparatevision\nasthefusionmodule,and2)modelsemployingadecoder-onlyas encoder to represent images. In Frozen, visual information is\nthefusionmodule. representedasasequenceoftwoembeddings,servingasavisual\nPromptingModelswithEncoder-decoderastheFusionMod- prefix.Theauthorsalsoproposetaskinductiontechniques,suchas\nule.EarlystudiesinVLMsofteninvolveddesigningtask-specific instructingthemodelto\u201cAnswerwithdaxorblicket,\u201dandevaluate\narchitectures on top of transformer encoders. However, recent the model\u2019s performance with various forms and amounts of in-\nadvancements have introduced a unified vision-language frame- context learning for downstream tasks. To effectively facilitate\nwork that incorporates an encoder as the fusion module. Notable cross-modalalignment,BLIP-2[45]doesnotfine-tunethevision\nexamples of such models include VL-T5 [26], SimVLM [29], encoder.Instead,itintroducesaQueryingTransformer(Q-Former)\nandOFA[28].Theyemploytwomainpromptingmethods:hand- toextractvisualfeaturesfromthefrozenimageencoder,usingthe\ncraftedinstructionsandprompttuning. extractedqueryembeddingsassoftvisualprompts.MAGMA[31]\nBothVL-T5[26]andOFA[28]utilizetextprefixesasprompts. follows a similar approach to Frozen, incorporating a new image\nFor example, \u201cvqa:\u201d is used for vision question answering, and prefix encoder while keeping the language model frozen. Task\n\u201ccaption:\u201disemployedforimagecaptioningtasks.SimVLM[29] instructions,suchas\u201cApictureof \u201dareusedforimagecaptioning.\nintroduces the prefix \u201ca photo of:\u201d to enhance the quality of Flamingo [1] explores the capabilities of few-shot learning and\ndecoded captions. In addition, VL-T5 [26] introduces shared employsvariousprompttechniques.Theauthorsintroducespecial\nvisual sentinel tokens (<vis_i>) to specify corresponding im- tokens, <BOS> (beginning of sequence) and <EOC> (end of\nage regions of Region of Interest (RoI) features. Text sentinel chunk), to differentiate sample pairs. In the zero-shot scenario,\ntokens(<text_i>)areusedtoreplacecontiguoustext.Similarly, textpromptsthatdonotcontaincorrespondingvisioninformation\nOFA [28] generates location tokens that specify the position of areused.Inthefew-shotsetting,differentformattingisemployed\nthe region (<x 1,y 1,x 2,y 2>). These special tokens facilitate the forvarioustasks(e.g.,\u201cQuestion:{question}Answer:{answer}\u201d\nstructuredincorporationofvisualandtextualinformation. for visual question-answering tasks), and the retrieval-based in-\nBuilding upon these special tokens, VL-T5 [26] utilizes the context example selection (RICES) [16] approach is utilized to\nprompt \u201ccaption region: <vis_i>\u201d for the grounded captioning selectsuitablesamplepairsasprompts.Promptensemblingtech-\ntask, indicating that the model should generate a caption based niquesarealsoemployedtocalculatethefinalscores.Forspecific"}
{"instruction": "Explain this concept:", "response": "6\ntaskssuchasHatefulMemes,promptsaredesignedtoincorporate In-context Learning. Recent studies have demonstrated that\nprovided OCR information. Additionally, hand-crafted dialogue the in-context learning capabilities of language models can be\nprompts are specifically designed for presented dialogues. [30] successfully transferred to vision-language-generating models.\nextends the multilingual capabilities of LLMs to VLMs without Frozen [17] exhibits the capability of fast concept binding, en-\nfreezinganyparameters.Theyachievedthisbyexplicitlyspecify- abling the model to associate a new word with a visual category\ningtheintendedlanguageinthepromptinstruction.Forexample,a using only a few examples and immediately utilize that word\npromptmaybeformulatedas\u201cGeneratethealt textin<lang>\u201d, appropriately. While the model performs well in the two-way\nwhere <lang> represents the language code associated with the setting(twonewwords),thisabilityfailstotransfertothefive-way\ndesired text string. Furthermore, Microsoft proposes a series of setting (five new words). Experimental results also indicate that\nMultimodal Large Language Models (MLLM), namely Kosmos- increasing the number of in-context learning samples enhances\n1 [15] and Kosmos-2 [46]. These models possess the ability to modelperformance,butthereisasaturationpoint,andadditional\nperceive diverse modalities and evaluate a wide range of tasks, repeatedcontentcanevenleadtoadeclineinperformance.Similar\nincluding zero-shot, few-shot, and multimodal chain-of-thought conclusions have been drawn in the case of Flamingo [1]. Both\nprompting scenarios. Textual instructions are used to enable the Flamingo [1] and Kosmos-1 [15] demonstrate that employing\nmodel to better understand downstream tasks. For example, in individual text prompts instead of image-text pairs can improve\nKosmos-1 [15] phrases like \u201cHere are three/four/eight images:\u201d model performance. However, it is important to note that using\nand\u201cThefollowingimageis:\u201dareemployedfortheRavenIQtest. individualtextpromptscanintroducebiastothemodel[1].\nInchain-of-thoughtprompting,Kosmos-1firstusesaprompt(e.g., Prompt Tuning. [43] conducted a study on Prompt tuning in\n\u201cIntroducethispictureindetail:\u201d)toguidethemodeltogenerate generativemultimodalmodels.Theirfindingsindicatethatprompt\narationale.Then,atask-awarepromptincorporatingthegenerated tuning consistently exhibits greater robustness than finetuning\nrationaleisutilizedtoproducethefinalresults.BasedonKosmos- across various tasks. The study also highlights the impact of\n1 [15], Kosmos-2 [46] incorporating grounding and referring different setups on prompting performance, revealing that longer\ncapabilitiesbyusingtextspanwithboundingboxasprompt,i.e., promptswithmoreparameterscanfacilitateimprovements.How-\n\u201c<p> text span </p><box><loc1><loc2></box>, where ever, there is a diminishing marginal utility, and excessively long\n<loc1> and <loc2> are location tokens <p>, </p>, <box> prompts may even have a detrimental effect on performance.\nand </box> are special boundary and text span tokens, respec- Furthermore, the results suggest that inserting prompts at the\ntively. bottomlayersmightleadtobetterperformance.\nPICa [16] takes a different approach by not learning visual\nembeddings. Instead, it converts images into textual descriptions\n3.5 ApplicationofPrompting\nand queries GPT-3 directly to predict the answer. Leveraging the\nfew-shot learning ability of GPT-3, PICa adapts to the visual Prompting has been widely adopted in many vision-language\nquestion-answering (VQA) task with only a few in-context ex- tasks evolving text generation, demonstrated promising results,\namples during inference time. GPT-4 [18], the latest version of andinspiredanewlearningparadigm,i.e.,in-contextlearning.\nChatGPT, has been introduced as an advanced VLM. In addition VisualQuestionAnswering.Thegoalofvisualquestionanswer-\ntoemployingtask-specifichardprompts,GPT-4alsoincorporates ing (VQA) is to train models to understand the information in\nthe in-context learning approach to tackle complex tasks such as an image and answer questions about it in natural language. In-\nAPArtHistory[47]. contextpromptsshowsurprisingresultsinfew-shot[1,16,17,15]\nand zero-shot scenarios [29, 30, 1, 15]. Some work also applies\n3.4 UnderstandingPrompting prompts to web page question answering [15] and grounded\nTo deeply understand the factors impacting prompting in question answering [28]. Web page question answering aims to\nmultimodal-to-text generation models, the following aspects will find answers to questions from web pages which requires com-\nbeintroduced: prehensionofboththesemanticsandstructuresoftexts.Huanget\nDataset-specific Prefixes. The choice of text prompts can have al. [15] uses the template prompt \u201cGiven the context below from\na significant impact on the performance of models. VL-T5 [26] the web page, extract the answer from the given text like this:\nexperimentedwithasingleprefix\u201cvqa\u201dforbothVisualQuestion Question: Who is the publisher of this book? Answer: Penguin\nAnswering(VQA)andGQA[48]tasks.Theresultsdemonstrated Books Ltd. Context: {WebText} Q: {question} A: {answer}\u201d\nthat a single model can effectively handle multiple VQA tasks where {WebText} stands for the text extracted from web pages.\nwithouttheneedfordataset-specificprefixes. Grounded question answering is firstly designed to reflect the\nFreezingtheLanguageModel.Manyexplorationsofprompting strongtransferabilityoftheOneForAll(OFA)model[28].Inthis\ninmultimodal-to-textgenerationmodelsrelyonthepowerfulgen- task, the model should answer a question about a certain region,\nerativecapabilitiesoflanguagemodels.Topreservetheextensive andspecialregiontokensforhardpromptsaredesigned.\ncapabilitiesofLLMs,approacheslikeFrozen[17],MAGMA[31], Visual Commonsense Reasoning. This task requires an under-\nFlamingo [1], and BLiP2 [32] freeze the language model during standing of the properties of everyday objects in the real world,\ntraining.Thispreventsknowledgelossandenablestheretentionof suchasobjectsizereasoningandobjectcolorreasoning[15].The\npromptcapabilities.Ontheotherhand,approacheslikeOFA[49] modelisrequiredtopredictthesizeorcolorrelationbetweenThe\nandKOSMOS-1[15]directlyadopttheencoder-decoderstructure Kosmosmodel[15]usesexamplepromptslikeIs{Item1}larger\nwithout additional model components to pursue unified models. than{Item2}?{Answer}andThecolorof{Object}is?{Answer}\nHowever, fine-tuning the language model alone can lead to a inthezero-shotscenariosandachievespromisingresults.\ndecrease in language ability. To address this, both OFA [49] Zero-shotImageClassification.Promptingcombinedwithlarge-\nand KOSMOS-1 [15] add language-only tasks during training to pre-trainedmultimodalmodelshasshowngreattransferabilityon\npreventthelossoflanguageability. out-of-domain test data such as zero-shot image classification."}
{"instruction": "Explain this concept:", "response": "7\nKosmos [15] concatenates the input image with a prompt like A classic matching loss is formulated as below to align the\nThephotooftheandletsthemodelcompletethepromptsentence imageandtextembeddingswithanImage-to-TextlossL anda\ni2t\nwith the predicted class. Besides, to incorporate additional rules Text-to-ImagelossL .\nt2i\nin the classification, Kosmos also sends class descriptions along\nw Imit ah gp ero Cm ap pt ts ioto nip nr go .m Gp et nt eh re atm ino gde dl ef so cr ria pts iop nec sifi gc ivc ea nte ag nor imy.\nage is a L =\u2212\n1 (cid:88)N\nlog(\nexpsim(f vl(v i),f tl(t i))\n) (4)\ntypical multimodal-to-text generation task that requires the com- i2t N i (cid:80)N j expsim(f vl(v i),f tl(t j)\nprehensionofbothvisionandlanguageinformation.Promptsare\nu ps oe wd erm fuo lst cly apin acf ie tyw .- Fsh lao mta inn gd oze [r 1o ]-s ah no dts Pc ae Ln Iar [io 3s 0]an ad dod pe tm po rn os mtr pat te\ns L t2i =\u2212\nN1 (cid:88)N\nlog(\n(cid:80)e Nxp exs pim s( imf tl (( ft\ni\nl) (, tf )vl ,( fv\nli\n() v)\n)) (5)\nto generate image captions in few-shot settings. For example, i j t i v j\nPaLI[30]usesthepromptGeneratethealt textinEN togenerate By prompting, we substitute model input by following learn-\nimage captions. Prompts are also studied in zero-shot settings, ableprompts:fortextualprompts:fl([{v }M ,{z }M ])andfor\nv i i=1 i i=1\nsuchasBLIP-2[32],MAGMA[31],SimVLM[29],andOFA[28]. thegeneralvisualprompt:fl([{c }M ,{t }M ])whereM isthe\nt i i=1 i i=1\nChatbot. The advent of chatbots such as ChatGPT [50] is one numberofpromptsweuse.\nof the most remarkable breakthroughs in AI research. Following\nwork such as Visual ChatGPT [12] and GPT4 [18] extend chat-\n4.2 PromptingTextEncoderofVLM\nbots to multimodal applications which support both images and\nPrompting language models has been long studied. As discussed\ntext prompts. Visual ChatGPT [12] is built based on ChatGPT\nin Sec. 3.2, we categorize prompts into hard prompts and soft\nand visual foundation models. It uses a Prompt Manager which\nprompts in this section. As shown in Fig. 3(a), learnable textual\nspecifiesinput-outputformats,convertsvisualinformationtolan-\nprompts are optimized on image-text pairs in a supervised man-\nguageformat,andhandleshistoriesofdifferentvisualfoundation\nner. Recent works [60] also investigate a different scenario with\nmodels. GPT4 [18] is able to accept prompts consisting of both\nunlabelled data. In this section, we will delve into the details of\nimagesandtexts,whichletsusersspecifyanyvisionandlanguage\nsoft prompts, exploring different types, such as global prompts,\ntaskbygeneratingtextoutputsgivenarbitrarilyinterlacedtextand\ntask-specificprompts,andinstance-specificprompts.\nimage prompts. Besides, some work migrates GPT to a specific\ndomainsuchasBiomedGPTonbiomedicalresearch[51].\n4.2.1 Hardprompt\nPrompting language models within the context of VLMs has\n3.6 ResponsibleAIConsiderationsofPrompting been extensively investigated. The introduction of prompts has\nLanguage-based VLMs inherit the risks of the underlying LLMs played a pivotal role in discovering and utilizing large-scale pre-\nand vision models, such as gender and racial biases when trained Language Models. Textual prompts mitigate handcrafted\nprompted with images [52]. Several surveys on the ethics of text templates (e.g., ,\u201ca photo of a [CAT]\u201d), which enables\nLLMsareavailable[52,53].Someworkstudiestherobustnessof the model to understand and respond to specific tasks without\nVLMsagainstbothnaturaldistributionshifts[54]andadversarial requiringexplicittask-specifictraining,showcasingtheflexibility\nrobustness [55]. A recent study [56] investigates the robustness and versatility of the model. [2] utilize hard prompts to test its\nof prompt tuning on VLMs against natural distribution shifts. zero-shot performance on several tasks. Hard prompts demand\nMoreover, [57] proposes robust prompt tuning on VLMs by significant expertise in the domain and often involve high costs.\nintegratingmultiple-scaleimagefeaturesintotheprompt. Thishasgivenrisetoanewlearningparadigmascarefullyrefining\npromptstooptimizeperformance.\n4 PROMPTING MODEL IN IMAGE-TEXT MATCHING 4.2.2 Softprompt\nThetaskofselectinganappropriatepromptisacomplexendeavor\n4.1 PreliminaryofImage-TextMatchingModels\nthat demands experience and domain expertise, and significantly\nMatching-basedVLMshaveintroducedanoveltrainingparadigm impacts model performance. This raises an important question:\nthatfacilitatestheacquisitionofjointmulti-modalrepresentations. can we dynamically \u2019search\u2019 for optimal prompts using gradient-\nProminent models in this field, such as CLIP [2], ALIGN [58], descent-based learning methods? Soft prompts refer to prompts\nALBEF [58] and Multi-Event CLIP [59], leverage contrastive that incorporate learnable parameters within their design. We\nlearningtechniquestoachievejointrepresentationsforimagesand categorizesoftpromptsintothreemaintypes:globalsoftprompts,\ntextswithalearningobjectivethataimstobringtherepresentation group-specificprompts,andinstance-specificprompts.\nof an image-text pair closer together while pushing non-pairs Global Soft Prompt. A straightforward yet powerful approach\nfurtherapart. toadaptinglanguagemodelsfordownstreamtasksinvolvesmod-\nBy expanding training datasets [2] and scaling up the model ifying template tokens specifically for those tasks. Studies such\nparameter, matching-based models exhibit adaptability across a as [61, 62, 63] have employed learnable prompts as input token\nbroad spectrum of downstream tasks, including zero-shot bench- embeddings when dealing with new tasks. Compared to fine-\nmarksandfine-tuningscenarios. tuning the entire model, learning a small set of prompt embed-\nDepending on the target of prompting, existing methods can ding parameters proves to be more parameter-efficient and data-\nbe classified into three categories: prompting the text encoder, efficient.Theseprompts,referredtoas\u201cglobalsoftprompts,\u201dare\npromptingthevisualencoder,orjointlypromptingbothbranches utilized consistently across all instances within a given task. The\nasshowninFig.3.Theseapproachesaimtoenhancetheflexibility term \u201cglobal\u201d signifies their universal usage throughout the task,\nandtask-specificperformanceofVLMsinrecentstudies. enablingthemodeltogeneralizeandperformwellacrossinputs."}
{"instruction": "Explain this concept:", "response": "8\nFig.3:PromptingtuningonImage-TextMatchingVLMscanbeappliedindifferentbranches:(a)Textprompting,(b)Visualprompting,\nand(c)Unifiedprompting,ontheinputdata.Lightblueboxesdenotelearnableparameters.Amatchinglossisemployedtooptimize\noverasmallnumberoflearnableparameters,inthelossformulationqdenotesthequeryingmodalityandkdenotesthetargetmodality.\nGroup-specific Prompt. Several recent studies [63, 64, 65] have embeddings. [68] proposes applying normalized visual prompts\nemployed a group of soft prompts specifically tailored to adapt to augmented images, unleashing the potential of visual prompt-\nto different tasks or types of inputs. These models enable the ing in diverse data settings. In terms of promoting diversity in\nmodels to query and select appropriate prompts dynamically. prompts,[69]employsdifferentvisualpromptsfordistinctsubsets\n[63,64,65]useagroupofsoftpromptstargetedtoadaptdifferent ofdata.\ntasks/types.Differentpromptsarequeriedbasedontheinputdata. Annotation Prompts. Visual prompting can also be performed\nCoOp [63] finds that using different context prompts for classes explicitly by directly manipulating images, similar to the process\n(class-specific context) can enhance performance in fine-grained ofannotation,whichwetermannotationprompts.ColorfulPrompt\nclassification. [64] uses task-specific prompts to adapt CLIP on Tuning (CPT) introduced in [70] focuses on colorizing specific\na wide range of video understanding tasks. [65] proposes to use regionsofimagesasvisualprompts.Byincorporatingcolorcues,\nMVLPTwithdifferenttaskpromptsforsourceandtargettasksto the model is guided to ground objects and better understand\nshareknowledgeacrosstask-specificprompts. the visual context. [71] explores the use of annotations, such\nInstance-specific Prompt. While effective in some cases, task- as red circles, as an innovative visual prompting design. These\ngrouped prompts can suffer from overfitting issues and may annotations serve as cues to guide the model\u2019s attention toward\nstruggle to adapt to unseen classes or novel samples. In contrast, specific areas of interest, thereby enhancing its understanding\ninstance-specificpromptsaimtocustomizepromptsforindividual of images. The study delves into CLIP\u2019s emergent ability to\nsamples,allowingforamorepersonalizedandadaptiveapproach. comprehend images through the clever use of visual prompts.\nCoCoOp [66] is a model that adopts instance-adaptive prompts, Furthermore, [72] proposes the use of example input and output\nspecificallyinstance-specificprompts,insteadofrelyingsolelyon images as visual prompts. By providing a pair of images demon-\nglobal prompts. This approach has been shown to enhance the strating a desired task, such as image inpainting, edge detection,\ngenerabilityofthemodel. or image colorization, the model is guided to complete similar\ntasksbasedontheprovidedexamples.Thesestudiesdemonstrate\n4.3 PromptingImageEncoderofVLM thecreativeandeffectiveuseofexplicitvisualpromptingmethods\nIn line with the achievements of prompt tuning in Natural Lan- and offer a practical and interpretable approach to improving the\nguageProcessing,therehavebeenendeavorstoextendtheconcept model\u2019sperformance.\ntovisualinputs.Accordingtothewayofdesigningvisualprompts,\nwe categorize them into two classes: patch-wise prompts, where\n4.4 UnifiedPromptingonVLM\nprompts are added as visual patches that are prepended to the\noriginalimages,andannotationpromptswhichinvolveannotating As prompt engineering continues to advance in both the vision\npromptsdirectlyontherawimages and language branches, there has been a recent development in\nPatch-wisePrompts.Addinglearnablepatchesasvisualprompts joint prompting. This approach aims to enhance matching-based\nisanintuitivemethodtoincorporatevisualcuesintoVLMs.Just VLMs by leveraging prompts from both the visual and language\nas textual soft prompts serve as input tokens, [67] introduces domains. As in Fig. 3, learnable prompts in both branches are\nVisual Prompt Tuning (VPT), which learns a small set of visual optimized. According to whether the visual prompt and textual\nprompts as visual patches. These patches are concatenated with prompt are independent of each other, they can be categorized\ninput images to adapt pre-trained models to new tasks. VPT intocoupledanddecoupledunifiedprompting,respectively,inout\ninvestigates visual prompts in the input and latent layers and follow-updiscussion.\noutperforms most other adaptation methods like full fine-tuning. Coupled Unified Prompting. UPT [49] issues that prompting\nInasimilarvein,[10]explorestheuseofvisualperturbationasa singlemodalitydoesnotfitallcases:textualpromptsmaystruggle\nvisualpromptingtechnique.Throughadversarialreprogramming, to handle data with high intra-class visual variance, while visual\nthe model learns to add visual prompts to input images. Addi- prompts may struggle with data exhibiting high inter-class visual\ntionally, [65] adopts patchified visual tokens as learnable input variance. Thus it employs a tiny neural network to optimize"}
{"instruction": "Explain this concept:", "response": "9\nlearnable textual and visual prompts jointly and finds unified with soft prompts to generate relation predictions. [83] presents\npromptingoutperformsanyunimodalprompting. a Relation Prompt for video open-vocabulary relation detection\nDecoupledUnifiedPrompting.[65]employssofttextualprompts by generating subject-object sensitive prompts based on object\nand VPT-like visual prompts on both the language and vision motioncues.\nbranches. This approach leverages the benefits of prompt engi- SemanticSegmentation.Semanticsegmentationisaclassiccom-\nneeringinbothmodalities [70]introducesaninnovativeapproach puter vision task with the goal of assigning each pixel to a class\nthatcombinesvisualandtextualsub-promptsforvisualgrounding label. DenseCLIP [84] converts image-text matching to pixel-\ntasks. By utilizing regions in the image as visual prompts and text matching to enable a pixel-wise dense prediction including\nphrases in a sentence as textual sub-prompts, the model can semantic segmentation task; class-conditioned text prompts are\nestablishco-referenceacrossdifferentmodalities.[73]introduces used to contextualize visual cues in texts. Segment anything [85]\nMaPLe hierarchical prompts on both branches and synergizes presents a large-scale foundation model for segmentation which\nprompt training in both modalities via a Vision-Language cou- takesimagesandpromptablesegmentationqueriesasinputs.\npling function. By leveraging the strengths of both visual and Notonlyonasingletaskbutpromptinghasalsobeenproven\ntextualprompts,jointpromptingcontributestomoreeffectiveand to be beneficial for domain adaptation and generalization of pre-\nversatileVLMsthatexcelinmultimodalunderstanding. trained models. Further studies investigate prompt tuning on pre-\ntrainedmodeltransferabilityunderdistributionshift.\nDomain Adaptation. Prompt learning also enables continual\n4.5 ApplicationofPrompting\nlearningofpre-trainedmodelsintasksliketest-timedomainadap-\nPrompting matching-based VLMs offers the promise of transfer- tation, which aims to adapt models to unlabeled test data under\nringrepresentationslearnedbypre-trainedmodelstodownstream a distribution shift. [86] attempts to embed domain information\ndomains and niche tasks like pure-vision tasks including im- discrepancy in domain-specif textual prompts and can preserve\nage/videoclassification,semanticsegmentation,relationdetection, semantic features of pre-trained VLMs. [87] adds prompts to\nandmultimodaltasks. different stages of ViT and fine-tunes prompts in the unlabelled\nImage Classification. Image classification is extensively re- targetdomain.\nsearched in computer vision for many years. A new approach Continual Learning. Continual learning is aimed at tackling\nto object classification has been suggested by prompting the text catastrophicforgettinginnon-stationarydatadistribution.Prompt\nencoders in VLMs. A Na\u00a8\u0131ve solution to image classification is tuningbecomesanewmethodologyforcontinuallearning.Learn-\nusing a fixed prompt like \u201cAn image of [CLASS]\u201d as in ing to Prompt (L2P) [88] shows a brand new prompt-based\nCLIP [2] and TPT [62], which uncovers pre-trained capacities approachforcontinuallearningbyqueryingtrainabletask-specific\nof zero-shot classification performance. Accordingly, learnable promptsfromapromptpoolforeachinputinstanceandprepends\nprompts are adapted to image classification in works like [62] it to input before pre-trained models to instruct the model. [89]\nPrompt engineering also shows its efficacy in more challenging presents DualPrompt, to learn task-invariant and task-specific\nclassification tasks like long-tailed classifcation [74], multi-label instructions across tasks, unlike that L2P uses only one prompt\nclassification[75,76]. toolandcallsthemGeneralandExpertpromptspace.\nText Classification. Text classification appears to present a dual Domain Generalization Domain generalization targets adapting\nchallengeakintothatofimageclassification.Duetothescopeof modelstounseendomainsinthetrainingstage.[90]encapsulates\nthis survey, we only cover works that focus on text classification domain-specific knowledge in domain prompts generated by a\nofVLMs.[77]usesvisualpromptsconcerningdifferentclassesto prompt adapter and prepends it with input data; at test-time,\nbetterleveragevisualinformationfortextclassification. promptsaregeneratedbasedonthesimilaritiesbetweendomains.\nObject Detection. Object detection is aimed at predicting class\nlabels of object bounding boxes in an image. With abundant\n4.6 ResponsibleAIConsiderationsofPrompting\ninformation on classes in texts, prompt engineering is also used\nformulti-labelrecognition.[76]proposesDualContextOptimiza- The importance of AI Integrity and ethics has been attached to\ntion (DualCoOp) using labels as a part of prompts and learning prompting matching-based VLMs to construct trustworthy mul-\npositiveandnegativepromptpairstoalignimagesandpromptsto timodal models. The discussion covers model robustness, safety,\nsolvemulti-labelrecognitiontasks.[75]proposedTexts-as-Images fairness,bias,privacy,andthelike.\n(TaI)promptingformulti-labeldetection.Open-vocabularyobject Adversarial Robustness of Prompt. Robustness analysis eval-\nclassificationisapromisingapplicationofpromptengineeringin uates the performance of the model under different conditions\nobject classification, where the detectors can predict new classes andperturbations.[91]studieshowVPTandfine-tuningimprove\nthat are not in training. ViLD [78] generate a fixed prompt zero-shot robustness under adversarial attack on CLIP and finds\ntemplate, e.g., \u201ca photo of [CATEGORY] in the scene\u201d. [79] that VPT is more effective in the absence of texts. [92] also\nintroduces detection prompt (DetPro) to learn continuous prompt attempt to leverage universal visual prompting to improve the\nrepresentations. PromptDet[80] uses regional prompt learning to adversarialrobustnessattesttime.Visualpromptingismoreflex-\nalignregionfeaturesandtextfeatures. ible compared to conventional adversarial defenses, as it allows\nVisualRelationDetection.Visualrelationdetectionisacomputer universal (i.e., data-agnostic) input prompting templates, which\nvision task that targets extracting relations between objects in an are capable of plug-and-play during testing. [93, 94] investigate\nimage. Prompt tuning boosts visual relation detection with its thereasonsbehindVLMs\u2019robustnesstonaturaldistributionshifts\npowerful commonsense knowledge contained in LLMs. [81] op- systematicallyandrevealsthatdiversetrainingdataistheprimary\ntimizes a small continuous task-specific vector for visual relation reason for robustness gain. [95] explores the model vulnerability\ndetection.[82]pre-trainsVLMswithamatching-basedstrategyto that injecting triggers brings to pre-trained models in prompt\nalign image regions and dense captions and fine-tune a decoder tuning."}
{"instruction": "Explain this concept:", "response": "10\nRecently, the diffusion model (DM) has spurred another line of\nstate-of-the-artmodelsfortext-imagegeneration[114].Diffusion\nmodels,alsoknownasdiffusionprobabilisticmodels[115],origi-\nnatefromnon-equilibriumstatisticalphysics[116]andsequential\nMonte Carlo [117] and are designed to fit any data distribution\nwhile keeping tractable. The denoising diffusion probabilistic\nmodels (DDPMs) [118] first adopt DMs in the image generation\ndomain and inspire the whole community of generative models.\nIninference,DDPMsbuildaMarkovchainthatgeneratesimages\nfrom noisy data within finite transitions which is called reverse\nFig. 4: Prompting Generative Models. A depiction of a typical\nprocess.Intraining,DDPMslearnfromtheforwardprocesswhere\ntext-to-image generation framework, detailing elements such as\nnoise is added to the natural images and estimated by the model.\nconditional information, an image encoder, a generative model,\nGivenacleanimagex fromadistributionq,diffusionstepT and\n0\nnoiseinjection,latentspacerepresentation,andadecoder.Condi-\nhyperparameters\u03b2 ,theforwardprocessgeneratesx following\nt T\ntional information can take various forms such as hard prompts,\nT\nlearnablesoftprompts,oracombinationofthetwo.Furthermore, (cid:89)\nq(x |x ):= q(x |x ), (6)\npromptscanbepresentedintextual,visual,orbothformats. 1:T 0 t t\u22121\nt=1\n(cid:16) (cid:112) (cid:17)\nq(x |x ):=N x ; 1\u2212\u03b2 x ,\u03b2 I . (7)\nt t\u22121 t t t\u22121 t\nBackdoor Attack of Prompt Learning. [96] studies on the\nThe noised image from any arbitrary step t then can be\nbackdoor and poisoning attacks on CLIP and find CLIP trained\nreformulatedas\nonmanuallylabeleddatasufferbadlyfromsuchattacks.Itshows\n\u221a\nthatthetrainingonnoiseanduncurateddatasetsmakesbackdoor q(x |x ):=N (cid:0) x ; \u03b1\u00af x ,(1\u2212\u03b1\u00af )I(cid:1) , (8)\nt 0 t t 0 t\nand poisoning attacks a significant threat. [97] proposes a new\nwhere\u03b1 :=1\u2212\u03b2 ,\u03b1\u00af :=(cid:81)t \u03b1 .\nbackdoorattackmethodnamedBadEncoderonCLIPandexposes t t t s=0 s\nGiventhedefinedforwardprocess,theDDPMistrainedinthe\nthis threat to VLMs. Once a pre-trained image encoder has been\nreverseprocesswhichstartsfromp (x )bythelossdefinedas\ninjected backdoors, the downstream classifiers built on it for \u03b8 T\ndifferent downstream tasks simultaneously inherit the backdoor\nbehavior. Given such vulnerability to backdoor attacks, Clean- L(\u03b8):=E t,x0,\u03f5(cid:104)(cid:13) (cid:13)\u03f5\u2212\u03f5 \u03b8(cid:0)\u221a \u03b1\u00af tx 0+\u221a 1\u2212\u03b1\u00af t\u03f5,t(cid:1)(cid:13) (cid:13)2(cid:105) , (9)\nCLIP [98] is proposed as a finetuning framework that weakens\nthelearnedspuriousassociationsintroducedbybackdoorattacks. where t is uniform between 1 and T, \u03f5 \u223c N(0,I) is random\nFairness and Bias. Social bias is an important topic in a fair AI\nnoise,and\u03f5\n\u03b8\nisknownasnoisepredictorparametrizedby\u03b8.\nsystem. A wide range of works have studied different aspects of By incorporating additional control information, typically in\nbiases.[99]showcasesananalysisofbiasregardingraceandgen- the form of textual prompts, the efficacy of the reverse process\nder misclassification in the CLIP model. In the meantime, many indiffusionmodelshasbeensignificantlyenhancedtocontrolthe\nexistingworksfocusonde-biasingthemodel.Inparticular,[100] synthesisresultsratherthanrandomsampling.Thistextual-based\nattemptstoalleviatebiasbycalibratingthebiasedpromptedtexts generationhassolidifieditspositionasthepioneeringfoundation\ntodebiasedcontentwhile[101]proposestomitigatebiasedresults in the field of text-to-image generation. Consequently, let \u0393 be\nin image retrieval tasks by post-processing of the VLMs output. an encoder that maps a conditioning input prompt P into a\nIn addition, [102] introduces a new dataset debiasing pipeline to\nconditioningvectorc:=\u0393(P),theconditionedlearningobjective\naugmentthedatasetwithhealthydata.\nhasbeenexpandedwithcthatrepresentstextualprompt.\nL(\u03b8):=E t,x0,\u03f5,c(cid:104)(cid:13) (cid:13)\u03f5\u2212\u03f5 \u03b8(cid:0)\u221a \u03b1\u00af tx 0+\u221a 1\u2212\u03b1\u00af t\u03f5,t,c(cid:1)(cid:13) (cid:13)2(cid:105) ,\n5 PROMPTING MODEL IN TEXT-IMAGE GENERA- (10)\nFigure2illustratesatypicaltext-imagegenerationframework,\nTION\nhighlightingitskeycomponentsandfunctionalities,including(1)\n5.1 PreliminaryofText-ImageGenerationModels\nfixed or learnable conditional information, such as hard textual\nThissectionprovidesanoverviewofthepreliminariesrequiredto prompts or learnable soft prompts. The conditional information\nunderstandthepromptingmodelintext-imagegeneration,witha can be in textual form or in other modalities; (2) An encoder E\nspecificfocusondiffusionmodels. of the input image; (3) A generative model, such as diffusion\nText-imagegenerationautomaticallysynthesisvividandreal- model, autoregressive model, or GAN; (4) Noise injection or\nistic images from natural language descriptions and has attracted interference; (5) A representation of features in the latent space\nmuch more attention. From the pioneering work DRAW [103], orlow-resolutionimages;(6)AdecoderDforimagedecodingor\ntext-imagegenerationmodelshaveseennumerousbreakthroughs. super-resolution for a high-fidelity generation. The training pro-\nGenerative adversarial network (GAN) [104] then used to design cess involves dataset utilization, loss functions, and optimization\nend-to-end differentiable image generation structure [105] which techniquestotrainthemodelforgeneratingcoherentandvisually\nis followed by many works [106, 107, 108]. Besides, varia- appealing images based on text prompts. During the inference\ntional auto-encoder (VAE) [109] is also adapted to generate im- stage, the trained model is utilized to generate images based on\nages[110,111].However,thesemodelsaretrainedonsmall-scale user-specifiedprompts.Theformulationofpromptsplaysacrucial\ndataandlackgeneralization[112].Autoregressivemethodsdriven role as it governs communication with the model and influences\nbylarge-scaledatasets,suchasDALL-E[112],andParti[113],are thedesiredoutcomesofimagegeneration.Thissectionfocuseson\nproposedanddemonstratesurprisingzero-shotgenerationability. promptengineeringintext-imagegenerationanditsapplications."}
{"instruction": "Explain this concept:", "response": "11\n5.2 UnderstandingPrompting use of a unique modifier token S\u2217 for each concept i, initialized\ni\nwithdifferentraretokensandpositionedaheadofcategorynamex.\nTo gain a deeper understanding of the factors that influence the\nOver the days, textual prompts only cannot meet the specific\ngenerated images, we will introduce prompt design in text-to-\nneeds of image-processing tasks, and controllable text-to-image\nimagefromtheviewofsemantics,promptdiversity,andcontrol-\ngeneration is gaining attention [129, 130, 131]. A wide range of\nlableprompts.\ntask-specific input conditions, such as canning edge encoded by\nSemantic Prompt Design. The art of prompt semantics has image encoder [132], are added with trainable network architec-\nsignificantimpactsonimagegenerationindiffusionmodels[119]. ture to the diffusion model in the work of ControlNet [133]. The\nThe linguistic components such as adjectives, nouns, and proper additionaltask-specificconditionsc isaddedtotheoveralltrain-\nf\nnounsinpromptinfluenceimagegenerationindifferentwaysbut ing objective as L(\u03b8):=E [\u2225\u03f5\u2212\u03f5 (x ,t,c,c ))\u22252(cid:3) .\nt,x0,\u03f5,c,cf \u03b8 t f 2\nconsistently. While descriptors (simple adjectives) subtly affect Notably, in order to improve the semantic recognition ability\nthe output, nouns introduce new content more effectively. Inter- of the encoder from control maps and optimize ControlNet\u2019s\nestingly,usinganartist\u2019snametendstogenerateimagesdeviating performanceevenwhenexplicitpromptsareabsent,ControlNet\u2019s\nsignificantlyfromtheoriginal,andincorporatinglightingphrases training utilizes a method where half of the text prompts are\ncandramaticallymodifyimagecontentandmood.Therefore,the randomlyreplacedwithemptystrings.\nqualityofimagegenerationcanbeenhancedthroughclear,noun- Controlling the synthesis results can also be done after the\nbasedstatements,effectiveseeds,andtheemulationofartiststyles. generation process with prompt editing methods. To bypass the\nDiversifyGenerationwithPrompt.Apartfromdirecthandcraft- commondemandofuser-definedspatiallyfixedmasks[124,125],\ningindividualpromptsinasemanticway,recentworksexperiment Prompt-to-Prompt[134]caneditimagesbyonlyeditingprompts\nwith various prompt modifiers M focusing on enhancing the by replacing a word, specifying a style, changing adjectives, etc.\ndiversity of initial prompts P by P\u02dc = M(P) with P\u02dc be the The manipulations are infiltrated by injecting the cross-attention\ndiversified prompts. DiffuMask [120] explores two strategies in maps controlling which pixels attend to which tokens of the\npromptmodifiersM,i.e.,retrieval-basedpromptandpromptwith prompt text during which diffusion steps. Prompt-based image\nSub-class,withP setto\u201dPhotoofa[sub-class]carinthestreet\u201d. editingmethodsthatmerelymodifythetextpromptprovidemore\nSpecifically, they retrieve real images and captions sets [121, 2], intuitiveeditingexperiences.\nwith captions as the prompt sets for generating synthetic images.\nBesides, they select sub-classes from Wiki based on the main 5.3 ApplicationofPrompting\nclass.ImaginaryNet[122]usesGPT2[36]asMwithagivenclass\nText-to-image diffusion models, aided by prompting techniques,\nname y of the target object to generate a complete description\nhaveexcelledindatagenerationapplications.Thissectioninvesti-\nof an imaginary scene\nP\u02dc\nunder the guidance of prefix phrases\ny gatestheirefficacyingeneratingtrainingdatathatboostthescope\nof \u201dA photo of a\u201d. The prompt serves as generating diversified\nand flexibility of learning procedures. Additionally, we explore\nphoto-realistic imaginary images for the imaginary supervised\nthe versatility of these models in crafting diverse data in target\nobject detection task. Similarly, [123] uses a word-to-sentence\ndomains,spanningdiverseoutputformatslikeimages,videos,3D\nT5 model [22] as M to generate detailed prompts P\u02dc targeted\ny models,andmotion.Also,weunveilitspotentialincomplextask-\nfor a specific label space y, thereby maximizing the potential of\nsolvingandadversarialattacks.\nsynthesizeddataindata-scarcesettingsbyenrichingthediversity\nofprompts.TheseapproachesfurtherobtaindiversifiedimagesI 5.3.1 GeneratingSyntheticTrainingData\nbyI =G(\u03f5|P\u02dc)whereG representsthegenerativemodel. Recentadvancementshavesparkedagrowinginterestinprompt-\nComplexControlofSynthesisResults.Asthesynthesizedimage ing text-to-image models as innovative synthesized training data\ngeneration is usually inconsistent due to noise injection and ran- generators for various tasks downstream tasks such as seg-\ndomnesslyinginthestochasticnatureofdiffusionmodels,recent mentation, object detection, and image recognition. Challenges\nwork has been emerging in the area of complexly controllable such as data scarcity and the need for high-resolution synthetic\ngeneration.Toavoidcontrollabilitylimitationswithuser-provided images can be mitigated through intricate prompt engineering.\nmasks that restrict the modified area [124, 125], prompt-based DiffuMask[120]automaticallygenerateshigh-resolutionsynthetic\ncontrol is gaining attention. OneWord [126] aims to solve the training images with the aforementioned prompt engineering\nproblemofgeneratingpersonalizedimageswithspecificsubjects strategies in Sec. 5.2. Its created pixel-level semantic masks\nthatarehardtodescribewithpuretexts.Thereforetheyproposeda betweenpromptsandgeneratedimagescanbeseamlesslyapplied\npromptmethodthatdesignatesaplaceholderstringS torepresent for segmentation tasks, including semantic segmentation, open-\n\u2217\nthenewconceptsuchas\u201daphotographofS onthebeach\u201dwith vocabulary segmentation, and domain generalization on real im-\n\u2217\nits associated learned embedding v . A similar design is done ages. ImaginaryNet [122] generates synthesis data to tackle the\n\u2217\nbyDreamBooth[127].Insteadofcreatingnewwords,theydesign challenge of insufficient real images and annotations for training\npromptswith(uniqueidentifier,subject)pairsthatbindraretokens object detection. It generates scene descriptions with LLM from\nfromT5-XXLtokenizer[22]asuniqueidentifiersforthespecific class labels and prompts the text-to-image model for creating\nsubjectsandthecoarseclassnameofthesubjects,suchas\u201dA[V] imaginary training data. Under different training settings of pure\ndog\u201d,with[V]astherare-tokenidentifiers.Theyfurtherretainthe or mixed imaginary and real data, object detectors are enhanced\nrepresentationofclassnamesinpromptsbyintroducingextended for the Imaginary Supervised Object Detection task (ISOD),\nclass-prior preservation loss to the training objective. Custom especially under settings where real images and annotations are\nDiffusion [128] extended the customization into a multi-concept unavailable. Synthetic data is also proven feasible for image\nscenario where multiple personalized concepts are composed in recognition tasks, specifically in zero-shot and few-shot settings.\nthe same generated image, such as family members in the same [123] creates the synthetic data for image recognition in a two-\nfamily photo. They design prompts at this aim by including the phase manner. Firstly, novel samples are synthesized using target"}
{"instruction": "Explain this concept:", "response": "12\ncategorynames.Secondly,afine-tunedlanguagemodelisusedto Borrowing the idea from [127], Magic3D is capable of person-\nconvertcategorynamesintorichlycontextual,diversifiedlanguage alized prompt-based editing of 3D models by binding the [V]\npromptsfordiversifyingthetrainingdata. identifier in the prompt with the 3D object. Besides, prompt-\nbased editing can be done through finetuning with LDM in the\n5.3.2 GeneratingDatainTargetDomain coarse-to-fine stage with the modified prompt. Inaccurate and\nInadditiontotheroleoftrainingdatagenerators,diffusionmodels unfaithfulstructuresintext-to-3Dgenerationduetorandomshape\nalsoplayapivotalroleastargetdatagenerators.Importantly,their initialization without prior knowledge lead Dream3D [146] to\ncapabilities extend beyond the generation of images. They can explicit 3D shape priors into the CLIP-guided 3D optimization\nefficientlygeneratevideodata,three-dimensionaldata,andmotion process [143, 147, 148]. Specifically, it connects the T2I model\ndata,furtherbroadeningtheirapplicationrangeandutility. and a shape generator as the text-to-shape stage to produce a\nText-to-VideoGeneration.Make-A-Videofrom[135]isthefirst 3D shape prior with shape components in the prompts. Then it\napproach for directly translating the tremendous recent progress harnesses the 3D shape prior to the initialization of NeRF and\nin text-to-image (T2I) generation to text-to-video (T2V) without optimizes it with the full prompt. To close the gap between the\npaired text-video data. It infers actions and events in the prompt synthesis image and shape, and also inspired by [126, 127],\nand generates video by leveraging joint text-image priors to Dream3Dlinksrenderingswithstylizedtextpromptsuffixesinthe\nbypass the need for paired text-video data. Imagen Video [136] formatof\u201daCLSinthestyleof\u2217\u201dwhereCLSrepresentstheshape\npropelsT2Vgenerationtowardsamoreefficientstage,delivering categoryand\u2217isaplaceholdertokenthatrequiresoptimizationof\nhigher video resolution outputs by combining a frozen T5 text itstextembeddingjointlywiththeweightsofStableDiffusionfor\nencoder[22],abasevideodiffusionmodel,andinterleavedspatial capturingthestyleoftherenderedimages.\nand temporal super-resolution diffusion models, i.e., cascaded\ndiffusion models [137]. However, works on T2V commonly face\nText-to-Motion Generation. Another area where the power of\nchallengesineditingcapabilitiesandeffectivetrainingonspecific\nprompt-basedgenerationisexemplifiedisintherealmoftext-to-\ndomains.FateZero[138]overcomestheselimitationswithazero-\nmotion (T2M). MotionDiffuse [149] is a diffusion model-based\nshottext-basededitingmethodcapableofeditingattributes,style,\ntext-driven motion generation framework with motion sequence\nand shape on real-world videos without per-prompt training or\nastheinputx .Ithasabodypart-independentcontrollingscheme\nuse-specific mask. Specifically, FateZero utilizes a pair of user- 0\nthat generates separate sequences for each body part under m\nprovided source prompt P and the editing prompt P . The\nsrc edit fine-grained prompts P with i \u2208 [1,m] for each body part i\nsource prompt is for obtaining a noisy latent representation x\nt\nandpredictseach\u03f5parti\n= \u03f5 (x ,t,\u0393(P )).Besides,itgenerates\nofthesourcevideoframe,thenx isdenoisedconditionedonthe i \u03b8 t i\nt arbitrary-length continuous motion synthesis using time-varied\neditingpromptP .Tune-A-Video[139]tacklesthechallengeof\nedit textpromptswithmintervals,denotedasarray{P ,[l ,r ]}\ncomputationalexpensivenesswiththeone-shottuningstrategyon i,j i,j i,j\nand predicts the \u03f5time. All noises are interpolated mutually with\nonetext-videopairandonlyonthefirstandformervideoframes. i\nother parts for the continuous motion sequence generation. Simi-\nThisstudyisintheinspirationthatT2Imodelsattendwelltoverbs\nlarly, as in T2I, T2V, and text-to-3D, it is also required for T2M\nin the prompt in generating still images and exhibit surprisingly\nsynthesis with flexible editing capability. Thus, FLAME [150]\ngood motion consistency alignment with prompts when extended\nenables editing with free-form language description with novel\nto T2V. Tune-A-Video is also equipped with editing capability\ntransformer-based diffusion architecture. It takes diffusion time-\nby capturing essential motion information from the input video\nstep tokens, motion length tokens, language tokens, and motion\nandsynthesizingnovelvideoswitheditedpromptspreservingthe\ntokensasinputtokenstothetransformerandcanthereforehandle\nmotionwords.Moreover,textualprompt-basedgenerationhasad-\nmotionsequencesofvariablelength.MDM[151]alsointroduces\nvancedtomulti-modalgeneration,e.g.,generatingsimultaneously\neditability and controllability with a similar idea borrowed from\nalignedaudio-videopairs[140,141].\nimageinpaintingbyaddingsuffixesandprefixestothemotionin\nText-to-3DGeneration.Previousworksfacechallengesofinsuf-\nthe temporal domain. And the textual condition guides MDM to\nficientlarge-scalelabeled3Ddatasetsandinefficientarchitectures\nfillthemissingbodypartwithaspecificmotionwhilekeepingthe\nfordenoising3Ddata.Asaconsequence,prompt-basedgeneration\nrestintactinthespatialdomain.\nhas advanced from T2I to T2V models and also in text-to-3D\nscenarioswherehigh-quality3Dobjectsandscenesaregenerated\nfrom text prompts [142]. DreamFusion [143] firstly randomly Complex Conditional Scene Generation. The use of diffusion\ninitializesthe3DobjectwithNeRF[144]foreachtextpromptand modelshasexpandedbeyondsingletargetdatageneration,finding\nproduces2Dimagerenderingsx=g(\u03b7)withdifferentiableimage applications in various scenarios that involve generating more\ngenerator g(\u03b7). These renderings are generated from various complex scenes tailored to specific use cases with more complex\nangles and paired with view-dependent prompts prefixes such as conditional inputs. In robotics, text guidance is used to perform\n\u201doverhead view\u201d and \u201dfront view\u201d and then diffused and recon- aggressive data augmentation on top of our existing robotic\nstructed by Imagen [136] with q(x |x ) := q(g(\u03b7) |g(\u03b7) ). manipulation datasets to generate robotic scenes via inpainting\nt 0 t 0\nThe sampled noise \u03f5 guides a gradient direction to be backprop- variousunseenobjectsformanipulation,backgrounds,anddistrac-\nagated to the NeRF parameters \u03b7. To tackle the issue in the tors[152].Inautonomousdriving,diffusionmodelsareleveraged\ngrowing popular DreamFusion regarding the optimization effi- to generate controllable pedestrian trajectories that align with the\nciencyofNeRFwhichleadstolow-quality3Dmodelswithalong surrounding environment\u2019s context that enables the simulation of\nprocessingtime,Magic3D[145]proposedatwo-phasecoarse-to- realisticpedestrianbehavior[153].Additionally,diffusionmodels\nfineoptimizationframework,i.e.,firstlyobtainingcoarsediffusion can incorporate conditional information in the form of graphs\nprior from text prompts with Imagen [136] and then rendering that represent individual rooms to generate house floorplans,\nefficientlywithhigh-resolutionlatentdiffusionmodels(LDM)[3]. facilitatingthedesignandplanningofresidentialspaces[154]."}
{"instruction": "Explain this concept:", "response": "13\n5.3.3 Prompt-centeredComplexTask demonstrate that the text encoders pose a major tampering risk.\nBeyondtheformerdirectapplicationsoftext-to-othergeneration, The attack is a teacher-student approach and only involves fine-\nprompt-centeredcomplexapplicationinvariousscenariosreveals tuningatextencoderbygeneratingbackdoortargetsandtriggers\nthe field\u2019s true versatility and potential. In the context of story- onthefly.Zhaietal.[162]designthreetypesofbackdoorattacks,\ntelling,StoryBook[155]retainsavisualnarrativestorybookwith namely pixel-backdoor, object-backdoor, and style-backdoor, and\nconsistent character faces through a series of prompt-centered demonstrate the text-to-image diffusion models\u2019 vulnerability to\nsteps. It first generates prompts of scene descriptions with LLM, backdoor attacks. Huang et al. [163] explore the vulnerability to\nwhicharepromptedtothelatentdiffusionmodelwithdesignated backdoor attacks via personalization for a more efficient attack.\nspecial token placeholder S like [126], to ground consistent Text-to-image personalization guides the diffusion-based text-to-\n\u2217\ncharacterfacesduringgeneration.Similarly,[156]proposedmul- image model to generate user-provided novel concepts through\ntimodalprocedureplanning(MPP)task,wheretheinitialstepwise naturallanguage.Huangetal.[163]devisedbackdoorattackson\ntextual plan is generated with LLM and then serves as prompts two families of personalization methods, Textual Inversion [126]\nto diffusion model for synthesizing text-grounded image plan. andDreamBooth[127].\nWhat\u2019s different is that the image plans are verbalized through FairnessandBias.GenerativeAImodelsaretypicallytrainedon\nimage captioning backward to the LLM for revising the initial web-scale datasets scraped from the internet and are inevitable\nplanshowingthepotentialformultimodalprompting. to biased human behavior as shown in [164, 165, 166, 167].\nFor example, Stable Diffusion only generates images with white\nmale-appearingpersonsasfirefighters[164].Somestudiesstartto\n5.4 ResponsibleAIConsiderationsofPrompting\npay more attention to the fairness issues related to text-to-image\nArtificial Intelligence is revolutionizing our world through its generations and can be grouped into three paradigms: 1) training\nformidable learning ability, transformative force, and profound datapre-processingtoremovebiasbeforelearning[102,168],2)\ninfluence across diverse areas of society. It also spurred intense enforcingfairnessduringtrainingbyintroducingconstraintsonthe\ndebate about ethical issues, principles, and integrity in AI devel- learningobjective[168],3)post-processingapproachestomodify\nopment and applications. There is a global convergence around themodeloutcomeatthedeploymentstage[164,100,169].\nfive ethical principles [157]: transparency, justice and fairness, Privacy. There might be privacy-sensitive information, e.g., face\nnon-maleficence, responsibility, and privacy. In this subsection, identity, in the huge amount of training data for training text-\nwediscussethicalissueswhenpromptingtext-to-imagegenerative to-image models. Such information may arise privacy risks in\nmodels. real-world applications such as information leaks. Membership\nAdversarialRobustnessofPrompt.Theadversarialattackshave inferenceattacksareanapproachtoinvestigatingprivacyleakage\nbeen introduced to text-to-image diffusion models for mainly 2 by inferring whether a specific data sample was used in the\naims. Some work takes diffusion models as a tool to facilitate or trainingphase(calledmemberornon-memberrespectively)[170].\ndefendagainstadversarialattacks[158,159].Someworkdirectly Some work [170, 171, 172] studies the privacy risks of text-to-\nattacks diffusion models [160] and aims to erase image content image generation models from the perspective of membership\ngivencharacterperturbations.Asthepioneertointroducediffusion attacks. From the perspective of prompting, Shen et al. [173]\nmodels in the adversarial attack field, DiffAttack [158] unveils proposepromptstealingattack,whichstealspromptsfromimages\nthepotentialofdiffusionmodelsforcraftingadversarialexamples generated by text-to-image generation models. The creation of\nwithsatisfactoryimperceptibilityandtransferabilitybymanipulat- high-quality prompts can be challenging, time-consuming, and\ning the latent space rather than pixel space. This approach main- costly. Hence successful prompt stealing attacks direct violate\ntainsvisualqualitywithembeddingperturbationsundetectableto intellectual property and even jeopardize the business model of\nhumansandtransferableacrossdiversemodelarchitectures.Diffu- prompttradingmarkets.\nsionmodelscanbeutilizedforadversarialpurification-adefense\nstrategy that removes adversarial perturbations. DiffPure [159]\n6 PROMPTING VLM VS. UNI-MODAL MODELS\nimplements this approach, adding a minimal amount of noise to\nan adversarial example before reversing the generative process 6.1 PromptinginNaturalLanguageProcessing\nto restore the original image, thus exhibiting robust defense This section summarizes existing studies on prompt engineer-\ncapabilitiesagainstpowerfuladaptiveattacks.Zhuangetal.[160] ing on textual language models. Prompt engineering has been\nstudythequery-freeattackgenerationonStableDiffusionswhere widely adopted in various natural language processing applica-\nanadversarialtextpromptisobtainedintheabsenceofend-to-end tions including question answering [174, 175], text classifica-\nmodel queries. They show the vulnerability of Stable Diffusions tion [61, 176], text generation [36, 23, 177], and information\nrooted in the text encoders. A five-character text perturbation is extraction[178,179],etc.RecentLLMssuchasInstructGPT[180]\nabletoshifttheoutputcontent. and PALM2 [181] have shown incredible generalized inference\nBackdoorAttackofPromptLearning.Backdoorattacksontext- ability through prompting. Early works [182] designed natural\nto-imagegenerativemodelsaimtocontrolthecontentofgenerated language templates to let pre-trained language models fill in to\nimages during inference by embedding inputs with predefined explain their predictions. Wei et al. [4] demonstrate that the\nbackdoortriggers.Theattackersecretlyinjectsbackdoors,suchas performance of LLMs can be significantly improved by adding\nspecific text characters, into the model during training to trigger intermediate reasoning steps into the prompt. In particular, the\nthe model to either generate images with pre-defined attributes promptofeachtaskcontainsafewmanualdemonstrationsconsist-\nor images following a hidden or even malicious description. The ingofaquestionandareasoningchainleadingtotheanswer.The\nbackdoor attack may lead to inappropriate outputs such as offen- LLMlearnstofollowthepromptandthinksstep-by-steptosolve\nsive content. On the other hand, it can also be used in copyright thegiventask.Liuetal.[183]findthatthequalityoftheprompt,\nprotection by watermarking the models. Struppek et al. [161] i.e.,theselectionofexamplesinpromptsandgivenexplanations,"}
{"instruction": "Explain this concept:", "response": "14\nlargely impacts LLM\u2019s performance. Fu et al. [184] demonstrate ing image classification [10, 197, 198, 199], image segmenta-\nthat prompting LLMs with complex example questions, which tion [85, 196], depth estimation [196], keypoint detection [196],\nrequires more intermediate reasoning steps, could achieve better denoising [196], detaining [196], and image enhancement [196]\nperformance and benefit the model\u2019s robustness regarding format etc.\nperturbationanddistributionshift. Several studies have identified two main mechanisms for\nManually crafting prompts for each task strongly depends on incorporating prompts into vision models. The first mechanism\nhumanexperience,andmanualtestingwouldberequiredtoeval- treatspromptingasanadaptationmethodthatfacilitatesthefine-\nuate and improve the template, which would be time-consuming. tuning of pre-trained vision models [10, 200, 197]. The second\nZhangetal.[5]workoneliminatingmanualeffortsbyleveraging mechanismutilizespromptsasamodulethatplaysaroleinboth\nLLMstoconstructreasoningchainswithdemonstrations.Besides, modelpre-trainingandinference[85,196,199].\nalineofworks[185,186,187]automatesthepromptengineering Pre-trained vision models have significantly improved per-\nbyutilizingadenseretrievertoaugmentthelanguagemodelswith formance, but their size has also increased drastically, making\nexternal resources, which has also been referred to as retrieval- trainingandfine-tuninginfeasibleformostusers.Toaddressthis\naugmented language models. For a given question, the dense issue,adaptingpre-trainedmodelstospecifictasksinaparameter-\nretriever retrieves relevant text from a knowledge source and efficient way is critical. Many studies have treated prompting as\nappends it to the language model input. Such language models anadaptationmethod.\nhave recently demonstrated strong performance on knowledge- Bahngetal.[10]useasinglevisualprompttoadaptafrozen\nintensive tasks. [6] propose prompt tuning that appends the input large-scale vision model to a new task. Adaptation approaches\nembedding layer with extra trainable tokens and learns these suchasfine-tuningandlinearprobesrequiresomelevelofaccess\ntokensthroughbackpropagationondownstreamtasks.Thisopens tothepre-trainedmodelduringbothtrainingandtesting.However,\nadirectionoflearningsoftpromptstoenhanceLLMs. visual prompting only requires model access during training,\nMany studies demonstrated that LLMs\u2019 performance consid- making it feasible for some applications [200]. Additionally,\nerably drops as the task complexity increases. A natural way for Tu et al. [198] propose Visual Query Tuning (VQT) to adapt\nhumanstosolvecomplextasksistodecomposethemintoaseries pre-trained Transformers to downstream tasks while keeping the\nofsimplesubtasksandsolvethecomplextaskbycompletingeach backbonefrozen,allowingformoreaccuratepredictionsutilizing\nsimple subtask. A line of works investigated enhancing LLMs\u2019 theintermediatefeaturesofapre-trainedmodel.\nperformance on complex tasks by prompting LLMs multi-times, As classical fine-tuning methods become more limiting when\nwheretheLLMsareexpectedtosolveasubtaskbyeachprompt. models are hosted as inference APIs, visual prompt learning is\nPress et al. [188] examine the capacity of language models to emerging as a potential solution for adapting frozen and cloud-\nexecute compositional reasoning tasks and found that LLMs are hosted models. Loedeman et al. [197] introduce the Prompt\ngood at memorizing facts but do not compose them to answer Generation Network (PGN), which generates input-dependent\nquestions. To narrow the compositionality gap, the authors let visualpromptstofacilitatedomainadaptation.PGNgeneratesnew\nLLMs ask themselves follow-up questions, answer the questions, prompts for every image by combining items from a commonly\nand decide whether they have sufficient information to give the learned library of tokens. It consists of a lightweight neural net-\nfinal solution. Kazemi et al. [189] propose a backward chaining work that learns the probability distribution for selecting prompt\nalgorithm to decompose a complex task by starting from the vectorsfromatokenlibrary.\nobjective and recursively breaking down the complex task into Inadditiontousingpromptsasanadaptationfordownstream\nsub-tasksbasedonrules.Khotetal.[190]decomposeacomplex tasks, some researchers have integrated prompting modules into\ntaskintosub-tasksandusesub-task-specificLLMstosolvethem, the entire model to improve pre-training performance and enable\nleading to improved performance on a line of textual multi-step moreflexibleinference.In[85],Kirillovetal. introducetheSeg-\nreasoningtasks. ment Anything Model (SAM), which aims to build a foundation\nResearchers have also noticed ethical and integrity issues modelforsegmentation.InspiredbypromptingtechniquesinNLP,\nrelated to prompt engineering in NLP. Yang et al. [191] propose they proposed the promptable segmentation task to generate a\na prompt-based adversarial attack to compromise NLP models validsegmentationmaskbasedonanysegmentationprompt.The\nandrobustnessenhancementtechniques.Thisworkindicatesthat prompt can include spatial or text information that identifies an\nthepromptingparadigmhasthepotentialinprobingfundamental objectintheimage,andtheoutputofthecorrespondingmodelis\nvulnerabilities of large language models and fine-tuning them a reasonable mask for at least one target object. This promptable\nfor downstream tasks. Dong et al. [192] adopt a prompt-based segmentation task is used in both pre-training and downstream\nlearning approach to automatically generate effective adversarial segmentationtasks.\nexamples to probe Dialogue State Tracker models. The prompt Painter, presented in [196], is a generalist model that can\nmayinheritthebiasinthepre-trainedmodelsand[193]reviewthe perform various vision tasks based on given task prompts. It can\nliteratureonfairnessmetricsforpre-trainedlanguagemodelsand perform tasks such as semantic segmentation, instance segmen-\nexperimentallyevaluatecompatibility.Moreover,onecanreferto tation, depth estimation, keypoint detection, denoising, detailing,\nseveral existing surveys [194, 195, 9] for a more comprehensive andimageenhancement,aswellasout-of-domaintaskslikeopen-\nreview. category object segmentation. To address the issue of general-\npurposepromptdefinition,Painterformulatesthedense-prediction\nvisionproblemasimageinpainting.Thisway,input/outputpaired\n6.2 PromptingonPureVisionModels\nimages from the same task can be used as input to indicate the\nAlthough prompt is first widely adopted in natural language taskthemodelshouldperform.\nmodels, many works also utilize prompts in pure vision mod- Following the work on Painter, Wang et al.propose Seg-\nels [85, 10, 196, 11, 197, 198, 199] and applications includ- GPT [11], which focuses on the segmentation task and enables"}
{"instruction": "Explain this concept:", "response": "15\nsegmentation of everything with a generalist Painter. Zhang et Inanintuitivesense,aunifiedpromptcanprovideuswithreferen-\nal.utilize auxiliary prompts to approach the Generalized Novel tialinformationthatspansacrossmodalities,asdiscussedin[70].\nCategoryDiscovery(GNCD)settingbyproposingaprompt-based Thishasthepotentialtofacilitatethedevelopmentofmultimodal\nContrastiveAffinityLearning(PromptCAL)method[199].Exist- modelsthatarecapableofvisualgroundingandenablereferential\ningsemi-supervisedlearningmethodsfailtolearnunlabeleddata dialoguesencompassingvisualandtextualco-reference.\nfromnovelsemanticclasses,butPromptCALisdiscriminativeto PromptingModelinText-ImageGeneration.Oneofthesignif-\nnovelsemanticinformation. icantchallengesinthefieldofpromptingtext-to-othergeneration\nThe combination of prompt engineering with visual models models,particularlyinthecaseofText-to-Video(T2V)andText-\nhas also triggered a line of work focusing on integrity and ethics to-3D (T2-3D) models, is their dependency on Text-to-Image\nissues.Chenetal.[92]leveragedvisualpromptingtoimprovethe (T2I) models. These models often share the same concern due\nadversarialrobustnessofafixed,pre-trainedmodelattestingtime. to the nature that they are extensions of T2I diffusion models.\nLi et al. [201] explored the benefits of visual prompting in con- For example, the inconsistency of the input control maps in T2I\nstructing compelling neural network classifiers with differential models can lead to errors in the consequently generated videos\nprivacy. However, such studies are still relatively rare and more and 3D objects, thereby affecting the overall performance and\nattentionisrequired. reliabilityoftheseextensionscenarios.\nLooking ahead, there are several promising directions for\nfuture research. One such direction is the incorporation of visual\n7 CHALLENGES AND OPPORTUNITIES prompting into T2I, T2V, and T2-3D diffusion models. In the\ncontext of text-to-image generation, visual prompting and visual\nPrompting Model in Multimodal-to-Text Generation.Inaddi-\nannotations can offer more visual cues, leading to the creation\ntion to visual and textual modalities, the incorporation of other\nof more personalized images. This approach allows for more\nmodalities such as audio and thermal is possible. It is crucial to\nattention to be paid to specific areas of the image, enhancing the\naddresstheinherentheterogeneityamongthesemodalities,which\ndetail and accuracy of the generated output. Visual prompts can\nincludesvariationsindataformats,scales,andstructures.\nalsobebeneficialforvideogeneration,eitheronaframe-by-frame\nTwo notable projects in this domain are Kosmos [15, 46], basis or for T2-3D generation aimed at improving 2D renderings\ndeveloped by Microsoft, and IMAGEBIND [202], developed by orshapes.Theconceptofvisualpromptscanbefurtherexpanded\nMeta AI. These projects aim to create unified models capable to include video prompts, object prompts, and motion prompts,\nof handling diverse modalities, promoting the utilization of such depending on the specific requirements of different target data\nunifiedmodelsasasignificantdirectioninthefield. generation scenarios. Furthermore, text-image matching models\nHowever, it is important to note that most of the research on hold the potential for better alignment as multi-modal prompting\nprompts for multimodal-to-text pre-trained models has primarily inthegeneration.Thisapproachcouldleadtomoreaccurateand\nfocused on hard prompts. Conversely, soft prompts based on contextuallyrelevantimagegeneration,openingupnewpossibili-\nimage-text matching models, such as CLIP [2], have received tiesfortheapplicationofpre-trainedvision-languagemodels.\nextensiveinvestigation.ModelslikeCoOp[63]andCoCoOp[66] Generalizing Prompting Methods from Unimodal to Multi-\nleverage soft prompts on it to enhance model performance. Nev- modal.Sec.6discussestheapplicationsofpromptengineeringin\nertheless,theexplorationofprompttuningforpopulargenerative both pure vision and pure language models, which can motivate\nmultimodal-to-textpre-trainedmodelsremainslargelyunexplored. furtherresearchinmulti-modalityresearch.Whencombinedwith\nAdditionally, multimodal-to-text pre-trained models employ instruction-tuning methods, pure language models have enabled\na range of challenging prompt techniques, including in-context phenomenal applications such as ChatGPT [50]. The potential of\nlearning [1] and instruction tuning [203]. Despite their effective- thesemethodssuchasReinforcementLearningfromHumanFeed-\nness,theunderlyingmechanismsbywhichthesemodelslearnand back (RLHF) [180] and Harmlessness from AI Feedback [204]\nthe specific contributions of different aspects of the demonstra- canbefurtherexploredinmultimodalmodelsasshowninseveral\ntionsremainlargelyunexplored.Adeeperunderstandingofthese recentstudies[203,205].ConstitutionalAIisamethodproposed\nfactors is crucial for refining and optimizing the performance of in[204]totrainaharmlessAIassistantthroughself-improvement,\nmultimodal-to-textpre-trainedmodels. without any human labels identifying harmful outputs. Although\nPrompting Model in Image-Text Matching. Although pre- some efforts have been put into language models [204], how to\ntrained encoders prompted by a matching loss have been widely implement constitutional AI in the multimodal domain is still an\nusedforadaptationindownstreamtasks,theexplorationofvisual openquestion.\npromptingonpre-trainedencodersremainsrelativelyunexplored. Another potential direction is the adoption of in-context\nSimilar to the seamless adaptation of textual encoders through promptsinmultimodalmodels.Largeunimodallanguagemodels\nlearningtextualprompts,theinvestigationofvisualpromptsisan canaddressaspecificnewtaskgivenseveraldemonstrationsofthe\nintriguing area that can unlock emergent abilities, especially in taskintheirtextpromptwithoutanygradientupdate.Flamingo[1]\ndifficultscenariossuchasdenseobjects,objecthallucination,and hasalsodemonstratedthefew-shotin-contextlearningability,but\nthe adaptation to modern VLMs. In the future, it is imperative how to further improve the in-context learning capacity is still\ntoaddressquestionsregardingthespecifictypeofvisualprompts under-explored.\nthatareessentialandthesemanticinformationthattheseprompts Responsible AI Considerations of Prompting. There are al-\nintroduce. By delving into these inquiries, we can gain a deeper ready a few studies concerning ethical issues on multimodal-\nunderstanding of the role and impact of visual prompts, thereby to-text generation in Sec. 3.6 and text-to-image generations as\nfurtheradvancingthefield. discussed in Sec. 5.4. Integrity and ethical issues of prompt\nMeanwhile, the investigation of how unified prompting can engineeringonvision-languagemodelsneedmuchmoreattention.\nenhance the performance of both branches remains understudied. One possible direction is to prevent bias and backdoor attacks"}
{"instruction": "Explain this concept:", "response": "16\ninherited from the pre-trained model during downstream prompt ofexistingpromptengineeringapproachesmakeitchallengingto\nadaptations [96, 206, 207, 208]. As large models are normally provide an exhaustive overview. Additionally, the survey focused\npre-trained on web-scale datasets which may preserve biased primarilyonpre-trainedvision-languagemodelsfromaprompting\nknowledge or sensitive privacy information, the post-deployment engineering perspective and may not have covered all recent\nprocedure conducted by prompt engineering should be able to advancementsinotherrelatedareas.\ncontrolthepotentialrisks. To address these limitations, we will maintain and release a\nAdversarialRobustnesshasbeenintensivelystudiedinvarious platformtokeeptrackingtheadvanceinthisarea.Furtherresearch\nmodel architectures, such as Convolutional Networks [209, 210], should explore the integration of prompt engineering techniques\nVision Transformers [211, 212], and Capsule Networks [213]. It withotheremergingtechnologies,suchasreinforcementlearning\nhas not been fully understood how the prompting on VLMs with or meta-learning, to enhance the performance and generalization\nbothavisionarchitectureandalanguagecomponentperformsun- capabilities of vision-language models. Additionally, investiga-\nder adversarial attacks. Especially, the impact of recent advances tionsintotheinterpretabilityandrobustnessofprompt-engineered\ninVLMonadversarialrobustnessremainstobestudied.E.g.,do models are crucial for ensuring their practical deployment and\nlargepromptsbringrobustnesstoVLMs[57]? ethicaluse.\nBesides,transparencyandcontrollablegenerationthroughfair Overall,thissurveycontributestotheexistingbodyofknowl-\npromptingarealsoessentialingenerativetasks.Generativemodels edge by providing a comprehensive overview of prompt engi-\nare shown to be vulnerable to privacy leakage [170, 171, 172] neering in pre-trained vision-language models. By elucidating\nand may generate biased content [164, 165, 166, 167]. Hence, the current state, key trends, and implications of prompt engi-\nconstructingtransparentandcontrollablepromptsthatarecapable neering techniques, this survey serves as a valuable resource for\nto conserve privacy and prevent unethical generation is critical researchers and practitioners aiming to harness the potential of\nfor real-world applications. Last but not least, managing the vision-language models for various applications. It fills a gap in\naccompanying risks of prompt engineering and large models research by offering insights into the adaptation of pre-trained\nrequires the close collaboration of society, research institutions, modelsinthecontextofvisionandlanguage,pavingthewayfor\nandgovernment[214,215]. furtheradvancementsinthisexcitingfield.\nRelationshipbetweenPromptsonDifferentVLMs.Therecent\nwork [216] studies the relationship between concepts learned by\nmultimodal-to-text and image-to-text and text-to-image models.\nACKNOWLEDGEMENTS\nThey show the studied two types of models cannot fully under- We would like to thank Ananth Balashankar (Google Research)\nstand each other, while they also share some concepts. Similarly, and Ashkan Khakzar (University of Oxford) for constructive\nthe relationship between prompts on different types of models feedbackonanearlierversionofthismanuscript.\nshould be explored in future work, especially the feasibility of\nbuilding universal prompts across different models trained on\nthe same data. In addition to the inter-model relationship, the REFERENCES\ninteraction between prompts and model architecture should be [1] J.-B. Alayrac et al. Flamingo: a visual language model for\ninvestigated since most prompts are proposed on Transformer- few-shotlearning. AdvancesinNeuralInformationProcessing\nbasedmodels.Concretely,howthemodel\u2019sself-attentionchanges Systems,35:23716\u201323736,2022.\n[2] A. Radford et al. Learning transferable visual models from\nduringprompting[212].\nnatural language supervision. In International conference on\nmachinelearning,pp.8748\u20138763.PMLR,2021.\n8 CONCLUSION [3] R.Rombachetal. High-resolutionimagesynthesiswithlatent\ndiffusionmodels. InProceedingsoftheIEEE/CVFConference\nThis survey paper on prompt engineering of pre-trained vision- on Computer Vision and Pattern Recognition, pp. 10684\u2013\nlanguage models has provided valuable insights into the current 10695,2022.\nstateofresearchinthisfield.Themainfindingsandtrendsidenti- [4] J. Wei et al. Chain-of-thought prompting elicits reasoning\nin large language models. Advances in Neural Information\nfied through the analysis shed light on the effective utilization of\nProcessingSystems,35:24824\u201324837,2022.\nprompts in adapting large pre-trained models for vision-language\n[5] Z.Zhangetal. Automaticchainofthoughtpromptinginlarge\ntasks.\nlanguagemodels. InTheEleventhInternationalConferenceon\nOne key finding is the versatility and applicability of prompt LearningRepresentations(ICLR2023),2023.\nengineering across different types of vision-language mod- [6] B. Lester et al. The power of scale for parameter-efficient\nels, including multimodal-to-text generation models, image-text- prompttuning. arXivpreprintarXiv:2104.08691,2021.\nmatching models, and text-to-image generation models. The sur- [7] Q.Dongetal. Asurveyforin-contextlearning. arXivpreprint\narXiv:2301.00234,2022.\nveyexploredeachmodeltypefromtheirrespectivecharacteristics,\n[8] P.Liuetal.Pre-train,prompt,andpredict:Asystematicsurvey\nhighlightingvariouspromptingmethodsonthem.\nof prompting methods in natural language processing. ACM\nThe implications of these findings are significant for both ComputingSurveys,55(9):1\u201335,2023.\nacademia and industry. By leveraging prompt engineering tech- [9] S. Qiao et al. Reasoning with language model prompting: A\nniques, researchers can achieve remarkable performance gains in survey. arXivpreprintarXiv:2212.09597,2022.\nvision-language models without the need for extensive labeled [10] H. Bahng et al. Exploring visual prompts for adapting large-\nscalemodels. arXivpreprintarXiv:2203.17274,1(3):4,2022.\ndata.Thishasthepotentialtoreducetheburdenofdataannotation\n[11] X. Wang et al. SegGPT: Segmenting everything in context.\nandacceleratethedeploymentofvision-languagemodelsinreal-\narXivpreprintarXiv:2304.03284,2023.\nworldapplications.\n[12] C.Wuetal. Visualchatgpt:Talking,drawingandeditingwith\nHowever,itisimportanttoacknowledgethelimitationsofthis visual foundation models. arXiv preprint arXiv:2303.04671,\nsurvey.Therapidlyevolvingnatureofthefieldandthewiderange 2023."}
{"instruction": "Explain this concept:", "response": "17\n[13] X. Liu et al. Gpt understands, too. arXiv preprint learners. OpenAIblog,1(8):9,2019.\narXiv:2103.10385,2021. [37] A.EfratandO.Levy. Theturkingtest:Canlanguagemodels\n[14] E. J. Hu et al. Lora: Low-rank adaptation of large language understand instructions? arXiv preprint arXiv:2010.11982,\nmodels. arXivpreprintarXiv:2106.09685,2021. 2020.\n[15] S.Huangetal. Languageisnotallyouneed:Aligningpercep- [38] O. Rubin et al. Learning to retrieve prompts for in-context\ntion withlanguage models. arXiv preprintarXiv:2302.14045, learning. In Proceedings of the 2022 Conference of the\n2023. NorthAmericanChapteroftheAssociationforComputational\n[16] Z. Yang et al. An empirical study of gpt-3 for few-shot Linguistics: Human Language Technologies, pp. 2655\u20132671,\nknowledge-basedvqa. InProceedingsoftheAAAIConference Seattle, United States, July 2022. Association for Computa-\nonArtificialIntelligence,2022. tionalLinguistics.\n[17] M. Tsimpoukelli et al. Multimodal few-shot learning with [39] X. Li et al. Unified demonstration retriever for in-context\nfrozen language models. Advances in Neural Information learning. In Proceedings of the 61st Annual Meeting of the\nProcessingSystems,34:200\u2013212,2021. AssociationforComputationalLinguistics(Volume1:LongPa-\n[18] OpenAI. Gpt-4 technical report. arXiv preprint pers),pp.4644\u20134668,Toronto,Canada,July2023.Association\narXiv:2303.08774,2023. forComputationalLinguistics.\n[19] S. Long et al. Vision-and-language pretrained models: A [40] J.Yeetal.Compositionalexemplarsforin-contextlearning.In\nsurvey. In L. D. Raedt, editor, Proceedings of the Thirty- InternationalConferenceonMachineLearning.PMLR,2023.\nFirst International Joint Conference on Artificial Intelligence, [41] G. Qin and J. Eisner. Learning how to ask: Querying LMs\nIJCAI-22, pp. 5530\u20135537. International Joint Conferences on with mixtures of soft prompts. In Proceedings of the 2021\nArtificialIntelligenceOrganization,72022. SurveyTrack. Conference of the North American Chapter of the Associa-\n[20] J. Devlin et al. Bert: Pre-training of deep bidirectional tion for Computational Linguistics: Human Language Tech-\ntransformers for language understanding. arXiv preprint nologies, pp. 5203\u20135212, Online, June 2021. Association for\narXiv:1810.04805,2018. ComputationalLinguistics.\n[21] L.PritchettandJ.Sandefur. Learningfromexperimentswhen [42] X. L. Li and P. Liang. Prefix-tuning: Optimizing continuous\ncontextmatters. AmericanEconomicReview,105(5):471\u2013475, prompts for generation. arXiv preprint arXiv:2101.00190,\n2015. 2021.\n[22] C.Raffeletal. Exploringthelimitsoftransferlearningwitha [43] H. Yang et al. Prompt tuning for generative multimodal\nunifiedtext-to-texttransformer. InJMLR,2020. pretrainedmodels. arXivpreprintarXiv:2208.02532,2022.\n[23] T. Brown et al. Language models are few-shot learners. [44] D.NoeverandS.E.M.Noever.Themultimodalandmodularai\nAdvances in neural information processing systems, 33:1877\u2013 chef:Complexrecipegenerationfromimagery. arXivpreprint\n1901,2020. arXiv:2304.02016,2023.\n[24] A. Singh et al. Flava: A foundational language and vision [45] J.Lietal. Blip-2:Bootstrappinglanguage-imagepre-training\nalignmentmodel. InProceedingsoftheIEEE/CVFConference withfrozenimageencodersandlargelanguagemodels. arXiv\non Computer Vision and Pattern Recognition, pp. 15638\u2013 preprintarXiv:2301.12597,2023.\n15650,2022. [46] Z.Pengetal.Kosmos-2:Groundingmultimodallargelanguage\n[25] J. Lu et al. Vilbert: Pretraining task-agnostic visiolinguistic modelstotheworld. arXivpreprintarXiv:2306.14824,2023.\nrepresentations for vision-and-language tasks. Advances in [47] J.B.Nici. APArtHistory:5PracticeTests+Comprehensive\nneuralinformationprocessingsystems,32,2019. Review+OnlinePractice. Barron\u2019sEducationalSeries,2020.\n[26] J. Cho et al. Unifying vision-and-language tasks via text [48] D. A. Hudson and C. D. Manning. Gqa: A new dataset\ngeneration. InInternationalConferenceonMachineLearning, for real-world visual reasoning and compositional question\npp.1931\u20131942.PMLR,2021. answering. In Proceedings of the IEEE/CVF conference on\n[27] S. Ren et al. Faster r-cnn: Towards real-time object detection computervisionandpatternrecognition,pp.6700\u20136709,2019.\nwithregionproposalnetworks.Advancesinneuralinformation [49] Y.Zangetal. UnifiedVisionandLanguagePromptLearning.\nprocessingsystems,28,2015. arXiv:2210.07225,2022.\n[28] P.Wangetal.Ofa:Unifyingarchitectures,tasks,andmodalities [50] Chatgpt. https://openai.com/blog/chatgpt. Accessed:2023-07-\nthroughasimplesequence-to-sequencelearningframework. In 22.\nInternational Conference on Machine Learning, pp. 23318\u2013 [51] K.Zhangetal.Biomedgpt:Aunifiedandgeneralistbiomedical\n23340.PMLR,2022. generative pre-trained transformer for vision, language, and\n[29] Z. Wang et al. SimVLM: Simple visual language model multimodaltasks. arXivpreprintarXiv:2305.17100,2023.\npretrainingwithweaksupervision.InInternationalConference [52] L. Weidinger et al. Ethical and social risks of harm from\nonLearningRepresentations,2022. languagemodels. arXivpreprintarXiv:2112.04359,2021.\n[30] X. Chen et al. PaLI: A jointly-scaled multilingual language- [53] S. Guo et al. Threats to pre-trained language models: Survey\nimage model. In The Eleventh International Conference on andtaxonomy. arXivpreprintarXiv:2202.06862,2022.\nLearningRepresentations,2023. [54] J.Qiuetal. Benchmarkingrobustnessunderdistributionshift\n[31] C.Eichenbergetal. MAGMA\u2013multimodalaugmentationof ofmultimodalimage-textmodels. InNeurIPS2022Workshop\ngenerative models through adapter-based finetuning. In Find- on Distribution Shifts: Connecting Methods and Applications,\ningsoftheAssociationforComputationalLinguistics:EMNLP 2022.\n2022, pp. 2416\u20132428, Abu Dhabi, United Arab Emirates, [55] Y. Zhao et al. On evaluating adversarial robustness of large\nDecember2022.AssociationforComputationalLinguistics. vision-language models. arXiv preprint arXiv:2305.16934,\n[32] J.Lietal. Blip-2:Bootstrappinglanguage-imagepre-training 2023.\nwithfrozenimageencodersandlargelanguagemodels. arXiv [56] S. Chen et al. Benchmarking robustness of adaptation meth-\npreprintarXiv:2301.12597,2023. ods on pre-trained vision-language models. arXiv preprint\n[33] A. Dosovitskiy et al. An image is worth 16x16 words: arXiv:2306.02080,2023.\nTransformers for image recognition at scale. arXiv preprint [57] J.Guetal.Towardsrobustpromptsonvision-languagemodels.\narXiv:2010.11929,2020. arXivpreprintarXiv:2304.08479,2023.\n[34] S. Zhang et al. Opt: Open pre-trained transformer language [58] J.Lietal. Alignbeforefuse:Visionandlanguagerepresenta-\nmodels. arXivpreprintarXiv:2205.01068,2022. tionlearningwithmomentumdistillation. Advancesinneural\n[35] H. W. Chung et al. Scaling instruction-finetuned language informationprocessingsystems,34:9694\u20139705,2021.\nmodels. arXivpreprintarXiv:2210.11416,2022. [59] G.Zhangetal.Multi-eventvideo-textretrieval.InProceedings\n[36] A.Radfordetal. Languagemodelsareunsupervisedmultitask oftheIEEE/CVFInternationalConferenceonComputerVision"}
{"instruction": "Explain this concept:", "response": "18\n(ICCV)(toappear),2023. [81] S. Xiao and W. Fu. Optimizing Continuous Prompts for\n[60] T. Huang et al. Unsupervised prompt learning for vision- VisualRelationshipDetectionbyAffix-Tuning. IEEEAccess,\nlanguagemodels. arXivpreprintarXiv:2204.03649,2022. 10:70104\u201370112,2022.\n[61] T.Gaoetal.MakingPre-trainedLanguageModelsBetterFew- [82] T. He et al. Towards open-vocabulary scene graph generation\nshot Learners. In Proceedings of the 59th Annual Meeting of with prompt-based finetuning. In Computer Vision\u2013ECCV\ntheAssociationforComputationalLinguisticsandthe11thIn- 2022:17thEuropeanConference,TelAviv,Israel,October23\u2013\nternationalJointConferenceonNaturalLanguageProcessing 27,2022,Proceedings,PartXXVIII,pp.56\u201373.Springer,2022.\n(Volume1:LongPapers),pp.3816\u20133830,Online,August2021. [83] K.Gaoetal. CompositionalPromptTuningwithMotionCues\nAssociationforComputationalLinguistics. forOpen-vocabularyVideoRelationDetection. arXivpreprint\n[62] M. Shu et al. Test-time prompt tuning for zero-shot gen- arXiv:2302.00268,2023.\neralization in vision-language models. Advances in Neural [84] Y.Raoetal.Denseclip:Language-guideddensepredictionwith\nInformationProcessingSystems,35:14274\u201314289,2022. context-aware prompting. In Proceedings of the IEEE/CVF\n[63] K.Zhouetal. Learningtopromptforvision-languagemodels. Conference on Computer Vision and Pattern Recognition, pp.\nInternational Journal of Computer Vision, 130(9):2337\u20132348, 18082\u201318091,2022.\n2022. [85] A. Kirillov et al. Segment anything. arXiv preprint\n[64] C. Ju et al. Prompting visual-language models for efficient arXiv:2304.02643,2023.\nvideo understanding. In Computer Vision\u2013ECCV 2022: 17th [86] C. Ge et al. Domain Adaptation via Prompt Learning.\nEuropean Conference, Tel Aviv, Israel, October 23\u201327, 2022, arXiv:2202.06687,2022.\nProceedings,PartXXXV,pp.105\u2013124.Springer,2022. [87] Y. Gao et al. Visual Prompt Tuning for Test-time Domain\n[65] S. Shen et al. Multitask Vision-Language Prompt Tuning. Adaptation. arXiv:2210.04831,2022.\narXiv:2211.11720,2022. [88] Z. Wang et al. Learning to prompt for continual learning. In\n[66] K.Zhouetal.Conditionalpromptlearningforvision-language ProceedingsoftheIEEE/CVFConferenceonComputerVision\nmodels. In Proceedings of the IEEE/CVF Conference on andPatternRecognition,pp.139\u2013149,2022.\nComputer Vision and Pattern Recognition, pp. 16816\u201316825, [89] Z. Wang et al. Dualprompt: Complementary prompting for\n2022. rehearsal-free continual learning. In Computer Vision\u2013ECCV\n[67] M.Jiaetal. Visualprompttuning. InComputerVision\u2013ECCV 2022: 17th European Conference, Tel Aviv, Israel, October\n2022:17thEuropeanConference,TelAviv,Israel,October23\u2013 23\u201327,2022, Proceedings,Part XXVI, pp.631\u2013648. Springer,\n27, 2022, Proceedings, Part XXXIII, pp. 709\u2013727. Springer, 2022.\n2022. [90] Z. Zheng et al. Prompt Vision Transformer for Domain\n[68] J. Wu et al. Unleashing the power of visual prompting at the Generalization. arXivpreprintarXiv:2208.08914,2022.\npixellevel. arXivpreprintarXiv:2212.10556,2022. [91] C.Maoetal.UnderstandingZero-ShotAdversarialRobustness\n[69] Q. Huang et al. Diversity-aware meta visual prompting. In forLarge-ScaleModels,April2023.\nProceedingsoftheIEEE/CVFConferenceonComputerVision [92] A.Chenetal. Visualpromptingforadversarialrobustness. In\nandPatternRecognition,pp.10878\u201310887,2023. ICASSP2023-2023IEEEInternationalConferenceonAcous-\n[70] Y. Yao et al. CPT: Colorful Prompt Tuning for Pre-trained tics, Speech and Signal Processing (ICASSP), pp. 1\u20135. IEEE,\nVision-LanguageModels. arXiv:2109.11797,2022. 2023.\n[71] A. Shtedritski et al. What does clip know about a red [93] A. Fang et al. Data determines distributional robustness in\ncircle? visual prompt engineering for vlms. arXiv preprint contrastivelanguageimagepre-training(clip). InInternational\narXiv:2304.06712,2023. Conference on Machine Learning, pp. 6216\u20136234. PMLR,\n[72] A.Baretal. Visualpromptingviaimageinpainting. Advances 2022.\nin Neural Information Processing Systems, 35:25005\u201325017, [94] Z. Shi et al. Effective robustness against natural distribution\n2022. shifts for models with different training data. arXiv preprint\n[73] M.U.Khattaketal. Maple:Multi-modalpromptlearning. In arXiv:2302.01381,2023.\nProceedingsoftheIEEE/CVFConferenceonComputerVision [95] L. Xu et al. Exploring the universal vulnerability of prompt-\nandPatternRecognition,pp.19113\u201319122,2023. based learning paradigm. arXiv preprint arXiv:2204.05239,\n[74] B. Dong et al. LPT: Long-tailed Prompt Tuning for Image 2022.\nClassification. arXivpreprintarXiv:2210.01033,2023. [96] N. Carlini and A. Terzis. Poisoning and Backdooring Con-\n[75] Z. Guo et al. Texts as images in prompt tuning for multi- trastiveLearning,March2022.\nlabel image recognition. In Proceedings of the IEEE/CVF [97] J. Jia et al. Badencoder: Backdoor attacks to pre-trained\nConference on Computer Vision and Pattern Recognition, pp. encodersinself-supervisedlearning.In2022IEEESymposium\n2808\u20132817,2023. onSecurityandPrivacy(SP),pp.2043\u20132059.IEEE,2022.\n[76] X.Sunetal. Dualcoop:Fastadaptationtomulti-labelrecogni- [98] H. Bansal et al. Cleanclip: Mitigating data poisoning at-\ntionwithlimitedannotations. AdvancesinNeuralInformation tacks in multimodal contrastive learning. arXiv preprint\nProcessingSystems,35:30569\u201330582,2022. arXiv:2303.03323,2023.\n[77] J. Wen et al. Visual Prompt Tuning for Few-Shot Text [99] S. Agarwal et al. Evaluating clip: towards characterization\nClassification.InProceedingsofthe29thInternationalConfer- of broader capabilities and downstream implications. arXiv\nenceonComputationalLinguistics,pp.5560\u20135570,Gyeongju, preprintarXiv:2108.02818,2021.\nRepublicofKorea,October2022.InternationalCommitteeon [100] C.-Y. Chuang et al. Debiasing vision-language models via\nComputationalLinguistics. biasedprompts. arXivpreprintarXiv:2302.00070,2023.\n[78] X. Gu et al. Open-vocabulary Object Detection via Vi- [101] F.Kongetal. Mitigatingtest-timebiasforfairimageretrieval.\nsion and Language Knowledge Distillation. arXiv preprint arXivpreprintarXiv:2305.19329,2023.\narXiv:2104.13921,2022. [102] B. Smith et al. Balancing the picture: Debiasing vision-\n[79] Y. Du et al. Learning to prompt for open-vocabulary object language datasets with synthetic contrast sets. arXiv preprint\ndetection with vision-language model. In Proceedings of arXiv:2305.15407,2023.\nthe IEEE/CVF Conference on Computer Vision and Pattern [103] K. Gregor et al. Draw: A recurrent neural network for image\nRecognition,pp.14084\u201314093,2022. generation. In International conference on machine learning,\n[80] C.Fengetal. Promptdet:Towardsopen-vocabularydetection pp.1462\u20131471.PMLR,2015.\nusinguncuratedimages.InComputerVision\u2013ECCV2022:17th [104] I.Goodfellowetal. Generativeadversarialnetworks. Commu-\nEuropean Conference, Tel Aviv, Israel, October 23\u201327, 2022, nicationsoftheACM,63(11):139\u2013144,2020.\nProceedings,PartIX,pp.701\u2013717.Springer,2022. [105] S.Reedetal.Generativeadversarialtexttoimagesynthesis.In"}
{"instruction": "Explain this concept:", "response": "19\nInternationalconferenceonmachinelearning,pp.1060\u20131069. arXiv:2212.05032,2022.\nPMLR,2016. [130] D.Epsteinetal.Diffusionself-guidanceforcontrollableimage\n[106] X. Pan et al. Drag your gan: Interactive point-based ma- generation. arXivpreprintarXiv:2306.00986,2023.\nnipulation on the generative image manifold. arXiv preprint [131] B. Kawar et al. Imagic: Text-based real image editing with\narXiv:2305.10973,2023. diffusionmodels. InProceedingsoftheIEEE/CVFConference\n[107] T.Karrasetal. Astyle-basedgeneratorarchitectureforgener- onComputerVisionandPatternRecognition,pp.6007\u20136017,\native adversarial networks. In Proceedings of the IEEE/CVF 2023.\nconference on computer vision and pattern recognition, pp. [132] J.Canny. Acomputationalapproachtoedgedetection. IEEE\n4401\u20134410,2019. Transactionsonpatternanalysisandmachineintelligence,pp.\n[108] P. Isola et al. Image-to-image translation with conditional 679\u2013698,1986.\nadversarial networks. In Proceedings of the IEEE conference [133] L.ZhangandM.Agrawala.Addingconditionalcontroltotext-\non computer vision and pattern recognition, pp. 1125\u20131134, to-image diffusion models. arXiv preprint arXiv:2302.05543,\n2017. 2023.\n[109] D.P.Kingmaetal.Anintroductiontovariationalautoencoders. [134] A. Hertz et al. Prompt-to-prompt image editing with cross\nFoundations and Trends\u00ae in Machine Learning, 12(4):307\u2013 attentioncontrol. arXivpreprintarXiv:2208.01626,2022.\n392,2019. [135] U.Singeretal. Make-a-video:Text-to-videogenerationwith-\n[110] Y.Puetal.Variationalautoencoderfordeeplearningofimages, outtext-videodata. arXivpreprintarXiv:2209.14792,2022.\nlabelsandcaptions.Advancesinneuralinformationprocessing [136] J.Hoetal.Imagenvideo:Highdefinitionvideogenerationwith\nsystems,29,2016. diffusionmodels. arXivpreprintarXiv:2210.02303,2022.\n[111] A.VahdatandJ.Kautz. Nvae:Adeephierarchicalvariational [137] J.Hoetal. Cascadeddiffusionmodelsforhighfidelityimage\nautoencoder. Advances in neural information processing sys- generation. J.Mach.Learn.Res.,23(47):1\u201333,2022.\ntems,33:19667\u201319679,2020. [138] C.Qietal. Fatezero:Fusingattentionsforzero-shottext-based\n[112] A. Ramesh et al. Zero-shot text-to-image generation. In videoediting. arXivpreprintarXiv:2303.09535,2023.\nInternational Conference on Machine Learning, pp. 8821\u2013 [139] J. Z. Wu et al. Tune-a-video: One-shot tuning of image\n8831.PMLR,2021. diffusion models for text-to-video generation. arXiv preprint\n[113] J.Yuetal. Scalingautoregressivemodelsforcontent-richtext- arXiv:2212.11565,2022.\nto-imagegeneration. arXivpreprintarXiv:2206.10789,2022. [140] L. Ruan et al. Mm-diffusion: Learning multi-modal diffusion\n[114] P. Dhariwal and A. Nichol. Diffusion models beat gans on models for joint audio and video generation. In Proceedings\nimage synthesis. Advances in Neural Information Processing oftheIEEE/CVFConferenceonComputerVisionandPattern\nSystems,34:8780\u20138794,2021. Recognition,pp.10219\u201310228,2023.\n[115] J. Sohl-Dickstein et al. Deep unsupervised learning using [141] J.Zhuetal.Moviefactory:Automaticmoviecreationfromtext\nnonequilibrium thermodynamics. In International Conference usinglargegenerativemodelsforlanguageandimages. arXiv\nonMachineLearning,pp.2256\u20132265.PMLR,2015. preprintarXiv:2306.07257,2023.\n[116] D. Jeulin. Dead leaves models: from space tesselation to [142] N. Mu\u00a8ller et al. Diffrf: Rendering-guided 3d radiance field\nrandom functions proc. of the symposium on the advances in diffusion. In Proceedings of the IEEE/CVF Conference on\nthetheoryandapplicationsofrandomsets(fontainebleau,9-11 Computer Vision and Pattern Recognition, pp. 4328\u20134338,\noctober1996)eddjeulin,1997. 2023.\n[117] R. M. Neal. Annealed importance sampling. Statistics and [143] B. Poole et al. Dreamfusion: Text-to-3d using 2d diffusion.\ncomputing,11:125\u2013139,2001. arXivpreprintarXiv:2209.14988,2022.\n[118] J. Ho et al. Denoising diffusion probabilistic models. Ad- [144] B. Mildenhall et al. Nerf: Representing scenes as neural\nvances in Neural Information Processing Systems, 33:6840\u2013 radiance fields for view synthesis. Communications of the\n6851,2020. ACM,65(1):99\u2013106,2021.\n[119] S. Witteveen and M. Andrews. Investigating prompt engi- [145] C.-H. Lin et al. Magic3d: High-resolution text-to-3d content\nneeringindiffusionmodels. arXivpreprintarXiv:2211.15462, creation. arXivpreprintarXiv:2211.10440,2022.\n2022. [146] J.Xuetal. Dream3d:Zero-shottext-to-3dsynthesisusing3d\n[120] W.Wuetal. Diffumask:Synthesizingimageswithpixel-level shapepriorandtext-to-imagediffusionmodels. arXivpreprint\nannotationsforsemanticsegmentationusingdiffusionmodels. arXiv:2212.14704,2022.\narXivpreprintarXiv:2303.11681,2023. [147] H.-H.LeeandA.X.Chang. Understandingpureclipguidance\n[121] R.Beaumont. Clipretrieval:Easilycomputeclipembeddings for voxel grid nerf models. arXiv preprint arXiv:2209.15172,\nandbuildaclipretrievalsystemwiththem,2022. 2022.\n[122] M. Ni et al. Imaginarynet: Learning object detectors without [148] N. M. Khalid et al. Clip-mesh: Generating textured meshes\nrealimagesandannotations. InICLR,2023. from text using pretrained image-text models. arXiv preprint\n[123] R.Heetal. Issyntheticdatafromgenerativemodelsreadyfor arXiv:2203.13333,2022.\nimagerecognition? InICLR,2023. [149] M.Zhangetal.Motiondiffuse:Text-drivenhumanmotiongen-\n[124] O. Avrahami et al. Blended latent diffusion. arXiv preprint erationwithdiffusionmodel.arXivpreprintarXiv:2208.15001,\narXiv:2206.02779,2022. 2022.\n[125] A.Nicholetal.Glide:Towardsphotorealisticimagegeneration [150] J.Kimetal. Flame:Free-formlanguage-basedmotionsynthe-\nand editing with text-guided diffusion models. arXiv preprint sis&editing. arXivpreprintarXiv:2209.00349,2022.\narXiv:2112.10741,2021. [151] G.Tevetetal. Humanmotiondiffusionmodel. arXivpreprint\n[126] R.Galetal. Animageisworthoneword:Personalizingtext- arXiv:2209.14916,2022.\nto-image generation using textual inversion. In The Eleventh [152] T.Yuetal. Scalingrobotlearningwithsemanticallyimagined\nInternationalConferenceonLearningRepresentations,2023. experience. arXivpreprintarXiv:2302.11550,2023.\n[127] N. Ruiz et al. Dreambooth: Fine tuning text-to-image diffu- [153] D. Rempe et al. Trace and pace: Controllable pedestrian\nsion models for subject-driven generation. In Proceedings of animation via guided trajectory diffusion. In Conference on\nthe IEEE/CVF Conference on Computer Vision and Pattern ComputerVisionandPatternRecognition(CVPR),2023.\nRecognition,pp.22500\u201322510,2023. [154] M. A. Shabani et al. Housediffusion: Vector floorplan gen-\n[128] N.Kumarietal. Multi-conceptcustomizationoftext-to-image eration via a diffusion model with discrete and continuous\ndiffusion. arXivpreprintarXiv:2212.04488,2022. denoising. In Proceedings of the IEEE/CVF Conference on\n[129] W. Feng et al. Training-free structured diffusion guidance Computer Vision and Pattern Recognition, pp. 5466\u20135475,\nfor compositional text-to-image synthesis. arXiv preprint 2023."}
{"instruction": "Explain this concept:", "response": "20\n[155] H. Jeong et al. Zero-shot generation of coherent storybook tuningwithsynergisticoptimizationforrelationextraction. In\nfrom plain text story using diffusion models. arXiv preprint Proceedings of the ACM Web Conference 2022, WWW \u201922,\narXiv:2302.03900,2023. pp. 2778\u20132788, New York, NY, USA, 2022. Association for\n[156] Y.Luetal.Multimodalproceduralplanningviadualtext-image ComputingMachinery.\nprompting. arXivpreprintarXiv:2305.01795,2023. [179] L. Cui et al. Template-based named entity recognition using\n[157] A. Jobin et al. The global landscape of ai ethics guidelines. BART. InFindingsoftheAssociationforComputationalLin-\nNatureMachineIntelligence,1(9):389\u2013399,2019. guistics: ACL-IJCNLP 2021, pp. 1835\u20131845, Online, August\n[158] J. Chen et al. Diffusion models for imperceptible and trans- 2021.AssociationforComputationalLinguistics.\nferable adversarial attack. arXiv preprint arXiv:2305.08192, [180] L. Ouyang et al. Training language models to follow instruc-\n2023. tions with human feedback. Advances in Neural Information\n[159] W. Nie et al. Diffusion models for adversarial purification. ProcessingSystems,35:27730\u201327744,2022.\narXivpreprintarXiv:2205.07460,2022. [181] R. Anil et al. Palm 2 technical report. arXiv preprint\n[160] H.Zhuangetal. Apilotstudyofquery-freeadversarialattack arXiv:2305.10403,2023.\nagainst stable diffusion. arXiv preprint arXiv:2303.16378, [182] B.Paranjapeetal.Promptingcontrastiveexplanationsforcom-\n2023. monsense reasoning tasks. arXiv preprint arXiv:2106.06823,\n[161] L. Struppek et al. Rickrolling the artist: Injecting invisible 2021.\nbackdoors into text-guided image generation models. arXiv [183] J.Liuetal. Whatmakesgoodin-contextexamplesforgpt-3?\npreprintarXiv:2211.02408,2022. arXivpreprintarXiv:2101.06804,2021.\n[162] S. Zhai et al. Text-to-image diffusion models can be easily [184] Y.Fuetal.Complexity-basedpromptingformulti-stepreason-\nbackdooredthroughmultimodaldatapoisoning. arXivpreprint ing. arXivpreprintarXiv:2210.00720,2022.\narXiv:2305.04175,2023. [185] P.Lewisetal. Retrieval-augmentedgenerationforknowledge-\n[163] Y. Huang et al. Zero-day backdoor attack against text-to- intensivenlptasks.AdvancesinNeuralInformationProcessing\nimage diffusion models via personalization. arXiv preprint Systems,33:9459\u20139474,2020.\narXiv:2305.10701,2023. [186] S. Borgeaud et al. Improving language models by retrieving\n[164] F.Friedrichetal. Fairdiffusion:Instructingtext-to-imagegen- fromtrillionsoftokens.InInternationalconferenceonmachine\neration models on fairness. arXiv preprint arXiv:2302.10893, learning,pp.2206\u20132240.PMLR,2022.\n2023. [187] G. Izacard et al. Few-shot learning with retrieval augmented\n[165] R.NaikandB.Nushi. Socialbiasesthroughthetext-to-image languagemodels. arXivpreprintarXiv:2208.03299,2022.\ngenerationlens. arXivpreprintarXiv:2304.06034,2023. [188] O.Pressetal. Measuringandnarrowingthecompositionality\n[166] J.Wangetal.T2iat:Measuringvalenceandstereotypicalbiases gap in language models. arXiv preprint arXiv:2210.03350,\nintext-to-imagegeneration. arXivpreprintarXiv:2306.00905, 2022.\n2023. [189] S. M. Kazemi et al. Lambada: Backward chaining for\n[167] A.S.Luccionietal. Stablebias:Analyzingsocietalrepresen- automated reasoning in natural language. arXiv preprint\ntationsindiffusionmodels. arXivpreprintarXiv:2303.11408, arXiv:2212.13894,2022.\n2023. [190] T. Khot et al. Decomposed prompting: A modular approach\n[168] A. Seth et al. Dear: Debiasing vision-language models with for solving complex tasks. arXiv preprint arXiv:2210.02406,\nadditiveresiduals.InProceedingsoftheIEEE/CVFConference 2022.\nonComputerVisionandPatternRecognition,pp.6820\u20136829, [191] Y.Yangetal. Aprompting-basedapproachforadversarialex-\n2023. amplegenerationandrobustnessenhancement. arXivpreprint\n[169] Y.Kimetal. Explainingvisualbiasesaswordsbygenerating arXiv:2203.10714,2022.\ncaptions. arXivpreprintarXiv:2301.11104,2023. [192] X. Dong et al. Promptattack: Probing dialogue state trackers\n[170] Y. Wu et al. Membership inference attacks against text-to- with adversarial prompts. arXiv preprint arXiv:2306.04535,\nimage generation models. arXiv preprint arXiv:2210.00968, 2023.\n2022. [193] P. Delobelle et al. Measuring fairness with biased rulers: A\n[171] J.Duanetal. Arediffusionmodelsvulnerabletomembership comparative study on bias metrics for pre-trained language\ninferenceattacks? arXivpreprintarXiv:2302.01316,2023. models. In Proceedings of the 2022 Conference of the North\n[172] R.Webster. Areproducibleextractionoftrainingimagesfrom American Chapter of the Association for Computational Lin-\ndiffusionmodels. arXivpreprintarXiv:2305.08694,2023. guistics:HumanLanguageTechnologies,pp.1693\u20131706,Seat-\n[173] X. Shen et al. Prompt stealing attacks against text-to-image tle, United States, July 2022. Association for Computational\ngenerationmodels. arXivpreprintarXiv:2302.09923,2023. Linguistics.\n[174] D.Khashabietal. UNIFIEDQA:Crossingformatboundaries [194] R.Louetal. Ispromptallyouneed?no.Acomprehensiveand\nwith a single QA system. In Findings of the Association broader view of instruction learning. CoRR, abs/2303.10475,\nforComputationalLinguistics:EMNLP2020,pp.1896\u20131907, 2023.\nOnline, November 2020. Association for Computational Lin- [195] N. Ding et al. Openprompt: An open-source framework for\nguistics. prompt-learning. InV.Basileetal.,editors,Proceedingsofthe\n[175] Z.Jiangetal.Howcanweknowwhenlanguagemodelsknow? 60thAnnualMeetingoftheAssociationforComputationalLin-\nonthecalibrationoflanguagemodelsforquestionanswering. guistics,ACL2022-SystemDemonstrations,Dublin,Ireland,\nTransactionsoftheAssociationforComputationalLinguistics, May22-27,2022,pp.105\u2013113.AssociationforComputational\n9:962\u2013977,2021. Linguistics,2022.\n[176] B. Lester et al. The power of scale for parameter-efficient [196] X. Wang et al. Images speak in images: A generalist painter\nprompt tuning. In Proceedings of the 2021 Conference on forin-contextvisuallearning.InProceedingsoftheIEEE/CVF\nEmpiricalMethodsinNaturalLanguageProcessing,pp.3045\u2013 Conference on Computer Vision and Pattern Recognition, pp.\n3059,OnlineandPuntaCana,DominicanRepublic,November 6830\u20136839,2023.\n2021.AssociationforComputationalLinguistics. [197] J. Loedeman et al. Prompt generation networks for input-\n[177] T.SchickandH.Schu\u00a8tze.Few-shottextgenerationwithnatural basedadaptationoffrozenvisiontransformers. arXivpreprint\nlanguage instructions. In Proceedings of the 2021 Confer- arXiv:2210.06466,2023.\nence on Empirical Methods in Natural Language Processing, [198] C.-H. Tu et al. Visual query tuning: Towards effective usage\npp. 390\u2013402, Online and Punta Cana, Dominican Republic, of intermediate representations for parameter and memory\nNovember2021.AssociationforComputationalLinguistics. efficient transfer learning. arXiv preprint arXiv:2212.03220,\n[178] X. Chen et al. Knowprompt: Knowledge-aware prompt- 2022."}
{"instruction": "Explain this concept:", "response": "21\n[199] S. Zhang et al. Promptcal: Contrastive affinity learning via\nauxiliary prompts for generalized novel category discovery.\narXivpreprintarXiv:2212.05590,2022.\n[200] H. Salman et al. Unadversarial examples: Designing objects\nforrobustvision. AdvancesinNeuralInformationProcessing\nSystems,34:15270\u201315284,2021.\n[201] Y. Li et al. Exploring the benefits of visual prompting in\ndifferentialprivacy. arXivpreprintarXiv:2303.12247,2023.\n[202] R. Girdhar et al. Imagebind: One embedding space to bind\nthem all. In Proceedings of the IEEE/CVF Conference on\nComputerVisionandPatternRecognition(CVPR),pp.15180\u2013\n15190,June2023.\n[203] B. Li et al. Otter: A multi-modal model with in-context\ninstructiontuning. arXivpreprintarXiv:2305.03726,2023.\n[204] Y.Baietal. Constitutionalai:Harmlessnessfromaifeedback.\narXivpreprintarXiv:2212.08073,2022.\n[205] P. Gao et al. Llama-adapter v2: Parameter-efficient visual\ninstructionmodel. arXivpreprintarXiv:2304.15010,2023.\n[206] K. Gao et al. Backdoor defense via adaptively splitting\npoisoneddataset. InProceedingsoftheIEEE/CVFConference\nonComputerVisionandPatternRecognition,pp.4005\u20134014,\n2023.\n[207] K.Huangetal. Backdoordefenseviadecouplingthetraining\nprocess. arXivpreprintarXiv:2202.03423,2022.\n[208] S.Yangetal. Backdoordefenseviasuppressingmodelshort-\ncuts. InICASSP2023-2023IEEEInternationalConferenceon\nAcoustics, Speech and Signal Processing (ICASSP), pp. 1\u20135.\nIEEE,2023.\n[209] A. Madry et al. Towards deep learning models resistant to\nadversarial attacks. In International Conference on Learning\nRepresentations(ICLR),2017.\n[210] X. Jia et al. Las-at: adversarial training with learnable attack\nstrategy. In Proceedings of the IEEE/CVF Conference on\nComputer Vision and Pattern Recognition, pp. 13398\u201313408,\n2022.\n[211] B. Wu et al. Towards efficient adversarial training on vision\ntransformers. In European Conference on Computer Vision,\npp.307\u2013325.Springer,2022.\n[212] J. Gu et al. Are vision transformers robust to patch pertur-\nbations? In European Conference on Computer Vision, pp.\n404\u2013421.Springer,2022.\n[213] J. Gu et al. Effective and efficient vote attack on capsule\nnetworks. InInternationalConferenceonLearningRepresen-\ntations(ICLR),2021.\n[214] M.Anderljungetal.Frontierairegulation:Managingemerging\nriskstopublicsafety. arXivpreprintarXiv:2307.03718,2023.\n[215] P.Hackeretal. Regulatingchatgptandotherlargegenerative\nai models. In Proceedings of the 2023 ACM Conference on\nFairness, Accountability, and Transparency, pp. 1112\u20131123,\n2023.\n[216] H. Li et al. Do dall-e and flamingo understand each other?\nIn Proceedings ofthe IEEE/CVFInternational Conference on\nComputerVision(ICCV)(toappear),2023."}
{"instruction": "Explain this concept:", "response": "PROMPT ENGINEERING PLAYBOOK\n(Beta v3)\nLast updated 30 Aug 2023\nProduced By GovTech Data Science & AI Division\nThis version has been altered for public consumption. Public officers should check out LaunchPad\ndirectly to download the contextualised version of this Playbook for Public Service."}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nC o n t e n t s\nPROMPT ENGINEERING PLAYBOOK\nIntroduction.................................................................................................................................................................. 4\nNote ................................................................................................................................................................................. 7\nKey Concepts & Terminologies ............................................................................................................................. 9\nPrompt / Prompt Engineering .......................................................................................................................... 10\nModels (Foundation Models / Large-Language Models) ............................................................................ 11\nTemperature / Creativity .................................................................................................................................... 12\nZero Shot, One Shot, Few Shots ....................................................................................................................... 14\nTokens ...................................................................................................................................................................... 18\nHallucination Problems ....................................................................................................................................... 19\nPrompt Engineering Tips ...................................................................................................................................... 25\nThe CO-STAR approach to writing your prompts ..................................................................................... 26\nIterate Till Perfection ........................................................................................................................................... 41\nTask-Specific Prompts ........................................................................................................................................... 44\nWhat Are Task-Specific Prompts ..................................................................................................................... 45\nTask - Rewriting ..................................................................................................................................................... 46\nTask - Extracting .................................................................................................................................................... 54\nTask \u2013 Clustering ................................................................................................................................................... 59\nTask \u2013 Classifying.................................................................................................................................................. 62\nTask \u2013 Summarizing ............................................................................................................................................66\nTask - Generating ................................................................................................................................................... 71\nAdvanced Pro Tips & Tricks ................................................................................................................................. 88\nAdding Emojis ........................................................................................................................................................ 89\nChain Of Thought / Step by Step .................................................................................................................... 90\nRoleplay Mode ....................................................................................................................................................... 91\nTutorials ....................................................................................................................................................................... 95\nTutorial 1 \u2013 Rewriting ..........................................................................................................................................96\nTutorial 2 \u2013 Extracting ....................................................................................................................................... 101\nTutorial 3 \u2013 Clustering ....................................................................................................................................... 107\nTutorial 4 \u2013 Classifying ....................................................................................................................................... 114\nTutorial 5 \u2013 Summary ........................................................................................................................................ 119\nPage 2"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTutorial 6 \u2013 Generate ......................................................................................................................................... 127\nConclusion ............................................................................................................................................................. 134\nPage 3"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nIntroduction\nArtificial Intelligence, or AI, has taken an exciting turn in 2023.\nBack when Alan Turing proposed the Turing Test in 1950, it established a benchmark to\ndetermine whether a computer has reached a level of excellence such that even a human could\nnot tell man and machine apart. Yet, despite the buzz in the early years with some of the best\ntalents working in the field, no real sophisticated AI machines emerged that were powerful enough\nto pass it, let alone give the Turing Test a good challenge. However, science fiction has constantly\nfed us with the vision of sentient AI that can interact with us just like another human being. We\u2019ve\nseen the various portrayals of the AI, either in robotic form or as faceless software - powerful and\nknowledgeable like Hal 9000 in A Space Odyssey, useful and servile like Iron Man\u2019s Jarvis, or\ndestructive and malicious like Skynet or Ultron. Despite the differences in its manifestation, they\nrepresent our collective idea of what the epitome of AI is.\nHowever, for the past few decades, we have yet to see any AI that is remotely close, though\nmankind has never stopped trying. When games started incorporating computer AI to interact\nwith players, they merely projected an illusion of intelligence. Computer AI in games was nothing\nmore than scripted code to get the computer to behave in totally predictable ways, give and take\nsome randomization and variability. In the late 1990s, we saw new advancements in the form of\nrobot dogs, like the Aibo (short for Artificial Intelligence Robot) which was touted to be a trainable\nrobot companion that could walk, bark, whine, growl, and more. However, it was mostly\ndisappointing with its bugginess, and nothing like a digital replacement for man\u2019s best friend.\nThen there was the Furby toy which was marketed with the promise of learning how to speak from\nhumans. It sure got people excited as learning flexibly is one of the holy grails in AI. Furbies started\noff speaking Furbish like \u201cu-nye-boh-doo\u201d (which means \u201cHow are you?\u201d) and eventually learning\nhow to say \u201cHello\u201d in English. But we know now that it was merely pre-programmed to add\nadditional vocabulary over time, such as being able to speak \u201cHello\u201d after it has been turned on\nafter x number of times.\nAs the years went by, we saw two different pursuits \u2013 one being to create the visionary\nsentient AI we all imagined, and the other where the AI is narrower, but sufficiently powerful at\nPage 4"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nthe task it was trained to do. We\u2019ve done better in the latter. The world chess champion, Gary\nKasparov, was beaten in 1996 by IBM\u2019s Deep Blue. Ken Jennings lost to IBM Watson in 2011 in the\ngame of Jeopardy. World Go champion Lee Sedol lost to Google\u2019s Alpha Go in 2016. An AI beat\nelite physicians at a competition in Beijing to diagnose brain tumours in 2018. In 2019, the\nDeepmind AI, AlphaStar, defeated top notch professional gamers in the game of Stracraft II, one\nof the most highest competitive e-sports game. While all these examples of AI triumph showed\nhuge advancements in capabilities, they also continued to amplify the difference between the two\npursuits. Advancements in the narrow AI sense such as face detection, computer vision, anomaly\ndetection, and recommendation systems have all brought the modern society to a level of\ntechnological superiority the world has never seen before.\nBut as for the pursuit of the sentient AI we see in movies, we\u2019ve gained little ground over\nthe years. No doubt, voice assistants from the tech titans like Apple and Google have given us\nhopeful promises every now and then. But they turned out to be no different from the narrow AI\nwhere they are useful helpers for accomplishing specific tasks like adding reminders and playing\nour favourite music. But vary your instructions somewhat and you may get some very frustrating\nreplies. Imagine Iron Man desperately fighting Thanos and giving an instruction to his AI to \u201ccall\nDr Strange for backup\u201d, only to have the AI reply \u201ccalling your doctor back\u201d. We have gradually\naccepted that most chatbots and voice assistants are not a human replacement, and have learnt\nto use them for what they\u2019re worth. The string of Marvel movies from the late 2000s showing Iron\nMan and his AI Assistants, Friday and Jarvis, only served to highlight the contrast between AI in\nthe real world and fiction.\nBut fast forward to the buzz today and it seems like we\u2019re finally making progress. OpenAI\nreleased ChatGPT, which reignited global interest in AI among professionals, academics, and the\ngeneral public. Within a mere two months, it reached a milestone of 100 million users, making it\nthe fastest-growing consumer application in history. The difference is not just in the surprisingly\ngood replies the AI can make, but also the intuitive way humans can now interact with the AI. You\nsimply type in your instructions in plain old English, not programmatic code, and the AI, most of\nthe time, responds with a surprisingly good reply.\nThis opens up the door to many new ways of working not just for the tech developers and\nacademic researchers, but even more so for the everyday folks. Being able to get the AI to return\nPage 5"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nyou the desired results will add a boost to some aspects of your everyday tasks, be it work or\npersonal. Many people worry about being displaced by an AI. We should worry more about\ngetting displaced by a human who has mastered AI to do his work faster and better.\nThe million-dollar question is whether we\u2019re going to see a sentient AI any time soon. Well,\nprobably not. Even though the current wave of AI is one big upgrade in terms of capabilities, the\nunderlying technology is still a long way off from how the human brain truly works. But sometimes,\nthat question is a red herring. The real question is how we can safely harness this new AI capability\nto help you do your job better, right now.\nThis playbook is designed to help you navigate the new ways of interacting with the Large\nLanguage Model (LLM)-powered AI through what we now call prompt engineering. It provides you\nwith practical insights and guidance to best interact with this new form of AI. From basic prompts\nto advanced tips and tricks, this playbook is an indispensable resource for anyone looking to keep\nup with the rapidly-evolving field of AI, and not fall behind many others who are starting to pick\nup better prompt engineering skillsets.\nWe hope that you\u2019ll enjoy reading the playbook as much as we did writing it.\nPage 6"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nNote\nLaunchPad is an AI innovation and experimentation platform by GovTech for the whole-\nof-government. It is only available for public officers. The LaunchPad Playground interface is\nused throughout the Playbook to demonstrate the power of prompt engineering. The\nterminologies and screenshots of user interfaces for keying in your prompts and getting the AI\u2019s\nresponses are aligned to how LaunchPad Playground works and behaves, like what you see\nbelow.\nBut even so, the concepts, practical examples, and tips taught in this Playbook are\napplicable to most other similar ChatGPT-like applications and are not exclusive to only working\nin LaunchPad Playground. As you read this Playbook, we encourage you to follow along and try\nout the suggested prompts and tutorials directly in any LLM-powered chat applications that\nuse the OpenAI models, or comparable ones. For a full experience, you should use\napplications that allow you to vary the temperature settings of the LLM, such as OpenAI\u2019s\nPlayground.\nPage 7"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nAs a disclaimer, while the Prompts and Responses used in the screenshots are real (using\na mix of ChatGPT 3.5 Turbo and the ChatGPT 4 models), some slight modifications have been\nmade to better fit the screenshots into the page by removing some of the non-essential text. Since\nsome of the responses are probabilistic, you may not be able to replicate all of the responses you\nsee in the playbook, especially on another ChatGPT-like application which will carry a different\nstarting System Prompt, or when the \u201cCreativity\u201d setting (you will see what this means later on) is\nset to high.\nPage 8"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nKey Concepts & Terminologies\n\u201cThe beginning of wisdom is the definition of terms.\u201d\n--- Socrates\nPage 9"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nPrompt / Prompt Engineering\nIn the context of interacting with an AI, a prompt refers to a piece of text or input provided\nby a user to initiate a response or action from the AI. A prompt can take many forms, such as a\nquestion, a statement, or a command, and is typically used to provide context or direction to the\nAI's response.\nIn the example above, the prompt is \u201cExplain what AI is\u201d (the text that appears in the box\nwith the black background). The response is what follows (the text that appears over the grey\nbackground). As a fun fact, the picture you see next to the prompt is actually your own avatar in\nLaunchPad which you can customise, and the response appears to come from LaunchPad\u2019s\ncomputer system, hence the LaunchPad icon.\nPrompts are an important part of interacting with an AI, as they enable users to\ncommunicate their intentions to the AI and get back what they need. Prompt Engineering is the\nart of writing effective prompts that can help to improve the accuracy and relevance of the AI's\nresponses, and make the interaction more efficient and productive.\nPage 10"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nModels (Foundation Models / Large-Language Models)\nAn AI like ChatGPT that chats with you is powered by a model.\nThe reason why it is profoundly more powerful than most other chatbots you experience\nin the past is that they use something called a foundation model (FM). An FM specializing in text\nsuch as the one powering ChatGPT is called a large-language model (LLM). An LLM is a machine\nlearning model that was pre-trained on large amounts of data. The emergent behaviour is that it\ninterprets what you type, appears to rationalise and understand what you meant, and provides a\nresponse which seems convincingly like how a knowledgeable fellow human would.\nBut do not be fooled into thinking it knows exactly what you mean. If you will, think of it\nas an autocompletion system where it looks at what you type, and attempts to complete the\nsentences by drawing on words it learnt during its training. Imagine that the AI is merely picking\nthe most statistically probable words to display back to you.\nPage 11"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTemperature / Creativity\nIn the context of natural language processing, the concept of temperature setting refers\nhow random and creative you want the AI's responses to be. A higher temperature setting will\nresult in more diverse and less predictable responses, while a lower temperature setting will\nproduce more conservative and predictable responses. A temperature of 0 gives you deterministic\nresponses, i.e. the same answer is produced every time for the same prompt, at least in theory.\nThis parameter is typically used to balance the need for creativity and coherence in the\nAI's responses, depending on the specific use case and application. If you are asking the AI to\nwrite a poem, you may want to set the Temperature to the highest number of 1. If you are asking\nthe AI to give you precise information, you may want to set the Temperature to the lowest value\nof 0.\nIn LaunchPad Playground, we have used Creativity as a synonym in place of it since the\nconcept of temperature may not be so well-understood.\nThe mapping of Creativity to Temperature is shown below:\nCreativity Temperature\nLow 0\nBalanced 0.5\nHigh 1\nIf you use the following prompt with Creativity set to Low (i.e. Temperature of 0), you will\nalways get the same answer. It could look something like this, and running it a few times should\nPage 12"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\ngive you the same result (even if it may be different from what you see here). You can see that the\nresult is usually more \u201cconservative\u201d.\nHowever, with a creativity setting of High, you can run it a few times to see the different\nresults. The first try yielded this. The AI seems to \u201cthink out of the box\u201d now.\nA second try yielded this instead.\nExplore with the different Creativity settings especially if you feel that your prompts are not getting\nthe responses you need.\nPage 13"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nZero Shot, One Shot, Few Shots\nYou may have heard someone praise ChatGPT before with something like this \u201cWow, it\nperforms very well even with zero shot learning\u201d.\nIn this context of using such LLM-powered chat, one shot, few shots, and zero shots refer\nto how much data you are exposing to the pre-trained model (e.g. OpenAI\u2019s ChatGPT 3.5 Turbo)\nto help it with formulating its response. More specifically,\n\u2022 Few-shot learning is where you provide multiple examples to the AI model, and it is able\nto generalize to new examples of what you need.\n\u2022 One shot learning is similar, but instead of multiple examples, you only pass in one.\n\u2022 Zero-shot learning is where you directly pose a question type of machine learning\nwithout giving it any examples.\nYou can imagine that most models will do better in few-shot learning rather than zero-\nshot since the model is exposed and trained with more data. However, one of the reasons why\nChatGPT is so powerful is because its LLM is able to generalize very well even without being shown\nany examples.\nTake for example that you\u2019re trying to do sentiment analysis here. A zero-shot way of using\nit will look something like this.\nEven though there were no examples shown to the AI, it knows what type of response you\nare looking for and gets it correct.\nPage 14"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nWith few-shots learning, you will type in a prompt like this.\nYou can see that once you\u2019ve shown the AI how you want the response to be like, and how\nit should look like, the AI is able to emulate it pretty well. Since the samples given to the AI are a\nsingle word of positive, negative or neutral, the AI learns that you would like the response to be\njust a single word, which is \u201cpositive\u201d, and dutifully complies. Here\u2019s another example.\nPage 15"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nWhen the words are changed to +1, 0 or -1, the AI learns and responds accordingly.\nFew-shots learning is a very powerful way to control how you want the response of the AI\nto be like.\nLet\u2019s challenge it further to test out the power of few-shots learning. In the prompt below,\nthe samples are given without specifying what the task is about. A description is labelled by Q and\nthe answer we want is labelled by A. The AI is not explicitly told what to do, as compared to the\nexample above where we clearly asked for sentiments.\nPage 16"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nStill, the AI learns what you need, and it knows that we are asking about the country that\nbest fits the description. Note that in this case here, the prompt did not actually state a question\nor a task. Yet, the AI can infer from the pattern of given samples that what follows after \u201cA:\u201d is the\nquestion being asked.\nSo, give this a shot (pun intended) to help you with writing more effective prompts!\nPage 17"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTokens\nTokens are the fundamental units of text in Language Models (LLMs). A token may be a\nword, punctuation mark, or symbol, and represents the smallest unit of text that the LLM can\nprocess. A sentence like \u201cI love Prompt Engineering!\u201d has 26 characters, but 5 tokens, are they are\nas follows (each of the colours represent a new token):\nAnother sentence like \u201cI will master how to use AI before AI becomes my Master!\u201d has 56\ncharacters and 13 tokens.\nAs a rough guide from OpenAI\u2019s website, 100 tokens are roughly equivalent to 75 words,\nand a 1,500 words paragraph is about 2048 tokens.\nWhy do we need to know the concept of tokens? That\u2019s because there is a technical limit\nto how many tokens the AI can process. Different models provided by different companies can\ntake in a different number of tokens. For example, the model powering ChatGPT (GPT 3.5 Turbo)\nis able to accept up to 4096 tokens. Some of the more advanced GPT4 models are able to accept\nup to 32,768 tokens.\nYou need to be mindful of the length of the prompts you are typing in, because the AI can\nonly process so many tokens at a time. In other words, if 4K tokens is the limit and you pass in a\nchunk of text that is way more than that, the AI will not be able to understand the full text you\nprovided. The conversational history that you are used to experiencing when using the chat may\nalso fail because there are simply not enough tokens to remember all of your past chat history.\nPage 18"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nHallucination Problems\nOne important thing to take note of when using such AI powered by Large Language\nModels (LLMs) is that they often generate text that appears coherent and contextually relevant\nbut is factually incorrect or misleading. We call these hallucination problems. This issue arises\ndue to the inherent nature of how LLMs are trained and their reliance on massive datasets. While\nsome of the models like ChatGPT go through a second phase in the training where humans try to\nimprove the responses, there is generally no fact-checking mechanism that is built into these LLMs\nwhen you use them.\nThere is no easy foolproof safeguard against hallucination, although some system prompt\nengineering can help mitigate this. What makes hallucination by LLM worse is that the responses\nare surprisingly real, even if they are absolutely nonsensical. Know that you must never take the\nresponses as-is without fact-checking, and that you are ultimately responsible for the use of the\noutput.\nLook at the example below.\nAt first glance, the summary looks accurate and believable. You may not spot any obvious\nand factual errors in the text. However, if you compare this against the actual article, the entire\nchunk of text starting from \u201cThe Meteorological Service Singapore (MSS) stated \u2026 flash floods during\nthis period\u201d is completely fabricated by the AI.\nPage 19"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nIt is also important to note that some LLMs are unable to access the internet. ChatGPT for\none is unable to browse the web without additional plugins. It begs the question then how it could\nget such an accurate summary even without access to the internet. After all, the first sentence is\naccurate. While we may not know exactly how it learnt to construct the summary as such, one\nsuspicion would be that the AI is merely inferring from the URL to figure out what type of article\nit may be. The area in the response enclosed within the blue box (According to the Straits Times)\ncould be inferred from the main domain in the URL (www.straitstimes.com). The area in the\nresponse enclosed within the green box could be inferred from the descriptive address \u201cmore-\nwarm-nights-and-wet-mornings-expected-for-start-of-june\u201d. What the AI did is merely infer from\nwhatever information you provided in the prompt, and spin up a believable response.\nThis is why you need to always check for the accuracy of the responses, even if they seem\nreal. Remember that the AI is just picking the next probable word to complete the sentences.\nAnother example of hallucination is shown below.\nPage 20"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nThe details are factually wrong, even if the 2008 Summer Olympics really did take place in\nBeijing, China, and there was an opening ceremony. However, the ceremony happened on the 8\nof August (Chinese believed that the number 8 is lucky, so the event opened on the 8th day of the\n8th month), and not 15 of August as the AI responded.\nEven the generated citations can sometimes be wrong. The first and second links are\ncorrect, but the third link is totally made up. If you did not check, you might have trusted the\nPage 21"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nauthenticity of the article just because you saw that it is an article from BBC. But if you follow the\nlink, you will get a \u201cPage cannot be found\u201d notice.\nGranted, maybe this news site might have existed before. Let\u2019s try another one. While the\nAI may sometimes be cognizant that it cannot browse the internet, and thus not being able to\nfully validate the links it show you, it still has no qualms to provide URL links that can be totally\noff.\nPage 22"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nThere is nothing in the image that shows a cheetah.\nMany people have tried using the AI to generate biographies of famous people, or\nthemselves. Hallucination can very easily occur in such cases, especially if there are not a lot of\nmaterials about the person on the internet.\nPage 23"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nFor a prompt like this, the first and second URLs are correct, but the last 2 are wrong (or\nnot found because they are outdated). In conclusion, never completely trust the output of the AI,\nespecially for important information, facts and figures. Always verify with other sources as well.\nPage 24"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nPrompt Engineering Tips\n\u201cLearning a new language opens you up to the culture and knowledge of a\ncivilization.\nLearning prompt engineering opens you up to the world of AI.\u201d\n--- Unknown\nPage 25"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nThe CO-STAR approach to writing your prompts\nThere is no single way to write your prompts. Unlike programming, you do not have to get\nyour prompts to be completely accurate and error-free before the AI will respond. In LaunchPad\nPlayground, like ChatGPT, the AI retains memory of the conversation, and it is perfectly ok to\nwrite your prompts in an iterative manner to get closer to what you want.\nHowever, there are a few tips that you can take note of to sharpen your prompts and help\nthe AI understand what you need faster. If you're new to prompt engineering and want to learn\nhow to write better prompts, the CO-STAR acronym can help you remember how to create\neffective and clear prompts that will get better responses from the AI. Why CO-STAR? What does\nit mean? In short, CO-STAR stands for Context, Objective, Style, Tone, Audience and Response. C\nand O stands for Context and Objective respectively. These two are important is giving the AI\nsome background information, as well as instructing it to perform a specific task.\nBy specifying the Style, Tone, Audience, and Response format & length, you would get a\nresponse closer to what you need.\nHere's a deep dive into what each letter in CO-STAR means:\nC - Context\nProvide the necessary background information that will help the AI understand the topic\nyou want to discuss. This can include any relevant details about the subject matter or the situation\nyou're in.\nThe below shows a vague prompt without any context. As a result, a very generic response\nis returned, which is unlikely to be what you need.\nPage 26"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nWhen the context is provided in the prompt, it elicits a sharper response that will be closer\nto your intent. Take a look at the below when the reason for thanking your colleague is provided.\nPage 27"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nAlso note that it is sometimes good practice to write the Context above the Objective.\nPage 28"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nO - Objective\nClearly state your goal or objective that you want the AI to perform. What response do\nyou want the AI to output? Are you asking a question? Are you asking it to complete a sentence?\nBe as specific as you can.\nHere\u2019s an example of a 2-word prompt, \u201chaiku poem\u201d, which is very ambiguous as to what\nyou want the AI to do. The AI may second-guess that you want it to generate a poem for you.\nHowever, what you might have wanted is to ask the AI to explain what a haiku poem is.\nPhrasing your objective clearly helps the AI respond better.\nPage 29"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nS - Style\nSpecifying the style you want the AI to craft the response in can make the difference\nbetween a generic response vs a specific one. You can influence the AI to mimic a particular\nperson\u2019s style of writing or speaking (provided the person is famous enough with a lot of such\ninformation available out there). Better still, it may not even be a specific famous person, but you\ncan ask the AI to write in the style of a particular expert in a profession (e.g. a career coach).\nWhen the style is not mentioned, the AI may generate a generic response.\nHowever, when the AI adopts a particular style or persona you indicated, it gets into the\nrole and will likely generate a better response to what you need. Here, you can see that it pretends\nto be a career coach and responds with advice on resume, cover letter, and a post-interview\nfollow-up.\nPage 30"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nSome other possible styles you can ask the AI to adopt could be:\n1. Career Coach: Provide advice on career development, job searching, and professional\ngrowth.\n2. Marketing Consultant: Provide advice on marketing strategies, branding, and customer\nengagement.\n3. Personal Stylist: Provide advice on fashion, grooming, and personal style.\n4. Technology Consultant: Provide advice on technology trends, software development,\nand IT infrastructure.\n5. Travel Advisor: Provide advice on travel destinations, accommodations, and activities.\n6. Tech Expert: Provide advice on technology trends, software development, and general\ntech advice.\n7. CEO: Provide advice on business strategy, leadership, and organizational development.\n8. Teacher: Provide advice on education, teaching strategies, and student engagement.\nWarning: Even when the AI is asked to assume the role of an expert, be very careful when trusting\nits responses, especially financial or medical advice. Wrong or misleading information may lead\nto irreversible financial losses or medical complications. You should always double-check the\nadvice you receive from the AI, and not rely solely on it to make important decisions.\nPage 31"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nT \u2013 Tone\nThe AI can also be influenced to respond in a particular tone. Do you want the response\nto be casual? Professional? Or humorous? This is especially useful when you\u2019re using the AI to\nhelp draft a speech, and you want it to be delivered in a particular tone that will resonate better\nwith the audience.\nAs you can see below, without a specified tone, the AI returns a response that is rather\nneutral, and potentially boring.\nWhen the tone is specified, the AI tweaks the response to make it more humorous.\nPage 32"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nSome possible tones you can ask the AI to adopt could be:\n1. Formal: Use formal language and tone to convey professionalism and respect.\n2. Casual: Use informal language and tone to create a friendly and approachable tone.\n3. Humorous: Use humour and wit to add levity and entertainment to the conversation.\n4. Empathetic: Use language that shows understanding and compassion towards the user's\nsituation.\n5. Authoritative: Use confident and assertive language to convey expertise and knowledge.\n6. Inspirational: Use language that is motivational and inspiring to encourage the user to\ntake action or pursue their goals.\nPage 33"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nA \u2013 Audience\nAre you writing to a general audience or a specific group of people? Specifying your\naudience will help the AI tailor its response and chose words and phrases that the audience would\nunderstand or resonate with better. When asked to explain what Leadership is to a CEO, the AI\ntweaks its response by using more professional lingo.\nBut when asked to explain what leadership is to a 6-year-old child, the AI changes its\nresponse to include examples that a child would understand, like talking about a teacher / coach.\nPage 34"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nR \u2013 Response (Length & Format)\nBefore you fire off the prompt, visualise how you want the response to be. Do you want a\nshort answer or a detailed explanation? Do you want it in a wall of text, in numbered form, or in a\ntabular form? While you can leave it to the AI to second-guess your preference, why not instruct\nthe AI to provide something closer to your needs?\nWithout further instructions about the preferred length of the response, the AI comes out\nwith a paragraph-long write-up. However, when asked to expand on its explanation, the AI dives\ndeeper into the history of science fiction, even providing some examples along the way.\nPage 35"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\n\u2026 response removed for brevity \u2026\nSometimes, an entire chunk of text may not be what you need. In the last second\nparagraph above, the AI has returned a list of books and movies that are embedded within the\nparagraph. Wouldn\u2019t it be nice to have them printed out as a list? This is where you enhance your\nprompt by asking the AI to rank the output.\nPage 36"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nYou will notice that the AI was instructed to rank it by popularity, and contend that this\nmay be subjective. It is interesting to note that the AI actually reminds you that popularity can\nvary depending on multiple factors. How about we ask the AI to return something less\ncontroversial, such as the order of publication year? You can see that the AI is able to return you\nthe results as a nicely formatted table.\nPage 37"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nSo add the words \u201creturn it in tabular form\u201d or something to that effect so that the AI can\nreturn you the results that are nicely formatted. You can also ask the AI to return the response to\nyou in a proper report format, like the below.\nPage 38"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nMix and Match Your CO-STAR\nBy following the CO-STAR acronym, you can create prompts that are clear, concise, and\neffective. You do not have to use every single one of them, every single time. Mix and match\ndepending on what your objectives are, and you'll be on your way to doing better prompt\nengineering!\nRemember that the AI is merely your co-star, and you are still the star of the show!\nPage 40"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nIterate Till Perfection\nEven though the CO-STAR method gives you a framework to be very comprehensive in\nyour prompts, it is perfectly fine to iterate and slowly enhance them too. One advantage is that\nyou simply want to see what the AI would return without investing too much time and effort\nupfront in writing very long prompts.\nIteration also simulates a process of brainstorming, where the AI\u2019s responses nudges you\nto think deeper into what you need, and refine your prompts accordingly. Take note that the\nmemory of the earlier prompts & responses persists (up to a certain extent), so there is generally\nno need to repeat what you said previously.\nIn the example shown below, you can see how the process of iterating with the AI can help\nplan out an itinerary for a trip that gets closer to what you need the more you converse with it.\nPage 41"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nWhile the itinerary looks plausible, you may have already visited Mt Fuji and would like an\nalternative suggestion. You can easily do that with a follow-up prompt.\nPage 42"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTask-Specific Prompts\n\u201cUsing an LLM for only its generative capabilities is like buying a swiss army knife\nand only using the blade. There\u2019s much more to it.\u201d\n--- Unknown\nPage 44"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nWhat Are Task-Specific Prompts\nChatGPT has reignited a lot of excitement around AI, but generating stuff often take the\nheadlines. You hear about AI writing poems, code, answer test questions, and more. All these\nhighlight the generative capabilities of AI. However, the power of the AI that are powered by\nmodern Large Language Models (LLMs) go beyond content generation. You can use it to translate\ntext from one language to another, correct your grammar, summarise long text, detect whether a\nfeedback is positive or negative, and much more.\nIn this segment, we\u2019ll run through 6 types of tasks that you can use an LLM-powered AI to\ndo:\n1) Rewriting\n2) Extracting\n3) Classifying\n4) Clustering\n5) Summarising\n6) Generating\nEven though the Playbook groups these tasks into 6 categories, there is no hard-and-fast\nrule that they can only be performed in such defined ways. It is possible to mix-and-match, as well\nas using a prompt that extracts text, rewrite them, and generate a new paragraph of text.\nThe sky is the limit here, so feel free to unleash your creativity as you push the AI to do\nmore for you.\nPage 45"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTask - Rewriting\nOne of the easiest tasks you can get the AI to help you with is in rewriting a passage. A\nrewriting task involves the process of generating new text based on existing text. The goal of a\nrewriting task is to produce text that is similar in meaning to the original text but is expressed in\na different way. This could involve paraphrasing, using a different style and tone, or translating\ntext from one language to another. Or it can be about simplifying a passage or an article.\nSimplification\nPage 46"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nThe Shakespearean style of writing is not the easiest to understand. Fortunately, the AI\ndoes a very good job in interpreting the original text, and rewriting it into a form that is a lot\neasier to understand.\nPage 47"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nCorrection\nOne of the most powerful features of the AI models like ChatGPT is that they write in near-\nperfect, if not perfect, English. As such, you can rely on the AI to help you with grammatical\ncorrections, and general improvement of your sentences while writing.\nPage 48"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTranslation\nSome of the AI models have been trained on multiple languages, and they do a reasonable\njob in some situations to translate paragraphs of text from one language to another. While this\ncan be useful, you must exercise caution especially if there are huge consequences in getting the\ntranslation wrong.\nLet\u2019s see how well the AI performs when translating the Singapore Pledge from the three\nother official languages (Chinese, Malay and Tamil) back into English.\nPage 49"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nThe results of the translations are pretty decent. For the LLM powering ChatGPT, English\nis still its main proficient language, although it also knows various other major languages like\nSpanish, French, Chinese, Korean, etc.\nWarning: Language is complex and sometimes a difference in a word or two could completely\nchange the meaning of the sentence. Do not simply put in text for translation and use the response\ndirectly for display to your audience. Always verify its accuracy with a native speaker.\nPage 51"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nEnhancement\nOne advanced use of the AI in the rewriting task could be to expand and add on to some\nraw notes that you have taken. Imagine going into a meeting, and having only the time to type\nout some rough pointers as such. You can ask the AI to help expand your points into fuller,\nprofessional emails that you can send out.\nThe above will yield a response that looks like this, with similar emails written for Bernard,\nClaire, and Denise that highlights the tasks that they need to perform.\nPage 52"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTask - Extracting\nAnother set of tasks that the AI can help with is extracting information from a given text.\nThis involves identifying and pulling out specific details or data from a larger piece of text. For\nexample, extracting the names of all the characters in a novel, or extracting the key findings from\na research article. This can be useful for formatting information into the way you want it, zooming\nin to particular entities, or for conducting further analysis on specific aspects of the text.\nHere you can see a recipe that is not very well organized. Suppose you want to see what\nall the ingredients are from the recipe. You can use a prompt as shown below.\nThe AI will examine the text, and extract out in bullet points the ingredients that are used\nin the recipe.\nPage 54"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nYou can also use an extraction prompt to distill a long chunk of text into a few key points.\nShown below are 4 paragraphs of text which can take a while to read through. The AI can be\ninstructed to extract 3 key points for you, which will save you a great deal of time reading through\nthe whole passage.\nPage 55"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nWhen instructed, the AI dutifully scans through the document, and makes an assessment\non which sentences or text to extract. Note that this process is left up to the AI to decide which of\nthe sentences are more important, or representative of the entire article, and the level of Creativity\nyou set may influence the results.\nPage 56"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nYou can better control the extraction process by instructing the AI specifically what to extract.\nPage 57"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nThe AI is able to pick out the information you need, and output them in the format you require.\nPage 58"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTask \u2013 Clustering\nClustering involves grouping similar items together based on their characteristics or attributes.\nPage 59"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nIn the example above, even without specifying what the criteria is used for the clustering,\nthe AI makes a reasonably good guess and groups all the items into the correct categories.\nHere\u2019s another example of how the clustering can work.\nPage 60"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nGiven the list of personnel with descriptions such as their job and performance grade, the\nAI can make sense of the common job roles and cluster them together.\nIf instead, we want to group by performance grades, we can also instruct the AI to use that\nclustering criteria.\nPage 61"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTask \u2013 Classifying\nClassification involves labeling text into specific categories, such as sentiments or type of\nissues. By defining the labels and writing the right prompts to match your data to the labels, AI\ncan assist you to give additional information to your data which will help in the identification of\npatterns and trends. This can inform decision-making and aid in the analytics work.\nThis is a simple example of how you can ask the AI to perform a classification task to detect\nthe sentiments of a piece of text.\nPage 62"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nThe AI got all the above correct. You can also specify categories like Littering, Noise and\nSmell and see if the AI can map a feedback into the right category.\nYou do not just have to use single-word categories that are common. You can engineer a\nprompt that are simply A, B and C, and then instruct the AI what these categories mean.\nPage 63"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nIn this case here, the AI once again gets most of the categories correct. However, Tennis\ncould also be in the Category A, since the tennis ball is round. The AI slips up on this and only\nlabelled Tennis under Category B.\nHowever, you can follow-up with another prompt to correct the AI, and see whether it\nadmits the mistake and changes its answer. In this case, the AI reconsiders and corrects its previous\nanswer.\nPage 64"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTask \u2013 Summarizing\nReading long articles can be very time consuming. With the right prompts, the AI can also\nassist you with summarizing text by condensing a longer piece of text into a shorter version that\ncaptures the main points and ideas. Summarizing can be useful for quickly reviewing the key\ninformation of a text, or for creating a brief overview that can be shared with others. This can be\ndone by identifying the main ideas, and presenting them in a clear and concise manner.\nShortener\nTake a look at the story of The Three Little Pigs below.\nPage 66"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nLet\u2019s see how the AI performs when asked to summarize the 4 paragraphs of the story.\nThe succinct one paragraph summary captures most of the key points of the story, where\nthe first two pigs were using straw and sticks as their building materials, whereas the third little\npig built a brick house that withstood the wolf.\nKey Points\nYou do not just have to ask the AI to simply shorten the article or text when you are\nsummarizing. You can also ask it to just condense the story into one or more key points, like the\nbelow.\nPage 67"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nThe response changes into a one liner (note that the AI may not always obey you and may\nstill come up with two or three liners) and condenses the story into a statement.\nPage 68"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nMerging Summary\nSometimes, you may have different notes of a similar event or meeting like below.\nPage 69"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nHere, both Alan and Bernice have taken notes of the same meeting. While both of them\nhave captured most of the common points, each of them recorded the notes differently, and there\nare slight variations in the information capture. With a simple prompt, you can instruct the AI to\nsummarize both of their notes and combine them into one.\nPage 70"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTask - Generating\nA generate task involves the process of generating new text based on a given prompt or\ncontext. The goal could be to generate coherent and relevant text that you can use in an email, a\nspeech, or your marketing collaterals. You can also use it to write poems, tell jokes, and write\nstories. While a generative task is also where the hallucination problem is the most obvious, it can\nbe quite useful to instruct the AI to create a first draft to avoid staring at a blank document, or to\nbrainstorm multiple ideas with you to start you off.\nSpeech Writing\nWe sometimes forget that most of what the AI is really doing, is to perform an\nautocomplete task. And autocompleting your sentences or even phrases is where it can generate\ncontents to complete your writing, or brainstorm ideas with you.\nWhen you set the Creativity to High (i.e. the Temperature setting is 1), you can run the\nprompt again to get a different result.\nSo, all you need to provide the AI is your starting phrase, and it will generate out contents\nthat it thinks best meets what you want.\nPage 71"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nHowever, don\u2019t just leave it to chance. With more guidance and by providing a structure\nand frame, you can really shape the response closer to what you need. The below example shows\nhow you can use the AI to help craft a speech delivered at a graduation ceremony by just providing\nsome key points.\nThis is merely a shortened example, and you can follow up by asking the AI to expand\nupon its points.\nPage 72"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nMarketing\nFor marketers and staff in corporate communications roles, there are often a lot of\nmarketing pitches in various formats \u2013 emails, LinkedIn, Facebook, Instagram, that they need to\nchurn out every other day. Starting to write from scratch may be a thing of the past now that AI\ncan help you with it. Imagine writing a pitch for a company recruitment event like the below.\nThis forms a first draft for you. But it\u2019s not perfect, and it may not carry your key messages.\nYou can enhance it by iterating with the AI further, by combining some of the earlier tasks you\nlearnt before like the rewriting prompt.\nPage 74"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nWhy not go one step further then and ask it to generate an appropriate LinkedIn post for\nyou.\nPage 75"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nThe AI is smart enough to know what hashtags to include for the event. Since there will be\nmultiple marketing channels that a marketer will need to post at, you can keep on going by asking\nthe AI to generate another presentation of the contents by leveraging on what you have created\nso far, but tailored for another social media platform.\nPage 76"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nCommendation / Testimonial\nFrom time to time, we may have to write commendation emails to thank our peers. If you\nare a manager, you may have to write performance appraisals for your staff every year. If you are\na teacher, you may have students coming to you to write testimonials for them before they\ngraduate. Drafting such contents in perfect English can sometimes be daunting, or time-\nconsuming even if you are a good writer. But AI is a perfect companion for this.\nThe AI would easily generate out the following.\nPage 78"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nIn this example, you can see how a short 76 words prompt can be expanded into a 181\nwords performance appraisal (or more if you need by asking the AI to generate it longer). The\nbest part? It\u2019s almost always in perfect English free of grammatical errors (it would be quite\nembarrassing if you send your colleagues, reportees or your students a write-up that has a lot of\ntypos).\nPage 79"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nA teacher can easily generate the following for his students as well. Here, you can see some\nadditional \u201ctricks\u201d that have been added into the prompts, where you can specify information\nused like variables (if you are familiar with programming), and have them appear in the response.\nYou can see that by defining the information of Start Term, and End Term, and instructing\nthe AI to \u201cstart off\u201d and \u201cend off\u201d its generated response by using those pieces of information, the\nresult gets closer to what you may need, even though the AI did not exactly follow your last\ninstruction to end off with a specific sentence. This is one of the quirks of using an LLM, where\nyou are not always able to instruct it to follow what you want to the dot.\nPage 80"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTest Questions\nYou may sometimes want to generate quizzes either as a fun trivia, or a quick test of\nknowledge. This could be useful for HR professionals running some company wide welfare event\nand needed some fun pop quiz to warm up the event. It could also be useful for teachers who\nhave just taught a particular topic, and wanted to generate quick test questions. For motivated\nstudents, they can even generate test questions themselves to practise for their exams.\nImagine this set of company information that a HR staff can copy off the company website.\nThe HR staff is running an orientation programme for the newbies who joined, and wanted to run\na pop quiz for them, drawing from the company information.\n***\nIntroduction:\nXYZ Synapse is a Singapore-based technology company that was established\non May 15, 2021. The company's primary focus is to provide innovative\nsolutions that transform businesses and improve people's lives. XYZ Synapse\nis committed to upholding its core values of Integrity, Innovation, Excellence,\nCollaboration, and Customer Satisfaction. These values are the foundation of\nthe company's culture and guide its decision-making processes.\nStaff Strength:\nXYZ Synapse has a team of 50 highly skilled and dedicated employees who\nwork together to achieve the company's goals. The company's staff strength\ncomprises of professionals from various fields, including software engineering,\ndata science, marketing, and customer support. The company values its\nemployees and invests in their training and development to ensure they have\nthe necessary skills to deliver exceptional service to clients.\nCompany Vision:\nXYZ Synapse aims to be a leading provider of innovative solutions that\ntransform businesses and improve people's lives. The company's vision is to\nPage 82"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nbecome the go-to technology partner for businesses looking to stay ahead of\nthe curve in the ever-changing digital landscape. XYZ Synapse recognizes the\nimportance of technology in modern-day business operations and seeks to\nprovide solutions that enable businesses to operate more efficiently, reduce\ncosts, and increase revenue.\nCompany Mission:\nXYZ Synapse's mission is to empower businesses with cutting-edge\ntechnology and exceptional customer service that exceeds expectations. The\ncompany strives to provide solutions that are tailored to the unique needs of\neach client, ensuring that they get the most out of their investment. XYZ\nSynapse's commitment to exceptional customer service means that clients can\nexpect prompt responses to their queries and concerns and a willingness to\ngo above and beyond to ensure their satisfaction.\nFinancial Performance:\nXYZ Synapse has had a successful year, with a projected profit of SGD 5 million\nfor 2022. The company's financial success can be attributed to its focus on\ninnovation, excellence, and customer satisfaction. XYZ Synapse's ability to\ndeliver solutions that meet the unique needs of its clients has enabled it to\nbuild a loyal customer base and attract new clients through word-of-mouth\nreferrals.\nConclusion:\nXYZ Synapse's commitment to its core values, vision, and mission has enabled\nit to establish itself as a leading provider of innovative solutions in Singapore.\nThe company's success is a testament to the hard work and dedication of its\nemployees and the trust and support of its clients. XYZ Synapse is poised for\ncontinued growth and success in the coming years.\nPage 83"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\n***\nWith a prompt like the below, she is able to generate a set of question and answers (with\n4 options) that she can now easily use for the pop quiz.\nNote that this prompt works on the basis that there is a prior context of the company information.\nNext, assume a teacher has just taught the Physics chapter on Newtonian motion. The\nbody of knowledge could look something like below.\nPage 84"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\n***\nSir Isaac Newton's three laws of motion are the foundation of classical\nmechanics. They describe the relationship between a body and the forces\nacting upon it, and how the body responds to those forces.\nThe first law, also known as the law of inertia, states that an object at rest will\nremain at rest, and an object in motion will remain in motion at a constant\nvelocity, unless acted upon by an external force. This means that an object will\ncontinue to move in a straight line at a constant speed unless a force acts upon\nit to change its motion. For example, a book sitting on a table will remain there\nuntil someone picks it up or pushes it off.\nThe second law, also known as the law of acceleration, states that the\nacceleration of an object is directly proportional to the force applied to it, and\ninversely proportional to its mass. This means that the greater the force applied\nto an object, the greater its acceleration will be, and the more massive an\nobject is, the less it will accelerate for a given force. The equation F=ma is used\nto calculate the force required to achieve a certain acceleration.\nThe third law, also known as the law of action and reaction, states that for\nevery action, there is an equal and opposite reaction. This means that when\none object exerts a force on another object, the second object exerts an equal\nand opposite force back on the first object. For example, when you jump off a\ndiving board, you push down on the board, and the board pushes back up on\nyou, propelling you into the air.\nOverall, Newton's three laws of motion provide a framework for understanding\nhow objects move and interact with each other. They are fundamental to many\nareas of science and engineering, from designing cars and airplanes to\nexploring the cosmos.\n***\nPage 85"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nThe teacher (or even the student himself) can use a prompt like the below to generate a\nset of test questions that he can administer during class as a practice. As always, it is important\nnot to rely solely on the AI for accuracy, and the teacher should always go in and verify that the\nquestion-and-answer pairs are indeed correct.\nNote that this prompt works on the basis that there is a prior context of what the Newtonian Laws are.\nPage 86"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nWhile this will not replace the need for teachers and lecturers in setting exam questions, it\nmight be able to provide quick in-class activities and checkpoints to test the students\u2019 knowledge.\nBy showing the AI more examples of how the questions are set, we should potentially be able to\nimprove the types of questions that are being generated.\nPage 87"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nAdvanced Pro Tips & Tricks\n\"The more that you read, the more things you will know. The more that you learn,\nthe more places you'll go.\"\n- Dr. Seuss\nPage 88"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nAdding Emojis\nThe AI is very capable of adding emojis into its responses. Just give it the word.\nPage 89"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nChain Of Thought / Step by Step\nSometimes you may wonder why the AI came to a particular conclusion, especially when\nthe answer is wrong. As of the state of technology now (in May 2023), it is hard to understand\nexactly how the AI generated a particular response. However, you can ask it to also generate a list\nof steps as to how it came to its conclusion. Simply add a \u201cShow your chain of thought\u201d to your\nprompt and you will see it make an attempt to explain. During the process, sometimes it may\nimprove the output. Alternatively, you can also tell the AI to \u201cshow your workings step by step\u201d or\n\u201cthink step by step\u201d.\nPage 90"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nRoleplay Mode\nSo far, we\u2019ve usually seen the interaction being rather one way; you put in a prompt and\nthe AI responds back to you. That is usually the end of the conversation until you decide to put in\nanother prompt to ask the AI to do something else for you. You can actually turn the interaction\ninto a dialogue.\nTry something like the below.\nYou see how this has become more interactive, and your following prompt is not exactly\na prompt as how you\u2019ve been learning it, but more like an answer to the AI\u2019s question.\nPage 91"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nThe AI will keep going and encouraging you to respond as it gathers enough information\nto answer your initial task, which is to identify a job role suitable for you, and suggest some plans\nto get there.\nPage 92"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nLet\u2019s stop here. You can see how this is going and you can keep on answering, or you can\njust drop off when you\u2019ve obtained the responses you need. This becomes a very powerful tool\nfor you to tap on the \u201cbrain\u201d of the AI to mentor or advise you in areas you are unsure of.\nPage 94"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTutorials\n\"Practice isn't the thing you do once you're good. It's the thing you do that makes you\ngood.\"\n--- Malcolm Gladwell\nPlease attempt the tutorial questions first before you take a look at the answers following it.\nDo bear in mind that there is no single one way to arrive at the same AI responses. Also, the\nperformance of the AI may change over time, and the responses you see here may not always\nbe the same as what you\u2019ll get.\nPage 95"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTutorial 1 \u2013 Rewriting\nWarning: Note that the data used here is purely fictional unless otherwise stated.\nQuestion 1\nPick one of the quotes below and translate it into\nanother language that you\u2019re familiar with.\n\u2022 A rose by any other name would smell as sweet. By William Shakespeare\n\u2022 That\u2019s one small step for a man, a giant leap for mankind. By Neil Armstrong\n\u2022 Speak softly and carry a big stick. By Theodore Roosevelt\n\u2022 Life is like riding a bicycle. To keep your balance, you must keep moving. By Albert\nEinstein\nPage 96"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nQuestion 2\nWe want a 6-year-old to be able to understand this.\nDigitalisation is a key pillar of the Government\u2019s public service transformation efforts. The\nDigital Government Blueprint (DGB) is a statement of the Government\u2019s ambition to better\nleverage data and harness new technologies, and to drive broader efforts to build a digital\neconomy and digital society, in support of Smart Nation.\nOur vision is to create a Government that is \u201cDigital to the Core, and Serves with Heart\u201d. A\nDigital Government will be able to build stakeholder-centric services that cater to citizens\u2019\nand businesses\u2019 needs. Transacting with a Digital Government will be easy, seamless and\nsecure. Our public officers will be able to continually upskill themselves, adapt to new\nchallenges and work more effectively across agencies as well as with our citizens and\nbusinesses.\nTwo years after the launch of the DGB, the Government has introduced new policies and\ninitiatives. COVID-19 has also reaffirmed our emphasis on capability building, and\ncompelled different parts of the Government to accelerate the use of data and of\ntechnology to offer digital services that minimise physical contact, and to use technology\nand digital tools to keep us safe.\nThe DGB has been updated to accurately reflect the current plans and to push for more\nambitious goals to pursue deeper and more extensive digitalisation within the\nGovernment. New examples are included to better explain the latest efforts and benefits\nof Digital Government. The refresh is in line with the approach to improve the blueprint\niteratively.\nExtracted from: https://www.tech.gov.sg/digital-government-blueprint/\nPage 97"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nQuestion 3\nThe text below is to be printed in the papers tomorrow.\nFind out what typos it may have, and correct them.\nProduct Transummariser is launching today!\nWe are excited to annouce the lauch of our new product that revoluzionizes the way\nmeetings are conducted and documented. Our product transcribes meetings in real-time\nand provides a concize summary of the discussion points, action items, and decisions\nmade during the meeting. This summary is then automaticaly emailed to all team\nmembers, ensuring that everyone was on the same page and has a clear understanding\nof what was discused. With this product, meetings are more effecient, productive, and\ninclusive, allowing teams to focus on what really matters - achieving their goals.\nPage 98"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTutorial 1 Answers\nCreativity\nQn Prompt Response Remarks\nSettings\n1 Balanced - That\u2019s one small step for a man, a \"\u90a3\u662f\u4e00\u4e2a\u4eba\u7684\u5c0f\u6b65\uff0c\u5374\u662f\u4eba\u7c7b\u7684\u5de8\u5927\u98de Pick any quote and\ngiant leap for mankind. By Neil the prompt should be\n\u8dc3\" - \u5c3c\u5c14\u00b7\u963f\u59c6\u65af\u7279\u6717\n(Temperature = 0.5) Armstrong similar. Change\nRewrite the above quote into \u201cChinese\u201d to any other\nChinese. language you are\nfamiliar with.\n2 Balanced <insert the chunk of text> The Government wants to use\ncomputers and new technology to make\n(Temperature = 0.5) Rewrite the above into a summary things easier and better for everyone in\nthat a 6 year old can understand. Singapore. They have a plan called the\nDo not use difficult vocabulary.\nDigital Government Blueprint that aims\nAdd in examples and analogies\nto create a government that is \"Digital to\nthat a child will have experience\nthe Core, and Serves with Heart\". This\nwith.\nmeans that they want to make it easy for\npeople to do things like pay bills or\nrenew licenses online, just like how you\ncan play games on your tablet or watch\nvideos on YouTube. They also want to\nuse technology to keep us safe, like how\nyou wear a helmet when you ride your\nbike. The Government is always learning\nand trying new things to make things\nbetter for us, just like how you learn new\nthings in school to become smarter and\nbetter at things.\n3 Balanced Product Transummariser is Product Transummariser is launching\nlaunching today! today!\n(Temperature = 0.5) We are excited to annouce the\nlauch of our new product that We are excited to announce the launch\nrevoluzionizes the way meetings of our new product that revolutionizes\nare conducted and documented. the way meetings are conducted and\nOur product transcribes meetings documented. Our product transcribes\nin real-time and provides a concize meetings in real-time and provides a\nsummary of the discussion points, concise summary of the discussion\naction items, and decisions made points, action items, and decisions made\nduring the meeting. This summary during the meeting. This summary is\nis then automaticaly emailed to all then automatically emailed to all team\nteam members, ensuring that members, ensuring that everyone is on\neveryone was on the same page the same page and has a clear\nand has a clear understanding of understanding of what was discussed.\nwhat was discused. With this With this product, meetings are more\nproduct, meetings are more efficient, productive, and inclusive,\neffecient, productive, and inclusive, allowing teams to focus on what really\nallowing teams to focus on what matters - achieving their goals.\nPage 99"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nreally matters - achieving their\ngoals. Changes:\nRewrite the above and correct\n\u2022 \"annouce\" changed to\ngrammatical or spelling errors.\n\"announce\"\nAfter that, provide a list of the\n\u2022 \"revoluzionizes\" changed to\nchanges.\n\"revolutionizes\"\n\u2022 \"concize\" changed to\n\"concise\"\n\u2022 \"discused\" changed to\n\"discussed\"\n\u2022 \"automaticaly\" changed to\n\"automatically\"\n\u2022 \"effecient\" changed to\n\"efficient\"\nPage 100"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTutorial 2 \u2013 Extracting\nWarning: Note that the data used here is purely fictional unless otherwise stated.\nQuestion 1\nFill in the prompt below\nText John Smith\nEmma Johnson\nMichael Davis\nOlivia Williams\nAbbey Thompson\nPrompt ???\nResponse Last Name +\nLast Name First Name\nyou want First Name\nSmith John Smith, John\nJohnson Emma Johnson, Emma\nDavis Michael Davis, Michael\nWilliams Olivia Williams, Olivia\nThompson Abbey Thompson, Abbey\nPage 101"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nQuestion 2\nFill in the prompt below\nText Three individuals have been arrested in connection with a series of thefts\nin the downtown area. The suspects have been identified as John Smith,\na 25-year-old male from the United States, Maria Gomez, a 32-year-old\nfemale from Mexico, and Ahmed Ali, a 40-year-old male from Egypt.\nAccording to police reports, Smith was caught stealing electronics from a\nlocal store. He was found to have stolen several laptops and\nsmartphones, with a total value of approximately $10,000. Smith has\nbeen charged with burglary and grand theft.\nGomez was arrested for stealing jewelry from a high-end boutique. She\nwas found to have taken several valuable pieces, including diamond\nnecklaces and bracelets, with a total value of approximately $50,000.\nGomez has been charged with grand theft and possession of stolen\nproperty.\nAli was apprehended for stealing cash from a local bank. He was found\nto have broken into the vault and taken several bags of cash, with a total\nvalue of approximately $100,000. Ali has been charged with burglary and\ngrand theft.\nAll three suspects are currently in police custody and are awaiting trial.\nThe police department has commended the officers involved in the\ninvestigation for their quick and efficient work in apprehending the\nsuspects.\nPrompt ???\nResponse Name of Criminal: John Smith\nyou want Age: 25\nPage 102"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nSex: Male\nNationality: United States\nCrime (include items stolen): Stealing electronics (burglary and grand\ntheft) - laptops and smartphones worth approximately $10,000\nName of Criminal: Maria Gomez\nAge: 32\nSex: Female\nNationality: Mexico\nCrime (include items stolen): Stealing jewelry (grand theft and possession\nof stolen property) - diamond necklaces and bracelets worth\napproximately $50,000\nName of Criminal: Ahmed Ali\nAge: 40\nSex: Male\nNationality: Egypt\nCrime (include items stolen): Stealing cash from a bank (burglary and\ngrand theft) - several bags of cash worth approximately $100,000\nPage 103"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nQuestion 3\nFill in the prompt below. From the trip report, you want to\nbe able to extract out action items that need to be done.\nText Trip Report:\nI am writing to provide a summary of our recent visit to the ABC Synapse\nfacility on [Date]. The purpose of the trip was to gain a deeper\nunderstanding of how ABC Synapse builds and sells electric vehicles\n(EVs) and to explore potential collaboration opportunities.\nDuring the visit, we had the opportunity to tour the manufacturing\nfacility, meet with key personnel, and discuss the company's approach to\nEV production and sales. The following are some key observations and\ninsights from the trip:\n\u2022 Manufacturing Process: ABC Synapse has a highly automated and\nefficient production line, utilizing advanced robotics and AI-\ndriven systems to optimize the assembly of their electric vehicles.\nThis has resulted in reduced production times and increased\noutput capacity. Let\u2019s make a note here to conduct a detailed\nanalysis of ABC Synapse's manufacturing process to identify best\npractices and potential areas for improvement in our own\nproduction facilities.\n\u2022 Battery Technology: The company has invested heavily in\nresearch and development to create high-performance, long-\nlasting batteries for their EVs. Their proprietary battery\ntechnology offers extended range and faster charging times\ncompared to competitors.\n\u2022 Sales and Distribution: ABC Synapse has adopted a direct-to-\nconsumer sales model, bypassing traditional dealerships. This\nallows them to maintain better control over the customer\nexperience and offer competitive pricing.\nGiven our company\u2019s push for EV production, let\u2019s schedule a follow-up\nmeeting with ABC Synapse's R&D team to discuss potential\nPage 104"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\ncollaboration on battery technology and explore opportunities for joint\nresearch projects. We will also need to review our current sales and\ndistribution strategies to determine if adopting a direct-to-consumer\nmodel similar to ABC Synapse's approach would be beneficial for our\ncompany.\nPlease feel free to reach out if you have any questions or require further\ninformation.\nPrompt ???\nResponse 1. Conduct a detailed analysis of ABC Synapse's manufacturing\nyou want process to identify best practices and potential areas for\nimprovement in our own production facilities.\n2. Schedule a follow-up meeting with ABC Synapse's R&D team to\ndiscuss potential collaboration on battery technology and\nexplore opportunities for joint research projects.\n3. Review our current sales and distribution strategies to determine\nif adopting a direct-to-consumer model similar to ABC Synapse's\napproach would be beneficial for our company.\nPage 105"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTutorial 2 Answers\nCreativity\nQn Prompt Response Remarks\nSettings\n1 Balanced John Smith As above\nEmma Johnson\n(Temperature = 0.5) Michael Davis\nOlivia Williams\nAbbey Thompson\nFor each of the names above in\nthe format of First Name, Last\nName, extract out in table form\nwith the first column being Last\nName and the second column\nbeing First Name and the third\ncolumn being Last Name + First\nName.\n2 Balanced <insert the news report> As above\n(Temperature = 0.5) Extract the following information\nfrom the above passage for each\nof the criminals:\nName of Criminal:\nAge:\nSex:\nNationality:\nCrime (include items stolen):\n3 Balanced <insert the trip report> As above Notice how that even though the action\nitem \u201c1. Conduct a detailed analysis of\n(Temperature = 0.5) Extract from the above passage ABC Synapse's manufacturing process to\nany action items that needs to be identify best practices and potential areas\ndone.\nfor improvement in our own production\nfacilities.\u201d is embedded within the key\nobservations away from the other 2 action\nitems, it is being detected and extracted.\nPage 106"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTutorial 3 \u2013 Clustering\nWarning: Note that the data used here is purely fictional unless otherwise stated.\nQuestion 1\nGet the AI to form meaningful clusters\nData 1) \"The Great Gatsby\" by F. Scott Fitzgerald\n2) \"Thriller\" by Michael Jackson\n3) \"To Kill a Mockingbird\" by Harper Lee\n4) \"Giraffe\"\n5) \"1984\" by George Orwell\n6) \u201cMy Heart Will Go On\u201d by Celine Dion\n7) \"Dolphin\"\n8) \"Elephant\"\n9) \" Hey Jude\u201d by The Beatles\n10) \" Cosmos by Carl Sagan\n11) \"The Hobbit\" by J.R.R. Tolkien\n12) \"Cheetah\"\n13) \"Octopus\"\n14) \"Hotel California\" by The Eagles\n15) \u201cPenguin\u201d\nPrompt ???\nResponse Group 1:\nyou want\n\u2022 Giraffe\n\u2022 Dolphin\n\u2022 Elephant\n\u2022 Cheetah\n\u2022 Octopus\n\u2022 Penguin\nPage 107"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nGroup 2:\n\u2022 Thriller by Michael Jackson\n\u2022 My Heart Will Go On by Celine Dion\n\u2022 Hotel California by The Eagles\n\u2022 Hey Jude by The Beatles\nGroup 3:\n\u2022 The Great Gatsby by F. Scott Fitzgerald\n\u2022 To Kill a Mockingbird by Harper Lee\n\u2022 1984 by George Orwell\n\u2022 Cosmos by Carl Sagan\n\u2022 The Hobbit by J.R.R. Tolkien\nPage 108"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nQuestion 2\nCompany Meeting Reports\nData Your company spoke to several potential partners in tech. The following\ncall reports are recorded after two weeks of meetings.\n1. ABC Tech: Our representative met with ABC Tech to discuss the\nintegration of their video analytics solutions into our\norganization's security framework. Although their AI-driven\ntechnology shows potential in enhancing surveillance systems,\nconcerns were raised regarding the privacy implications of\nimplementing such solutions. Both parties agreed to conduct\nfurther research to address these concerns before moving\nforward with any collaboration. We also asked them to show\ndocumentation of compliance to local laws.\n2. DEF Synapse: Our team met with DEF Synapse to explore the\napplication of their innovative Deep Neural Networks for\nenhancing our data analysis processes. While their AI technology\nappeared promising, the complexity of integrating it into our\nexisting systems raised concerns about the feasibility and\nrequired resources. They did not seem to support\nimplementation on the cloud. Both parties agreed to continue\ndiscussions but will re-evaluate the practicality of this potential\npartnership.\n3. GHI Robotics: We engaged with GHI Robotics to discuss the\npotential incorporation of their cutting-edge manufacturing\nrobotics into our production facilities. Unfortunately, the high\ncosts associated with the implementation of their advanced\nrobotic systems led to doubts about the overall return on\ninvestment. When we probed them about the recent incident\nPage 109"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nwhere the robot malfunctioned and crushed a worker\u2019s arm, they\nquickly downplayed that as a one-off unfortunate accident, and\nthe bugs have been fixed. To be cautious, our team decided to\npostpone any collaboration and explore alternative solutions.\n4. JKL Innovations: We had a fruitful meeting with JKL Innovations\nto discuss the potential implementation of blockchain and crypto\ntechnology within our existing infrastructure. They shared that\nthey have gotten the go-ahead from authorities all over the\nworld like US, China, Dhubai, Australia to operate their business.\nThe company shared their expertise in web3 solutions, and we\nagreed to conduct additional research and analysis to determine\nthe feasibility of this partnership, in particular, integrating them\ninto payment systems here in Singapore.\n5. MNO Solutions: Our team met with MNO Solutions to explore\npotential consulting services aimed at enhancing our business\nprocesses and overall strategy. Despite the company's wide-\nranging experience in providing tailored solutions, their\nproposed consulting fees appeared to be significantly higher\nthan our budgetary constraints allowed. We will be reconsidering\nour options and seeking alternative service providers.\n6. PQR Automation: Our representative had a productive meeting\nwith PQR Automation to discuss the possibility of collaborating\non electric vehicle (EV) development projects. However, the\ncompany's limited track record in the EV industry raised concerns\nabout their ability to effectively challenge established giants like\nTesla. While their website and marketing materials boasted of big\ninvestors funding them, they were unable to reveal any big\nnames to us citing confidentiality clauses. We agreed to monitor\ntheir progress before committing to any partnership.\nPage 110"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\n7. STU Dynamics: We met with STU Dynamics to investigate the\nopportunities for integrating their next-generation robotics\ntechnology into our organization's operations. The company\nshowcased their advanced robotic systems, and we are\nenthusiastic about the prospects of this collaboration. Further\nevaluations will be conducted to determine the most effective\napproach.\nPrompt ???\nResponse Group 1 (Follow-up Actions):\nyou want\n1. ABC Tech: Integration of video analytics solutions; concerns\nabout privacy implications; further research needed;\ndocumentation of compliance requested.\n2. JKL Innovations: Implementation of blockchain and crypto\ntechnology; additional research and analysis required; exploring\nintegration into payment systems.\n3. STU Dynamics: Integration of next-generation robotics\ntechnology; further evaluations needed to determine the most\neffective approach.\nGroup 2 (Lower Interest to Proceed):\n4. DEF Synapse: Application of Deep Neural Networks; concerns\nabout complexity, feasibility, and required resources; no support\nfor cloud implementation; re-evaluating practicality.\n5. GHI Robotics: Incorporation of manufacturing robotics; high costs\nand doubts about return on investment; concerns about recent\nrobot malfunction incident; exploring alternative solutions.\n6. MNO Solutions: Consulting services for business processes and\nstrategy; proposed fees higher than budget; seeking alternative\nservice providers.\nPage 111"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\n7. PQR Automation: Collaboration on electric vehicle development;\nconcerns about limited track record and ability to challenge\nestablished giants; unable to reveal big investors; monitoring\nprogress before committing.\nPage 112"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTutorial 3 Answers\nCreativity\nQn Prompt Response Remarks\nSettings\n1 Low <insert the list of 15 items> As above.\n(Temperature = 0) Cluster the above into 3 groups in\na list format - Group 1 is animals,\nGroup 2 is music, and Group 3 is\nbooks. If unsure, put them into\nGroup 4.\n2 Balanced <insert the list of company As above.\nreports>\n(Temperature = 0)\nCluster the reports above into 2\nclusters - Group 1 is for meetings\nwhere there are follow-up actions,\nand Group 2 is for meetings where\nthere is lower interest to proceed.\nIf there are concerns raised, show\nit as well.\nThen, summarize the contents of\nGroup 1 and Group 2. Do not\nmake up new contents.\nPage 113"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTutorial 4 \u2013 Classifying\nWarning: Note that the data used here is purely fictional unless otherwise stated.\nQuestion 1\nBelow are some feedback responses about a course. Use the\nAI to determine if the course is well-received or not.\nData Feedback 1 - \"Mr. Baker Ree's cooking course was a great experience!\nThe course was affordable, and the cooking school had excellent\nequipment, especially the very cool oven. I attended other courses\nbefore that charged an arm and a leg for just learning one or two\nrecipes. This one had ten!\"\nFeedback 2 - \"Hygiene \u2013 1 star. Food variety \u2013 4 stars. Trainer \u2013 0 stars.\"\n\"Mr. Baker Ree's cooking course was a great value for the money. The\nschool had modern equipment, and people are generally nice. The\nlocation at Orchard also make it very accessible. We also learned how to\nmake a wide variety of dishes.\"\nFeedback 3 - \"Aiyo \u2026 I didn't enjoy Mr. Baker Ree's cooking course. The\ntrainer laughs at us when we make mistakes. Can like that meh? Very\nunprofessional. Ask him questions, he also never answer. Then we don\u2019t\nknow so when he ask, I cannot answer. Then I got laughed at. Siao.\nWon\u2019t go again. \"\nFeedback 4 - \"Mr. Baker Ree's cooking course was fantastic! What\nimpressed me most was the brand-new Gaggenau ovens and appliances.\nMake me feel like a diva cooking for celebrities.\"\nFeedback 5 - \"I loved Mr. Baker Ree's cooking course! The price was\nunbeatable, and the school had some of the best cooking equipment\nI've seen. The course mates were also amazing, and we had a blast\ncooking together.\"\nFeedback 6 - \"I wouldn't recommend Mr. Baker Ree's cooking course.\nHe didn't seem to take hygiene seriously, and sometimes I saw him\nPage 114"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\ntalking over the food and even spitting while talking. It really turned me\noff from the course.\"\nFeedback 7 - \u201cGreat investment! The school was modern and well-\nequipped, and I made some fantastic friends in the course. We also\nlearned so many recipes that I can't wait to try at home!\"\nFeedback 8 - \"Bad. Too much talking. Too little cooking.\"\nFeedback 9 - \u201ctesting, testing\u201d\nPrompt ???\nResponse Group 1 (Positive): Feedback 1, Feedback 4, Feedback 5, Feedback 7\nyou want\n\u2022 Affordable course with a good number of recipes taught\n\u2022 Excellent and modern equipment, including Gaggenau ovens and\nappliances\n\u2022 Great value for money\n\u2022 Accessible location at Orchard\n\u2022 Wide variety of dishes taught\n\u2022 Friendly course mates and enjoyable cooking experience\n\u2022 Good investment, with many recipes to try at home\nGroup 2 (Negative): Feedback 3, Feedback 6, Feedback 8\n\u2022 Poor hygiene, with the trainer talking and spitting over the food\n\u2022 Unprofessional trainer who laughs at mistakes and doesn't\nanswer questions\n\u2022 Too much talking and not enough cooking\nGroup 3 (Neutral): Feedback 2, Feedback 9\n\u2022 Feedback 2 provides mixed opinions, with positive remarks on\nfood variety and equipment, but negative comments on hygiene\nand the trainer\n\u2022 Feedback 9 is a test and does not provide any relevant\ninformation\nPage 115"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nQuestion 2\nClassify which feedback goes to which department for\nfollow-up\nData Feedback 1 - I was walking past Block 52 last night. I stepped onto a\nbanana peel and fell down. There were a lot of broken glass bottles on\nthe floor which cut me. Please do something about this, as a lot of\npeople are drinking in the area, and leaving behind their stuff.\nFeedback 2 - There is constant drilling sounds from late midnight to\n3am. It is coming from my neighbour above. Can you do something\nabout it?\nFeedback 3 - I was coming home last night and something felt funny. I\nthought my gas stove was on, but it wasn't. I leaned out of my window,\nand I think there's some weird stench coming from the floor below mine.\nMay be a decomposing corpse. Can you investigate?\nFeedback 4 - There is a strong stench coming from the rubbish chute\nfor the past week. Is nobody clearing it?\nFeedback 5 - A lot of people are leaving their beer bottles all around the\nvoid deck. It is very unsightly.\nFeedback 6 - I hear sounds of bouncing balls from the floor above. They\nare very inconsiderate. What can I do?\nAdditional Littering issues go to Department L\nInfo\nSmell issues go to Department S\nNoise issues go to Department N\nPrompt ???\nPage 116"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nResponse Group 1 (Littering issues - Department L):\nyou want\n\u2022 Feedback 1: Banana peel and broken glass bottles on the floor\n\u2022 Feedback 5: Beer bottles left around the void deck\nGroup 2 (Smell issues - Department S):\n\u2022 Feedback 3: Weird stench coming from the floor below\n\u2022 Feedback 4: Strong stench coming from the rubbish chute\nGroup 3 (Noise issues - Department N):\n\u2022 Feedback 2: Constant drilling sounds from late midnight to 3am\n\u2022 Feedback 6: Sounds of bouncing balls from the floor above\nPage 117"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTutorial 4 Answers\nCreativity\nQn Prompt Response Remarks\nSettings\n1 Low <insert the list of feedback> As above.\n(Temperature = 0) Based on the above feedback,\nclassify them into 3 groups -\nGroup 1 is positive, Group 2 is\nnegative, and Group 3 is neutral by\nlisting the Feedback ID.\n2 Low Based on the above feedback, As above.\nclassify them into 3 groups:\n(Temperature = 0) <insert the list of feedback>\nGroup 1 is for Littering issues\nhandled by Department L.\nGroup 2 is for Smell issues\nhandled by Department S.\nGroup 3 is for Noise issues\nhandled by Department N.\nPage 118"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTutorial 5 \u2013 Summary\nWarning: Note that the data used here is purely fictional unless otherwise stated.\nQuestion 1\nSummarize News Report\nData ABC Media\nHeadline: \"Embracing the Future: New Digital Payment Programme\nLaunches with Discounts and Convenience\"\nThe new Digital Payment programme is now live, with trials taking place\nat 25 locations across hawker centres, food centres, and coffee shops.\nPaying via QR code with any smartphone has never been easier, and\ncustomers can benefit from a 5% discount until the end of the year.\nResidents, like Ms Lim, appreciate the convenience of going cashless,\nsaying she no longer needs her wallet for food purchases. Many\nbusinesses have also welcomed the initiative, signalling a bright future for\ncashless transactions.\nDuring the launch, the minister responsible for the programme shared his\nenthusiasm for the new payment system, stating that it aligns with Country\nX\u2019s vision of becoming a Smart Nation. He believes that cashless\ntransactions will improve efficiency and provide better financial security\nfor both residents and businesses alike.\nThe minister also mentioned other countries that have successfully\nadopted cashless payment systems, such as China, Sweden, and South\nKorea, which have experienced significant growth in digital transactions\nover the past few years. Drawing from these examples, Country X aims to\nleverage on the Digital Payment programme to boost its economy and\nenhance the quality of life for its citizens.\nPage 119"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nDEF Newstand\nHeadline: \"Controversial Digital Payment Programme Causes Chaos and\nBusiness Discontent\"\nThe recent launch of the new Digital Payment programme has been met\nwith confusion and dissatisfaction. ABC Bakery, a tenant in one of the trial\nlocations, claims they were left in the dark about the programme until it\nbegan, causing customer confusion. Additionally, DEF Snacks & Biscuits\nargue that the 3% surcharge on businesses is unjust, especially as\ncustomers enjoy a 5% discount. Furthermore, Mr Mohammed expressed\nhis frustration with the lack of a cash option, citing inconvenience when\nhe forgets his phone.\nIn response to the concerns, the minister in charge of the programme\nadmitted that the rollout may not have been smooth in some cases, but\nemphasized that the trial phase is essential for identifying and addressing\nany issues before expanding the initiative nationwide. They also\nmentioned that other countries, such as China and Sweden, experienced\nsimilar growing pains during their own cashless transitions, but have since\nreaped significant benefits.\nThe minister also assured business owners that their concerns regarding\nthe surcharge would be taken into consideration, with plans to reassess\nthe fee structure and explore potential solutions to ensure a fair outcome\nfor all parties involved. Furthermore, they clarified that cash would still be\naccepted during the transition period, stating that the programme aims\nto promote cashless transactions while accommodating the needs of\nresidents like Mr Mohammed who prefer traditional payment methods.\nPrompt ???\nResponse Summary:\nyou want\nThe Digital Payment Programme has been launched in Country X, with\ntrial runs at 25 locations across hawker centres, food centres, and coffee\nshops. While the initiative aims to promote cashless transactions and has\nPage 120"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nbeen well-received by some residents and businesses, certain issues have\nsurfaced, including confusion, dissatisfaction with the 3% surcharge for\nbusinesses, and the desire for a cash payment option.\nKey Points Requiring Follow-up from the Ministry:\n\u2022 Address the confusion and communication issues experienced by\nbusinesses, such as ABC Bakery, during the launch of the\nprogramme.\n\u2022 Review and reassess the 3% surcharge imposed on businesses,\ntaking into consideration the concerns raised by DEF Snacks &\nBiscuits and other affected parties.\n\u2022 Address the need for a cash option alongside the Digital\nPayment programme to accommodate residents who prefer\ntraditional payment methods, like Mr Mohammed.\n\u2022 Collect and utilize feedback from businesses and residents\nthroughout the trial phase to improve and refine the programme\nbefore expanding it nationwide.\n\u2022 Ensure that the experiences and lessons learned from other\ncountries that successfully implemented cashless payment\nsystems (e.g., China, Sweden, South Korea) are applied to Country\nX's Digital Payment programme for a smoother and more\nefficient rollout.\nPage 121"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nQuestion 2\nSummarize Company Meeting Reports\nData Your company spoke to several potential partners in tech. The following\ncall reports are recorded after two weeks of meetings.\n1. ABC Tech: Our representative met with ABC Tech to discuss the\nintegration of their video analytics solutions into our\norganization's security framework. Although their AI-driven\ntechnology shows potential in enhancing surveillance systems,\nconcerns were raised regarding the privacy implications of\nimplementing such solutions. Both parties agreed to conduct\nfurther research to address these concerns before moving\nforward with any collaboration. We also asked them to show\ndocumentation of compliance to local laws.\n2. DEF Synapse: Our team met with DEF Synapse to explore the\napplication of their innovative Deep Neural Networks for\nenhancing our data analysis processes. While their AI technology\nappeared promising, the complexity of integrating it into our\nexisting systems raised concerns about the feasibility and\nrequired resources. They did not seem to support\nimplementation on the cloud. Both parties agreed to continue\ndiscussions but will re-evaluate the practicality of this potential\npartnership.\n3. GHI Robotics: We engaged with GHI Robotics to discuss the\npotential incorporation of their cutting-edge manufacturing\nrobotics into our production facilities. Unfortunately, the high\ncosts associated with the implementation of their advanced\nrobotic systems led to doubts about the overall return on\ninvestment. When we probed them about the recent incident\nPage 122"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nwhere the robot malfunctioned and crushed a worker\u2019s arm, they\nquickly downplayed that as a one-off unfortunate accident, and\nthe bugs have been fixed. To be cautious, our team decided to\npostpone any collaboration and explore alternative solutions.\n4. JKL Innovations: We had a fruitful meeting with JKL Innovations\nto discuss the potential implementation of blockchain and crypto\ntechnology within our existing infrastructure. They shared that\nthey have gotten the go-ahead from authorities all over the\nworld like US, China, Dhubai, Australia to operate their business.\nThe company shared their expertise in web3 solutions, and we\nagreed to conduct additional research and analysis to determine\nthe feasibility of this partnership, in particular, integrating them\ninto payment systems here in Singapore.\n5. MNO Solutions: Our team met with MNO Solutions to explore\npotential consulting services aimed at enhancing our business\nprocesses and overall strategy. Despite the company's wide-\nranging experience in providing tailored solutions, their\nproposed consulting fees appeared to be significantly higher\nthan our budgetary constraints allowed. We will be reconsidering\nour options and seeking alternative service providers.\n6. PQR Automation: Our representative had a productive meeting\nwith PQR Automation to discuss the possibility of collaborating\non electric vehicle (EV) development projects. However, the\ncompany's limited track record in the EV industry raised concerns\nabout their ability to effectively challenge established giants like\nTesla. While their website and marketing materials boasted of big\ninvestors funding them, they were unable to reveal any big\nnames to us citing confidentiality clauses. We agreed to monitor\ntheir progress before committing to any partnership.\nPage 123"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\n7. STU Dynamics: We met with STU Dynamics to investigate the\nopportunities for integrating their next-generation robotics\ntechnology into our organization's operations. The company\nshowcased their advanced robotic systems, and we are\nenthusiastic about the prospects of this collaboration. Further\nevaluations will be conducted to determine the most effective\napproach.\nPrompt ???\nResponse Group 1 (Follow-up Actions):\nyou want\n8. ABC Tech: Integration of video analytics solutions; concerns\nabout privacy implications; further research needed;\ndocumentation of compliance requested.\n9. JKL Innovations: Implementation of blockchain and crypto\ntechnology; additional research and analysis required; exploring\nintegration into payment systems.\n10. STU Dynamics: Integration of next-generation robotics\ntechnology; further evaluations needed to determine the most\neffective approach.\nGroup 2 (Lower Interest to Proceed):\n11. DEF Synapse: Application of Deep Neural Networks; concerns\nabout complexity, feasibility, and required resources; no support\nfor cloud implementation; re-evaluating practicality.\n12. GHI Robotics: Incorporation of manufacturing robotics; high costs\nand doubts about return on investment; concerns about recent\nrobot malfunction incident; exploring alternative solutions.\n13. MNO Solutions: Consulting services for business processes and\nstrategy; proposed fees higher than budget; seeking alternative\nservice providers.\nPage 124"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\n14. PQR Automation: Collaboration on electric vehicle development;\nconcerns about limited track record and ability to challenge\nestablished giants; unable to reveal big investors; monitoring\nprogress before committing.\nSummary:\nGroup 1 consists of meetings with ABC Tech, JKL Innovations, and STU\nDynamics, where there are follow-up actions planned. These meetings\nfocused on the integration of video analytics, blockchain and crypto\ntechnology, and next-generation robotics technology, respectively.\nConcerns and further research are needed in each case.\nGroup 2 includes meetings with DEF Synapse, GHI Robotics, MNO\nSolutions, and PQR Automation, where there is lower interest to proceed.\nConcerns in these meetings revolved around complexity, feasibility, high\ncosts, and limited track records. In some cases, alternative solutions or\nservice providers are being sought.\nPage 125"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTutorial 5 Answers\nCreativity\nQn Prompt Response Remarks\nSettings\n1 Balanced <insert the news reports> As above.\n(Temperature = 0.5) The above news reports talk about\nthe Digital Payment Programme\nour country is running.\nSummarize the two news report\nsuccinctly. Then list out key points\nmade that requires follow-up from\nthe ministry.\n2 Low <insert the list of feedback> As above.\n(Temperature = 0) Based on the above feedback,\nclassify them into 3 groups -\nGroup 1 is positive, Group 2 is\nnegative, and Group 3 is neutral by\nlisting the Feedback ID.\nAfter that, for each of the groups,\nsummarise the key points of the\nfeedback given. Do not make up\nnew contents.\nPage 126"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTutorial 6 \u2013 Generate\nWarning: Note that the data used here is purely fictional unless otherwise stated.\nQuestion 1-3\nMarketing For Tech Conference\nTask Marketing Department is organising a Tech Conference in 3 months\ntime. Several speakers have agreed to come and speak. Generate the\nfollowing:\nQn 1 - Overall tagline for the event\nQn 2 - Synopsis for each of the key speakers\nQn 3 - Marketing description for the event\nAdditional Guest of Honour: Dr Xavier McCarthy, Minister for Technology, Country X\nInfo\nVenue: Big Tech Convention Centre Lvl 3\nDate: 20 Nov 2023\nTime: 9am to 5pm\nKeynote Speaker 1\nAlan T, Director of AI from ABC Tech\nTopics:\n\u2022 Recent advancements in video analytics technology\n\u2022 How video analytics can be applied in various industries\n\u2022 Role of AI in developing new video analytics products and\nsolutions\nKeynote Speaker 2\nBernard Shaylor, Head of Data from DEF Synapse, a deep neural\nnetworks company\nTopics:\n\u2022 Discuss the advancements in deep neural networks\n\u2022 How they're driving innovation in data analysis\nPage 127"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\n\u2022 Potential impact on industries, academia, and government\ndecision-making processes\nKeynote Speaker 3\nCaroline B. Simmons, Senior Vice President of Sustainablity from GHI\nRobotics\n\u2022 Highlight the growing importance of sustainability in the robotics\nindustry\n\u2022 Strategies for integrating eco-friendly design and materials into\nthe development and production of robotic systems.\nOther speakers will be talking on a broad range of topics in AI, Robotics,\nDeep Learning, Responsible AI, Sustainability, etc.\nPartner companies include:\n\u2022 ABC Tech is an AI company building video analytics solutions\n\u2022 DEF Synapse is an AI company building innovative Deep Neural\nNetworks\n\u2022 GHI Robotics is a robotics company dealing in manufacturing\nrobotics\n\u2022 JKL Innovations is a new web3 company focusing on blockchain\nand crypto technology,\n\u2022 MNO Solutions is a consultancy company,\n\u2022 PQR Automation is a new EV company rising up to challenge\nTesla,\n\u2022 STU Dynamics is a robotics company building next-gen multi-\npurpose robots\n\u2022 VW Technologies is a consultancy company for all things tech,\nespecially Data Science and AI,\n\u2022 XYZ Secure Tech is a consultancy company focusing on\ncybersecurity, penetration testing, and virus & malware\nprevention.\nPage 128"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nPrompt ???\nResponse N.A. No right answers here. Assess the results for yourself.\nyou want\nPage 129"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nTutorial 6 Answers\nCreativity\nQn Prompt Response Remarks\nSettings\n1 High Brainstorm 5 possible taglines for 1. \"Revolutionizing Industries with AI,\na conference event that cover Robotics, and Deep Learning for a\n(Temperature = 1) topics such as AI, Robotics, Deep Sustainable Future\"\nLearning, Responsible AI and 2. \"Responsible AI: Paving the Way for\nSustainability, etc. Be inspiring and a Better Tomorrow\"\ncreative that will catch the 3. \"Building a Sustainable World with\nattention of industry professionals. AI and Robotics\"\n4. \"From Data to Action: Exploring the\nPower of AI and Deep Learning\"\n5. \"Innovative Solutions for a\nSustainable World: AI, Robotics, and\nBeyond\"\n2 High <insert all 3 topics bullet points Keynote Speaker 1: Alan T, Director of AI\nhere> from ABC Tech - \"Join us as we delve into\n(Temperature = 1) the latest advancements in video analytics\nFor each of the keynote\ntechnology and discover its endless\npresentations, generate a one\npossibilities across various industries.\nparagraph synopsis that will be\nLearn from Alan T, Director of AI at ABC\nshown on the marketing materials.\nTech, as he discusses the integral role of AI\nInclude contents and a call to\nin developing new video analytics\naction for industry professionals to\nproducts and solutions. Don't miss this\nbe captivated and motivated to\nopportunity to explore and discover how\ntake part.\nvideo analytics can reshape the future of\nyour industry!\"\nKeynote Speaker 2: Bernard Shaylor, Head\nof Data from DEF Synapse, a deep neural\nnetworks company - \"Exciting\nbreakthroughs in deep neural networks\nare revolutionizing the field of data\nanalysis, and Bernard Shaylor, Head of\nData at DEF Synapse, is at the forefront of\nthis innovation. Join us as he discusses\nhow deep neural networks are driving\ninnovation and changing the way\nindustries, academia, and government\nmake decisions. Be inspired to adapt and\nlearn about the potential impact of deep\nneural networks in your field.\"\nKeynote Speaker 3: Caroline B. Simmons,\nSenior Vice President of Sustainability from\nGHI Robotics - \"As sustainability becomes\nPage 130"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nmore important in the robotics industry,\nCaroline B. Simmons, Senior Vice President\nof Sustainability at GHI Robotics, sheds\nlight on the strategies for integrating eco-\nfriendly design and materials into the\ndevelopment and production of robotic\nsystems. Don't miss this opportunity to\nhear from one of the leading experts on\nsustainability in robotics and learn how\nyou can be a part of creating a sustainable\nfuture.\"\n3 High Guest of Honour: Dr Xavier Dear Industry Professionals,\nMcCarthy, Minister for Technology,\n(Temperature = 1) Country X It is our pleasure to invite you to attend\nthe upcoming conference at the Big Tech\nVenue: Big Tech Convention Convention Centre on November 20, 2023,\nCentre Lvl 3 from 9 am to 5 pm. Join us for an exciting\nday of exploring cutting-edge technology\nDate: 20 Nov 2023\nwith key industry leaders in video analytics,\ndeep neural networks, and robotics,\nTime: 9am to 5pm\namong others.\nKeynote Speaker 1: Alan T,\nWe are honored to welcome Dr. Xavier\nDirector of AI from ABC Tech -\nMcCarthy, Minister for Technology,\n\"Join us as we delve into the latest\nCountry X, as the Guest of Honour, who\nadvancements in video analytics\nwill give an opening address on the\ntechnology and discover its\nimportance of staying ahead of the curve\nendless possibilities across various\nin the ever-evolving tech industry.\nindustries. Learn from Alan T,\nAdditionally, we have an impressive lineup\nDirector of AI at ABC Tech, as he\nof keynote speakers, including Alan T from\ndiscusses the integral role of AI in\nABC Tech, Bernard Shaylor from DEF\ndeveloping new video analytics\nSynapse, and Caroline B. Simmons from\nproducts and solutions. Don't miss\nGHI Robotics, who will discuss recent\nthis opportunity to explore and\nadvancements and trends in AI, robotics,\ndiscover how video analytics can\nand sustainability. Moreover, our\nreshape the future of your\nesteemed partner companies, such as JKL\nindustry!\"\nInnovations, MNO Solutions, PQR\nKeynote Speaker 2: Bernard Automation, STU Dynamics, VW\nShaylor, Head of Data from DEF Technologies, and XYZ Secure Tech, will\nSynapse, a deep neural networks exhibit their latest innovations and share\ncompany - \"Exciting their expertise with attendees. Don't miss\nbreakthroughs in deep neural out on this opportunity to network and\nnetworks are revolutionizing the learn from industry experts, and register\nfield of data analysis, and Bernard now to secure your spot!\nShaylor, Head of Data at DEF\nSynapse, is at the forefront of this\ninnovation. Join us as he discusses\nhow deep neural networks are\ndriving innovation and changing\nthe way industries, academia, and\ngovernment make decisions. Be\nPage 131"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\ninspired to adapt and learn about\nthe potential impact of deep\nneural networks in your field.\"\nKeynote Speaker 3: Caroline B.\nSimmons, Senior Vice President of\nSustainability from GHI Robotics -\n\"As sustainability becomes more\nimportant in the robotics industry,\nCaroline B. Simmons, Senior Vice\nPresident of Sustainability at GHI\nRobotics, sheds light on the\nstrategies for integrating eco-\nfriendly design and materials into\nthe development and production\nof robotic systems. Don't miss this\nopportunity to hear from one of\nthe leading experts on\nsustainability in robotics and learn\nhow you can be a part of creating\na sustainable future.\"\nPartner companies include:\n\u2022 ABC Tech is an AI company\nbuilding video analytics\nsolutions\n\u2022 DEF Synapse is an AI company\nbuilding innovative Deep\nNeural Networks\n\u2022 GHI Robotics is a robotics\ncompany dealing in\nmanufacturing robotics\n\u2022 JKL Innovations is a new web3\ncompany focusing on\nblockchain and crypto\ntechnology,\n\u2022 MNO Solutions is a\nconsultancy company,\n\u2022 PQR Automation is a new EV\ncompany rising up to\nchallenge Tesla,\n\u2022 STU Dynamics is a robotics\ncompany building next-gen\nmulti-purpose robots\n\u2022 VW Technologies is a\nconsultancy company for all\nthings tech, especially Data\nScience and AI,\n\u2022 XYZ Secure Tech is a\nconsultancy company\nfocusing on cybersecurity,\nPage 132"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\npenetration testing, and virus\n& malware prevention.\nBased on the above information,\nwrite a short two paragraphs\ncontent that will be included in an\nemail to be sent out to all industry\nprofessionals under our\ncommunity mailing list. Make it\nsuccinct and inspiring, and\nhighlight the minister's\ninvolvement as the Guest of\nHonour.\nPage 133"}
{"instruction": "Explain this concept:", "response": "Prompt Engineering Playbook\nConclusion\nThis is the end of the Beta version of the Prompt Engineering Playbook. If you\u2019ve enjoyed it, please\nhelp us spread the word and share it with others that may benefit from this!\nPage 134"}
{"instruction": "Explain this concept:", "response": "MASTER THE\nART OF PROMPT\nENGINEERING\nStrategies for Optimization and Evaluation\nAuthors:\nNazarii Drushchak\nNataliya Polyakovska\nDmytro Zikrach"}
{"instruction": "Explain this concept:", "response": "INTRODUCTION\nPrompt engineering has emerged as a critical discipline in large language models (LLMs), which is\npivotal in maximizing the effectiveness and reliability of AI-enabled systems. The essence of prompt\nengineering lies in crafting input queries that guide these models to generate desired outputs with\nprecision. The quality and structure of these prompts strongly correlate with the performance,\nstability, fairness, and practical utility of LLM systems.\nIn this paper, SoftServe delves into the multifaceted impact of prompt engineering, beginning\nwith its foundational significance to enhance LLM operations. We explore how strategic prompt\ndesign improves the accuracy and relevance of model responses and drives business outcomes by\nenabling more sophisticated and context-aware AI applications. Beyond its direct benefits, prompt\nengineering serves as a lens through which the nuances of human-AI interaction and communication\nbecome apparent, offering insights into how models perceive and process natural language inputs.\nA critical aspect of harnessing the potential of prompt engineering involves the evaluation of\nprompt effectiveness. SoftServe discusses various techniques to assess and measure the impact of\ndifferent prompts on model performance, providing a framework for continuous improvement and\noptimization. Moreover, we outline strategies for improvement prompts to align closely with specific\ngoals and use cases, demonstrating how incremental adjustments will yield significant improvements\nin results.\nIn this paper, SoftServe will highlight the paramount importance of prompt engineering in using\nLLMs to their full potential by offering practical insights and strategies for optimization and\nevaluation.\nSIGNIFICANCE OF PROMPT ENGINEERING\nLLMs are powerful. They offer vast potential across many applications, from writing assistance to\ncomplex problem-solving. Prompt quality crucially determines their effectiveness. A well-crafted\nprompt leads Generative AI models to deliver pertinent and precise results. In contrast, a poor\nprompt may result in irrelevant outcomes. This emphasizes the importance of prompt engineering\nin Gen AI.\nPrompt engineering involves optimizing the technical aspects of prompts, refining queries or\ninstructions to generate precise, contextually appropriate, and efficient responses. Techniques like\nfew-shot learning and specificity in prompts are used to enhance the model's understanding of\ncontext and desired outcomes, minimizing errors and biases.\nWhite Paper Master the Art of Prompt Engineering 2"}
{"instruction": "Explain this concept:", "response": "On the other hand, prompt design focuses on user-friendliness, ethical considerations, and\ninclusivity. Design considerations encompass factors such as language simplicity, inclusivity,\nfairness, and safety, which contribute to the overall usability and acceptability of AI systems.\nIn the realm of LLMs, prompt engineering and designing outweigh model retraining. Unlike the\nlatter, which involves revising the entire AI architecture, prompt engineering only focuses on\nrefining instructions. This targeted approach is advantageous when resource or time constraints\nmake model retraining impractical.\nMoreover, prompt engineering enables quick adaptations to evolving contexts without extensive\nretraining cycles, which is too expensive for LLMs. By combining both prompt engineering and\ndesign principles, developers create prompts that optimize AI performance and enhance user\nsatisfaction and trust. This integrated approach emerges as a pivotal strategy for enhancing LLM\nperformance and versatility across diverse applications.\nPROMPT ENGINEERING VS. DESIGN\nPrompt engineering involves the creation of instructions to elicit desired responses from LLMs,\noptimizing for accuracy and relevance.\nPrompt design, a broader process, encompasses the creation, evaluation, and distillation of prompts for\nuser-friendliness, ethics, and inclusivity.\nASPECT PROMPT ENGINEERING PROMPT DESIGN\nFocus Scientific, iterative testing, and Artistic, strategic framing, and focus on\nmodel modification communication\nPurpose Optimize model output Create effective and responsible\nprompts\nTechniques Format specification, iterative Clarity, simplicity, and fairness\ntesting, context, and structure\nScope Narrow and focused on model Broad and includes user experiences\noptimization and ethical considerations\nWhite Paper Master the Art of Prompt Engineering 3"}
{"instruction": "Explain this concept:", "response": "GOOD VS. BAD PROMPTS\nSoftServe explores the prompt engineering concept with examples of good and bad prompts and why\nthis distinction matters. There is also a look at advanced techniques like chain-of-thought prompting.\nEXAMPLE 1\nBad prompt: Example of a good prompt:\n\"Give me a report on sales.\" \"Generate a detailed monthly sales\nreport for Q1 2023, comparing product\nlines A, B, and C in the North American\nGenerated output example\nmarket, and highlight key growth\n(by gpt-3.5-turbo):\ndrivers and areas for improvement.\"\nThe total sales for the period were $X\nmillion, with product 1, product 2, and\nGenerated output example\nproduct 3 as top sellers. The report\n(by gpt-3.5-turbo):\nlacks specific details about timeframes,\nmarkets, or product lines, making it too Q1 2023 saw product C leading with\ngeneric for action. a 10% sales increase driven by an\neffective marketing campaign. Product\nA sales grew by 5%, while product B\nWhy it is bad:\nfaced a 2% decline, which suggests the\nThis prompt is overly vague and lacks need for a revised strategy. The report\ncontext. The AI may generate a generic provides a detailed comparison and\nreport on sales without considering the actionable insights for each product\nspecific industry, timeframe, or region line in the North American market.\nimportant to the business. This leads to\nan output unactionable or irrelevant to\nWhy it is good:\nthe organization\u2019s needs.\nThis prompt is precise, specifying\nthe period (Q1 2023), products of\ninterest (product lines A, B, and\nC), and geographical focus (North\nAmerican market). It also asks for\nan analysis beyond raw numbers\n(key growth drivers and areas for\nimprovement), which guides the\nAI to produce a comprehensive,\nrelevant, and actionable report.\nWhite Paper Master the Art of Prompt Engineering 4"}
{"instruction": "Explain this concept:", "response": "GOOD VS. BAD PROMPTS\nEXAMPLE 2\nBad prompt: Example of a good prompt:\n\"Write a query to find customers.\" \"Given a database with tables\n'Customers' (with fields 'id,' 'name,'\n'location') and 'Orders' (with fields\nGenerated output example\n'id,' 'order_date,' 'customer_id'), write\n(by gpt-3.5-turbo):\nan SQL query to find the names of\nSELECT * FROM customers customers who have placed more than\nfive orders.\"\nWhy it is bad:\nGenerated output example\nThis prompt is extremely vague and\n(by gpt-3.5-turbo):\nprovides no context or specific request.\nSELECT c.name FROM Customers\nThe AI model has no information about\nc JOIN (SELECT customer_id FROM\nthe database schema, desired outcome,\nOrders GROUP BY customer_id HAVING\nor complexity of the query needed. As a\nCOUNT(id) > 5) o ON c.id =\nresult, it cannot generate a meaningful\no.customer_id.\nor relevant SQL query.\nWhy it is good:\nThis prompt specifies the database\nschema and exact requirements.\nIt provides enough detail for the\nAI to understand the context and\ngenerate a precise SQL query that\nachieves the request.\nWhite Paper Master the Art of Prompt Engineering 5"}
{"instruction": "Explain this concept:", "response": "BUSINESS IMPACT\nAt the heart of using AI in business is prompt optimization, a critical factor needed to enhance\nthe accuracy and stability of Gen AI models. This accuracy directly influences key performance\nindicators (KPIs) and makes it a cornerstone for measuring success across various business\noperations. Studies indicate that optimized prompts will lead to at least a 10%-30% increase in\nmodel accuracy, which translates to significant improvements in efficiency, customer satisfaction,\nand financial outcomes. It is this foundational benefit that sets the stage for a cascade of additional\nadvantages, such as:\nCost efficiency\nBy increasing the precision of AI outputs from the beginning, businesses will streamline\nprocesses and reduce overhead associated with corrections. For example, customer\nservice AI systems that more accurately understand and address customer queries will\nsignificantly reduce reliance on human intervention, which yields substantial savings.\nEnhanced experience\nOptimized prompts ensure AI systems provide timely and highly relevant responses. This\nprecision boosts user interactions in industries like retail, where AI-enabled personalized\nrecommendations may lift conversion rates.\nInformed decision-making\nMore accurate AI analysis fosters better, data-driven business decisions. For instance, to\nreliably predict market trends in the financial sector, AI helps organizations make smarter\ninvestment choices, which potentially enhances profitability.\nRisk mitigation\nEffective prompt engineering also significantly reduces AI-generated errors or\n\"hallucinations,\" which may lead to operational and ethical risks. This safeguards against\nmisinformation and protects company credibility and consumer trust.\nBy prioritizing prompt optimization, organizations bolster the direct accuracy and effectiveness\nof their AI systems and unlock a host of secondary benefits, from cost savings to enhanced\nmarket positioning. This underscores the multifaceted value of this practice in today's AI-enabled\nlandscape.\nWhite Paper Master the Art of Prompt Engineering 6"}
{"instruction": "Explain this concept:", "response": "PROMPT LIFECYCLE AND EVALUATION\nTECHNIQUES\nThe lifecycle of a prompt is as essential as the\nadvancement of the models themselves in\nprompt engineering for LLMs. By referring to\nthe lifecycle diagram, insight is gained into the\ndynamic process of developing, evaluating,\nand refining prompts. This lifecycle is one\ncomponent of the broader LLMOps lifecycle,\nwhich oversees the entire journey from the\ndesign to deployment of Gen AI applications.\nFollowing is a focus on the prompt part of the\ncomprehensive lifecycle, which will be explored\nin SoftServe\u2019s next series of white papers.\nDesign is the first phase, where the initial construction of a prompt takes place. It is\n1\nthoughtfully composed to elicit the best response from the LLM.\n2 Implementation is where the prompt is introduced to the model, and its initial performance\nis observed.\n3 The evaluation phase is pivotal in prompt engineering and similar to the entire LLM\nsystem evaluation techniques, which is discussed in another paper. A variety of metrics are\nemployed, with each one chosen based on the specific nature of the task and what aspects of\nperformance will be measured, such as:\n\u2022 Classical NLP tasks. For tasks like text classification or named entity recognition (NER), the F1 score,\nprecision, and recall offer a balanced view of the model's accuracy.\n\u2022 Generated content with training data. For tasks that involve content generation, such as\nsummarization or translation, metrics like BLEU or rouge are essential. They compare the generated\ntext to a reference set, which ensures linguistic quality and content fidelity. Use these when as a\ngolden standard against which to compare the output.\n\u2022 Generated content without training data. In scenarios like creative writing, where there may not be\na correct answer, perplexity measures the prediction smoothness of the model. Lower perplexity\nindicates better fluency and is particularly useful when evaluating new content creation.\n\u2022 Hallucination detection. These metric types are critical in fact-based tasks, such as news article\ngeneration or data-driven reports. It helps maintain the credibility of the content by identifying and\ncorrecting factual inaccuracy.\nWhite Paper Master the Art of Prompt Engineering 7"}
{"instruction": "Explain this concept:", "response": "\u2022 Custom task-specific metrics. Depending on the domain, such as verifying facts, solving\nmathematical problems, executing code, or ensuring readability, specialized metrics are developed\nto provide the outputs that meet the necessary criteria for success in these areas.\n\u2022 LLMs as evaluators. Using other models to evaluate content will provide additional validation, which\nis useful in tasks like content grading or peer review.\n\u2022 Fairness metrics. Particularly relevant for prompts with social implications, such as job application\nscreening tools or credit scoring systems, to ensure that outputs do not discriminate against any\ngroup.\n\u2022 Human feedback. This kind of feedback is indispensable in tasks where user satisfaction is the goal,\nfor example, through chatbots or interactive storytelling. It adds a layer of qualitative assessment\nthat metrics alone cannot provide.\nIn addition, the number of prompt input and output tokens are essential metrics to evaluate\ncost-effectiveness and response utility. They help fine-tune the prompt to ensure concise\ninputs lead to valuable and relevant outputs without unnecessary verbosity or truncation.\nEvaluating the model's efficiency with a given prompt is crucial in LLM-based applications.\nIt ensures that the model's responses are accurate, coherent, and delivered within an\nacceptable timeframe, which balances computational costs against performance. This\nmultifaceted approach to evaluation ensures that the prompts lead to an efficient, fair, and\neffective use of LLMs.\n4 Refinement, iteration, validation, deployment, and maintenance are steps that\nwarrant the prompt remains effective, stable, fair, and evolves along with the model and its\napplications.\nThis lifecycle and the associated metrics form the backbone of prompt engineering. They establish\nthe creation of prompts that produce stable and high-quality outputs and do so economically and\nethically.\nWhite Paper Master the Art of Prompt Engineering 8"}
{"instruction": "Explain this concept:", "response": "PROMPT OPTIMIZATION FRAMEWORK\nFor the creation of effective prompts that yield targeted and valuable AI responses, consider a\nstructured approach that incorporates the following steps:\n1 Define your objective. Clearly state the purpose of your prompt\nby focusing on the desired achievement with the AI's response.\n2 Provide context. Include relevant background information to frame a\nrequest, which enables the AI to understand the broader scope.\n3 Be specific. Detail exactly what information or output is\nneeded, including any parameters, criteria, or formats.\n4 Include examples. When possible, offer examples to\nguide the AI to deliver the appropriate response.\n5 Use clear and concise language. Avoid ambiguity by carefully\nchoosing words by aiming for clarity and brevity.\n6 Iterate and refine. Based on initial responses, refine\nprompts to improve precision and relevance.\nEffective prompts require a blend of art and science tailored to each AI model's nuances.\nExperimentation and adaptation are key, as best practices vary. By testing different approaches\nand adjusting based on AI responses, prompts may be refined for optimal results.\nFor example, transitioning from a general prompt like \"summarize this article\" to a more specific\none, such as \"provide a concise, three-sentence summary focusing on main arguments and\nconclusions\" enhances clarity and aligns with informational goals. Iterative refinement, informed by\nLLM feedback, improves precision and relevance in AI interactions.\nWhite Paper Master the Art of Prompt Engineering 9"}
{"instruction": "Explain this concept:", "response": "When refining prompts with LLM feedback, it is insightful to consider how an LLM process prompts\ninto a \"chain-of-thoughts\" approach. For example:\nInitial prompt:\n\"My blog posts are not engaging my audience. How can I improve?\"\nPrompt improved by LLMs:\n\"What are proven methods to increase engagement on blog posts within the tech\nindustry significantly? Include headline crafting, SEO optimization techniques, and tools\nfor engagement analysis.\"\nGenerated thought processes and queries by LLMs:\n1. Identify top strategies for increasing blog 3. Explore SEO best practices for the blog post\npost engagement. to boost visibility.\n2. Analyze effective headlines for blog posts in 4. Recommend tools to analyze blog post\nGen AI. engagement metrics.\nPlease note that the improvement will be an iterative process. In the end, a more\ncomprehensive prompt will be created.\nFor example:\nPrompt improved by LLMs:\n\"What are proven methods to increase engagement on blog posts within the tech\nindustry significantly? Include headline crafting, SEO optimization techniques, and tools\nfor engagement analysis.\"\nTo boost audience engagement with my blog posts in the tech industry, I seek comprehensive strategies\nencompassing content creation and distribution. Specifically:\nContent development:\nWhat are effective techniques for crafting headlines that capture attention and encourage clicks within\nthe tech industry?\nSEO optimization:\n\u2022 What are the latest SEO strategies tailored for tech blogs to improve visibility on search engines?\n\u2022 Can you recommend keyword research and SEO analysis tools that are particularly effective for\ntech industry content?\nWhite Paper Master the Art of Prompt Engineering 10"}
{"instruction": "Explain this concept:", "response": "Engagement analysis and improvement:\n\u2022 What metrics should I focus on to accurately measure engagement on my blog posts?\n\u2022 Are there specific tools or platforms recommended for tracking these metrics and analyzing reader\nbehavior?\n\u2022 How can I use feedback and engagement data to iterate on content strategy for better\nperformance?\nCommunity building:\n\u2022 What are proven methods for fostering a community around my tech blog that encourages regular\ninteraction and feedback?\n\u2022 Please provide actionable insights and examples where applicable, emphasizing methods proven to\nbe effective specifically within the tech blogging sphere.\"\nAlways employ the evaluation methodologies outlined before to determine the effectiveness\nof prompts. Through this informed refinement process, the quality of AI-generated content is\nenhanced, which makes sure that the prompts are effective and customized to specific demands\nof the task.\nCONCLUSION AND FUTURE WORK\nThis white paper marks the beginning of SoftServe\u2019s series on defining various stages of prompt\nengineering process efficiency by concentrating on optimization and evaluation. We delve into\ngeneral techniques for optimizing and evaluating prompts and demonstrate their crucial role in\nenhancing AI model performance and user satisfaction.\nEffective prompt engineering is essential for organizations to maximize the quality of their\nsystem. By crafting prompts tailored to specific tasks and objectives, organizations will trigger AI\nsystems to generate accurate and relevant responses, which improve operational efficiency and\ncustomer experiences.\nIn future white papers, SoftServe will dive deeper into advanced topics like creating and refining\nsummarization prompts through step-by-step optimization. These papers will also explore\ncustom evaluation techniques for specific use cases.\nAs SoftServe continues the exploration of these subjects, the objective is to give practitioners\nand researchers the necessary knowledge and resources to fully use prompt engineering in AI\napplications. Monitoring and evaluation are crucial components in the LLM world and serve as\ncheckpoints to gauge the efficacy and performance of prompt engineering techniques. They\nprovide essential feedback loops that enable the refinement and enhancement of LLM systems.\nWhite Paper Master the Art of Prompt Engineering 11"}
{"instruction": "Explain this concept:", "response": "ABOUT US\nSoftServe is a premier IT consulting and digital services provider.\nWe expand the horizon of new technologies to solve today's complex\nbusiness challenges and achieve meaningful outcomes for our clients.\nOur boundless curiosity drives us to explore and reimagine the art\nof the possible. Clients confidently rely on SoftServe to architect and\nexecute mature and innovative capabilities, such as digital engineering,\ndata and analytics, cloud, and AI/ML.\nOur global reputation is gained from more than 30 years of experience\ndelivering superior digital solutions at exceptional speed by top-tier\nengineering talent to enterprise industries, including high tech, financial\nservices, healthcare, life sciences, retail, energy, and manufacturing.\nVisit our website, blog, LinkedIn, Facebook, and X (Twitter) pages\nfor more information.\nNORTH AMERICAN HQ\n201 W. 5th Street, Suite 1550\nAustin, TX 78701\n+1 866 687 3588 (USA)\n+1 647 948 7638 (Canada)\nEUROPEAN HQ\n30 Cannon Street\nLondon EC4 6XH\nUnited Kingdom\n+44 333 006 4341\ninfo@softserveinc.com\nwww.softserveinc.com"}
